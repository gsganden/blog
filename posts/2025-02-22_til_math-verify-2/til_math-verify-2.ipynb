{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d2c30b-84c6-4826-a91a-00e022f627cb",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"TIL: How Math-Verify Verifies LLM Outputs\"\n",
    "author: \"Greg Gandenberger\"\n",
    "date: 2025-02-22\n",
    "categories: [ml, llms, evals, math-verify]\n",
    "---\n",
    "\n",
    "::: {.callout-note}\n",
    "This is a [TIL](https://dev.to/jbranchaud/how-i-built-a-learning-machine-45k9) (\"Today I Learned\") post. I expect it to be useful to my future self and maybe to others, but it is meant to be a quick, informal way to capture something I learned rather than a polished presentation.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de10513",
   "metadata": {},
   "source": [
    "Hugging Faceâ€™s [Math-Verify](https://github.com/huggingface/math-verify) library provides relatively robust tools to evaluate LLM performance on math problems. Its README demonstrates using it by calling its `parse` function on both the LLM output and the gold answer, and then passing those results to `verify`. My [last post](/posts/2025-02-20_til_math-verify/til_math-verify.html) examined the `parse` function. This post examines the `verify` function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8c4cc8",
   "metadata": {},
   "source": [
    "## Comparing Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b3dd5c",
   "metadata": {},
   "source": [
    "We saw previously that `parse` returns a list which may contain both a `sympy` expression and a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4187ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_verify import parse, verify\n",
    "import sympy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b5188e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1/3, '1/3']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\"1/3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3139934a",
   "metadata": {},
   "source": [
    "`verify` ostensibly compares everything in the first list with everything in the second list, and returns `True` if any of those combinations pass its equality check. However, its equality check always returns `False` for the combination of a `sympy` expression and a string, so in practice it just indicates whether either the two `sympy` expressions or the two strings are equal to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbbebe97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero = sympy.Number(0)\n",
    "one = sympy.Number(1)\n",
    "\n",
    "# Everything is equal\n",
    "verify(gold=[zero, \"0\"], target=[zero, \"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e58bb233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `gold` and `target` are each internally consistent but are not equal to each other\n",
    "verify(gold=[zero, \"0\"], target=[one, \"1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9533f76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `gold` and `target` sympy expressions are equal to each other while their strings are not\n",
    "verify(gold=[zero, \"1\"], target=[zero, \"2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6e2623b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `gold` and `target` strings are equal to each other while their `sympy` expressions are not\n",
    "verify(gold=[zero, \"2\"], target=[one, \"2\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "648fd901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `gold` `sympy` expression is equal to `target` string and vice versa\n",
    "verify(gold=[zero, \"1\"], target=[one, \"0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6ba05c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `gold` and `target` indicate the same value, but one is a `sympy` expression and the other is a string\n",
    "verify(gold=[zero], target=[\"0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85374f8f",
   "metadata": {},
   "source": [
    "This last example might seem surprising. The thinking behind this behavior, as I understand [this comment](https://github.com/gsganden/Math-Verify/blob/main/src/math_verify/grader.py#L672-L675), is that a string should only be present without a corresponding `sympy` expression if parsing that string failed, and it is unlikely that parsing failed on one side and yet the string on that side is genuinely equal to the `sympy` expression on the other side. This rationale makes sense *on the assumption that the input to `verify` came from `parse`*, which is probably what we want but could be documented more explicitly, as I suggested in [this issue](https://github.com/huggingface/Math-Verify/issues/20)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc8c587",
   "metadata": {},
   "source": [
    "## Equality for Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfb75a3",
   "metadata": {},
   "source": [
    "Equality for strings is simply Python `==` after stripping whitespace off the ends and ensuring that the strings are not both empty. This approach is obviously imperfect, but it is meant only to catch some of the cases where `sympy` parsing fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cfb1fdb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(gold=[\"1/3\"], target=[\"1 / 3\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fca4662",
   "metadata": {},
   "source": [
    "## Equality for `sympy` Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c18980c",
   "metadata": {},
   "source": [
    "Equality for `sympy` expressions is complex. At this core it uses `sympy` functionality such as `Eq` and `evalf` after applying various normalization steps, with support for a few options for strictness:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddbb63a",
   "metadata": {},
   "source": [
    "```\n",
    "float_rounding: Number of decimal places to round floats to. Defaults to 6.\n",
    "numeric_precision: Number of decimal places to consider for numeric comparisons. Defaults to 15.\n",
    "    - If know the evaluated expressions will be small, you should increase this. See: https://docs.sympy.org/latest/modules/evalf.html\n",
    "strict: Whether to enforce strict comparison mode. Defaults to True.\n",
    "    - In strict mode: Variables matter and sets are not comparable with tuples\n",
    "    - In non-strict mode: Variables are matched by position and sets can be compared with tuples\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391b6a6c",
   "metadata": {},
   "source": [
    "The presence of both `numeric_precision` and `float_rounding` parameters could lead to confusion, as [this issue](https://github.com/huggingface/Math-Verify/issues/21) notes: setting one of them to a high value will not have the result one might expect if the other is lower:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "357f6b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# `float_rounding` is 6 by default, so increasing `numeric_precision` has no effect in this case\n",
    "verify(parse(\"0.0000001\"), parse(\"0.0000002\"), numeric_precision=99999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3819e246",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96bb1da",
   "metadata": {},
   "source": [
    "I see two sharp edges in Math-Verify's `verify` function: it assumes that its inputs have passed through `parse`, and it has `numeric_precision` and `float_rounding` parameters that need to be adjusted together to avoid unexpected behavior. Otherwise it seems like a smart approach to the difficult problem of comparing LLM outputs to gold answers on open-ended math problems without relying on an LLM judge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef19dadd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
