{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d2c30b-84c6-4826-a91a-00e022f627cb",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"TIL: How Math-Verify Parses LLM Outputs\"\n",
    "author: \"Greg Gandenberger\"\n",
    "date: 2025-02-20\n",
    "categories: [ml, llms, evals, math-verify]\n",
    "---\n",
    "\n",
    "::: {.callout-note}\n",
    "This is a [TIL](https://dev.to/jbranchaud/how-i-built-a-learning-machine-45k9) (\"Today I Learned\") post. I expect it to be useful to my future self and maybe to others, but it is meant to be a quick, informal way to capture something I learned rather than a polished presentation.\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13553b6f",
   "metadata": {},
   "source": [
    "In [a previous post](/posts/2025-02-18_til_llmfoundry-eval/til_llmfoundry-eval.md), we saw that LLM Foundry's default evaluation procedure for open-ended math problems has some limitations. That post proposed to address this problem by using Hugging Face's [Math-Verify](https://github.com/huggingface/math-verify) library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64401176",
   "metadata": {},
   "source": [
    "This post is a first look at Math-Verify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8074bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math_verify import parse, verify, LatexExtractionConfig, ExprExtractionConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b7d03e",
   "metadata": {},
   "source": [
    "# Applying Math-Verify to Our Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f97950",
   "metadata": {},
   "source": [
    "Naively following the [Math-Verify README](https://github.com/huggingface/Math-Verify/blob/f3d85eda946f407d7cd4f245e8ac8894587462b2/README.md), we can apply it to one of the examples that the default LLM Foundry evaluation had trouble with as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4ed251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold = parse(\"40\")\n",
    "answer = parse(\"20 + 20 = 40\")\n",
    "\n",
    "verify(gold, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ccfae",
   "metadata": {},
   "source": [
    "Hear Math-Verify gets the right answer, correctly telling us that the model is correct."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7697b9c3",
   "metadata": {},
   "source": [
    "Let's try another example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3eb18409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold = parse(\"15 pounds x 1/4 pounds x 1/2 pounds = 15 pounds.\")\n",
    "answer = parse(\"15\")\n",
    "\n",
    "verify(gold, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffc4ec4",
   "metadata": {},
   "source": [
    "In this case the model gets the right answer for the wrong reason. If we are expecting Math-Verify to evaluate the model's answer and not its reasoning, then it is doing what we want here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2898a8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold = parse(\"12\\nB: 16\\nC: 24\\nD: 32\")\n",
    "answer = parse(\"12\")\n",
    "\n",
    "verify(gold, answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e319236",
   "metadata": {},
   "source": [
    "Here the model misunderstands its job -- instead of giving an answer, it gives multiple-choice options. The first option happens to be correct, but the model should not get credit for giving the right answer, and Math-Verify correctly rejects its response."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a8a59c",
   "metadata": {},
   "source": [
    "# How Does Math-Verify's `parse` Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5487df9",
   "metadata": {},
   "source": [
    "So far, so good -- Math-Verify is getting the right result in these three cases. But how? Is it getting the right results in ways that generalize?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed230fd",
   "metadata": {},
   "source": [
    "Let's dig one layer deeper by seeing how Math-Verify's `parse` is working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33f2c9af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, '32']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\"12\\nB: 16\\nC: 24\\nD: 32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db87f805",
   "metadata": {},
   "source": [
    "Ah. Here `parse` is returning a list of two values, both of which are some version of \"32\". \"32\" is the last multiple-choice optional that the model gave. I suspect that if 32 had been the correct answer, then Math-Verify would have counted the model's response as correct even though it was a set of options rather than a definite answer. Let's check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c77ecd76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verify(parse(\"12\\nB: 16\\nC: 24\\nD: 32\"), parse(\"32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f326e73",
   "metadata": {},
   "source": [
    "OK, so Math-Verify isn't magic. In this case it simply picking out the last number where LLM Foundry picked out the first number, which happens to give the correct result in this case but is not any better in principle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04259764",
   "metadata": {},
   "source": [
    "### `fallback_mode`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f9abe0",
   "metadata": {},
   "source": [
    "Why is `parse(\"12\\nB: 16\\nC: 24\\nD: 32\")` returning a list of two values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9fc48a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[sympy.core.numbers.Integer, str]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_results = parse(\"12\\nB: 16\\nC: 24\\nD: 32\")\n",
    "[type(item) for item in parse_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa60b34",
   "metadata": {},
   "source": [
    "The first item is a `sympy` object, and the second is a string."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c60b33",
   "metadata": {},
   "source": [
    "`sympy` objects have some advantages for our purposes. For instance, it can recognize objects as equal even when they are written differently, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2466a6d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\text{True}$"
      ],
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "sp.Eq(sp.sympify(\"1/2\"), sp.sympify(\"0.5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82299870",
   "metadata": {},
   "source": [
    "The string result is meant to be a fallback option. You can turn it off:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d5d81cd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\n",
    "    \"12\\nB: 16\\nC: 24\\nD: 32\",\n",
    "    fallback_mode=\"no_fallback\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcce026",
   "metadata": {},
   "source": [
    "`parse` works in two stages. First, it pulls out regex matches from the input text. Then it tries to cast each of those matches as a `sympy` object. With `fallback_mode=\"first_match\"`, `parse` returns the first regex match it pulls out as a string, independently of what happens with `sympy`. With `fallback_mode=\"no_fallback\"`, it does not return a string; it only returns the `sympy` object, if `sympy` processing succeeds, or an empty list if `sympy` processing fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d778997",
   "metadata": {},
   "source": [
    "Here is an example where `parse` pulls out the string inside `\\\\boxed{}`, but the string is not a well-formed mathematical expression, so `sympy` cannot process it. `parse` then returns just the string with `fallback_mode=\"first_match\"` (or by default) and an empty list with `fallback_mode=\"no_fallback\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a1465f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E=mc^']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\n",
    "    \"\\\\boxed{E=mc^}\",\n",
    "    fallback_mode=\"first_match\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "974d86a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E=mc^']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\n",
    "    \"\\\\boxed{E=mc^}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20fa1f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\n",
    "    \"\\\\boxed{E=mc^}\",\n",
    "    fallback_mode=\"no_fallback\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d855742",
   "metadata": {},
   "source": [
    "At this point it might be clearer for `parse` to have a Boolean parameter with a name like `return_string_fallback` rather than a string parameter with the name `fallback_mode` that simply controls whether or not `parse` returns the first match it tries to parse as a string. Perhaps the motivation for the current design is that it provides flexibility to add more \"fallback modes\" in the future without changing the function signature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075a27d4",
   "metadata": {},
   "source": [
    "### `extraction_mode`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443aecf4",
   "metadata": {},
   "source": [
    "`parse` has a second parameter `extraction_mode` that also affects what happens when casting to `sympy` fails. With `extraction_mode=\"first_match\"`, `parse` will only try casting to `sympy` once, and will not return a `sympy` object if it fails. With `extraction_mode=\"any_match\"`, `parse` will keep trying to cast matches to `sympy` until one succeeds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75bdf20",
   "metadata": {},
   "source": [
    "For instance, if we have one invalid expression and one valid expression, `parse` will return the valid expression as a `sympy` object with `extraction_mode=\"any_match\"`, but with `extraction_mode=\"first_match\"` it will not return any `sympy` objects if it processes the invalid expression before the valid one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "246c4c79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\n",
    "    \"$x + y$ $E=mc^$\",\n",
    "    fallback_mode=\"no_fallback\",  # do not return a string\n",
    "    extraction_mode=\"first_match\",  # give up on returning a `sympy` object if the first attempt fails\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c059c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['E=mc^']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\n",
    "    \"$x + y$ $E=mc^$\",\n",
    "    fallback_mode=\"first_match\",  # return a string from the first match regardless of whether casting to `sympy` succeeds\n",
    "    extraction_mode=\"first_match\",  # give up on returning a `sympy` object if the first attempt fails\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bc11d8fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x + y]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\n",
    "    \"$x + y$ $E=mc^$\",\n",
    "    fallback_mode=\"no_fallback\",  # do not return a string\n",
    "    extraction_mode=\"any_match\",  # keep trying to return a `sympy` object until one attempt succeeds\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "30bfe06c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x + y, 'E=mc^']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\n",
    "    \"$x + y$ $E=mc^$\",\n",
    "    fallback_mode=\"first_match\",  # return a string from the first match regardless of whether casting to `sympy` succeeds\n",
    "    extraction_mode=\"any_match\",  # keep trying to return a `sympy` object until one attempt succeeds\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6618b",
   "metadata": {},
   "source": [
    "In this last example, the two returned items come from different matches. The second item is the invalid expression, which we get as a string because it is processed first and we have fallback_mode=\"first_match\". The first item is the valid expression, which we get as a `sympy` object because setting `extraction_mode=\"any_match\"` caused `parse` to keep trying to cast to `sympy` until it succeeded."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d1d908",
   "metadata": {},
   "source": [
    "In these examples `parse` finds two matches and prioritizes the second one. However, it does not always prioritize the last match. Let's take a look at what it does instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39ce20b",
   "metadata": {},
   "source": [
    "### `extraction_config`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a095c1",
   "metadata": {},
   "source": [
    "`parse` has an additional `extraction_config` parameter that takes a sequence of `ExtractionConfig` objects. Each `ExtractionConfig` object specifies one procedure for finding and prioritizing matches. It supports three `ExtractionConfig` classes: `LatexExtractionConfig`, `StringExtractionConfig`, and `ExprExtractionConfig`. Based on which of these classes is used, `parse` generates a list of regexes with with associated priority levels that it uses to find and prioritize matches. Within a given priority level, `parse` processes matches in the reverse order they appear in the input text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7d8e2a",
   "metadata": {},
   "source": [
    "For instance, `LatexExtractionConfig` looks for expressions within various LaTeX delimiters, such as `$$...$$` and `\\[..\\]`. It prioritizes matches that are \"marked\" as the final answer, for instance by being inside `\\boxed{}` or after \"final answer is\". The details are quite complicated, and I do not fully understand them, but let's look at some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f446744d",
   "metadata": {},
   "source": [
    "With two expressions simply delimited by `$`, `parse` finds both matches and prioritizes the second one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "86a1be80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Eq(E, c**2*m)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\n",
    "    \"$x + y$ $E=mc^2$\",\n",
    "    extraction_config=[LatexExtractionConfig()],\n",
    "    extraction_mode=\"any_match\",\n",
    "    fallback_mode=\"no_fallback\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938147f4",
   "metadata": {},
   "source": [
    "However, `parse` will prioritize the first match if it is marked as the final answer in a way that it recognizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0e13ff1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x + y]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\n",
    "    \"the final answer is $x + y$ $E=mc^2$\",\n",
    "    extraction_config=[LatexExtractionConfig()],\n",
    "    extraction_mode=\"any_match\",\n",
    "    fallback_mode=\"no_fallback\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a959d35b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[x + y]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\n",
    "    \"\\\\boxed{x + y} $E=mc^2$\",\n",
    "    extraction_config=[LatexExtractionConfig()],\n",
    "    extraction_mode=\"any_match\",\n",
    "    fallback_mode=\"no_fallback\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616b714",
   "metadata": {},
   "source": [
    "This approach makes sense, but it is complicated and currently not well documented. It rewards models for marking their final answers in certain specific ways, which introduces some amount of coupling between the evaluation procedure and the details of how the model is trained and prompted."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc6ff4a",
   "metadata": {},
   "source": [
    "If I understand correctly, `ExprExtractionConfig` looks for numerical (rather than symbolic) mathematical expressions without relying on LaTeX delimiters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9322e88b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1 + 2]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\n",
    "    \"1 + 2\",\n",
    "    extraction_config=[ExprExtractionConfig()],\n",
    "    extraction_mode=\"any_match\",\n",
    "    fallback_mode=\"no_fallback\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808da2e2",
   "metadata": {},
   "source": [
    "It is prone to pulling out parts of larger expressions in ways that may or may not match our desires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2253622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we get `3`, which probably is what we want\n",
    "parse(\n",
    "    \"1 + 2 = 3\",\n",
    "    extraction_config=[ExprExtractionConfig()],\n",
    "    extraction_mode=\"any_match\",\n",
    "    fallback_mode=\"no_fallback\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3348afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we get \"2\", which probably is not what we want\n",
    "parse(\n",
    "    \"$1 + 2$\",\n",
    "    extraction_config=[ExprExtractionConfig()],\n",
    "    extraction_mode=\"any_match\",\n",
    "    fallback_mode=\"no_fallback\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb7617",
   "metadata": {},
   "source": [
    "By default, `extraction_config=[LatexExtractionConfig(), ExprExtractionConfig()]`, so `parse` will find both LaTeX expressions and numerical expressions. It combines their matches into one pool, prioritizing those matches by the priorities that the configs give them and breaking ties by working back to front."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c48c38b",
   "metadata": {},
   "source": [
    "The final config class, `StringExtractionConfig` is to a first approximation simply looking for any of a fixed set of strings, by default \"A\", \"B\", \"C\", and \"D\". I take it that it is meant to be used for multiple-choice questions rather than open-ended math problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05313188",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1154ab6",
   "metadata": {},
   "source": [
    "Now that we have some general idea of how `parse` works, let's look back at the examples from the previous post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "47574781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, '32']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\"12\\nB: 16\\nC: 24\\nD: 32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4d42e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\n",
    "    \"12\\nB: 16\\nC: 24\\nD: 32\",\n",
    "    extraction_config=[LatexExtractionConfig()],\n",
    "    extraction_mode=\"any_match\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ad98ee8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\n",
    "    \"12\\nB: 16\\nC: 24\\nD: 32\",\n",
    "    extraction_config=[ExprExtractionConfig()],\n",
    "    extraction_mode=\"any_match\",\n",
    "    fallback_mode=\"no_fallback\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dee414b",
   "metadata": {},
   "source": [
    "In this case, `LatexExtractionConfig` does not find any matches because the model's output is not formatted as LaTeX. `ExprExtractionConfig` presumably finds all four numbers and gives them the same priority, so it returns the last number, 32."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e03f7",
   "metadata": {},
   "source": [
    "The fact that `parse` returns the same result here as it would if the model had simply returned \"32\" is a problem, because it will cause us to count the model as correct even though it did not give a definite answer. The ideal behavior depends on our larger system design, but it would involve recognizing that the model did not give a definite answer and returning something that indicates as much, such as perhaps an empty list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a514728",
   "metadata": {},
   "source": [
    "In our other two examples, `parse` returns the rightmost number for the same reason, and that happens to be the right thing to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "436dead3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[40, '40']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\"20 + 20 = 40\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "969cee19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15, '15']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\"15 pounds x 1/4 pounds x 1/2 pounds = 15 pounds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4114893b",
   "metadata": {},
   "source": [
    "`parse` is simply picking out the last number in the model's output here, rather than intelligently handling the `=` sign, as this example shows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71a414ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[50, '50']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse(\"20 + 20 = 40. By the way, my favorite number is 50.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211db642",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e99bb2",
   "metadata": {},
   "source": [
    "Math-Verify is a library for evaluating LLM outputs on open-ended math problems. It provides a `parse` function that uses regexes to extract mathematical expressions and then attempts to cast them as `sympy` objects. It also provides a `verify` function that compares the parsed model output to the parsed gold answer. I will look at this `verify` function in a future post. The resulting evaluation process is not foolproof, but it is perhaps an improvement over LLM Foundry's default evaluation procedure."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
