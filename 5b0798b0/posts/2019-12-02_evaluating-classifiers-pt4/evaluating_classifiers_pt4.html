<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Greg Gandenberger">
<meta name="dcterms.date" content="2019-12-02">

<title>blog - Evaluating Classification Models, Part 4: Weighted Power Means of Precision and Recall</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Evaluating Classification Models, Part 4: Weighted Power Means of Precision and Recall</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">ml</div>
                <div class="quarto-category">evals</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Greg Gandenberger </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">December 2, 2019</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="formula.webp" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">G<span class="math inline">\(_{\beta,\rho}\)</span> generalizes the popular F<span class="math inline">\(_\beta\)</span> family of classification model evaluation metrics.</figcaption><p></p>
</figure>
</div>
<p>This post is part of a series on evaluating classification models:</p>
<ul>
<li><a href="../../posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html"><strong>Part 1: Weighing False Positives Against False Negatives</strong></a> explains why we need systematic ways to evaluate classification models.</li>
<li><a href="../../posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html"><strong>Part 2: The Sufficiency of Precision and Recall</strong></a> explains why <em>precision</em> and <em>recall</em> are sufficient for evaluating classification models in typical cases.</li>
<li><a href="../../posts/2019-11-26_evaluating-classifiers-pt3/evaluating_classifiers_pt_3.html"><strong>Part 3: <span class="math inline">\(F_\beta\)</span> and Other Weighted Pythagorean Means of Precision and Recall</strong></a> explains what patterns of preferences are encoded by the <em>Pythagorean means</em> of precision and recall. This class of metrics includes the popular <span class="math inline">\(F_\beta\)</span> family, among others.</li>
<li><a href="../../posts/2019-12-02_evaluating-classifiers-pt4/evaluating_classifiers_pt4.html"><strong>Part 4: Weighted Power Means of Precision and Recall</strong></a> generalizes beyond the Pythagorean means to the broader class of <em>weighted power means</em> of precision and recall.</li>
</ul>
<p>This series differs from other discussions of evaluation metrics for classification models in that it aims to provide a <strong>systematic perspective</strong>. Rather than providing a laundry list of individual metrics, it situates those metrics within a fairly comprehensive family and explains how you can choose a member of that family that is appropriate for your use case.</p>
<p>The <a href="../../posts/2019-11-26_evaluating-classifiers-pt3/evaluating_classifiers_pt_3.html">previous post</a> describes the weighted “Pythagorean” (arithmetic, geometric, and harmonic) means of precision and recall. The arithmetic mean with weight <span class="math inline">\(\gamma\)</span> says that you are willing to trade one unit of precision for <span class="math inline">\(\gamma\)</span> or more units of recall, regardless of the current precision and recall. It is appropriate in situations such as medical testing where an individual false positive or false negative is basically unaffected by the total number of false positives and false negatives. For a harmonic mean, by contrast, the number of points of units you require in exchange for one unit of precision depends on the current precision and recall. The same is true for a geometric mean, but to a smaller degree. As a result, a geometric or harmonic mean may be appropriate in settings such as information retrieval where the cost of a false positive or false negative depends on the “big picture” of the model’s outputs.</p>
<p><strong>This post discusses the full class of weighted power means of precision and recall, which includes the Pythagorean means as special cases.</strong> It may be overkill for practical purposes: an appropriately weighted arithmetic or harmonic mean is good enough for most problems, and using anything else will probably confuse your collaborators. However, it will give you a stronger handle on how the Pythagorean means work as well as additional options for handling unusual situations.</p>
<section id="weighted-power-means" class="level1">
<h1>Weighted Power Means</h1>
<p>The weighted arithmetic, geometric, and harmonic means are all instances of the class of <strong>weighted power means</strong> (defining the Weighted Power Mean with <span class="math inline">\(\phi=0\)</span> to be the limit of the class as <span class="math inline">\(\phi\)</span> approaches 0):</p>
<p><span class="math display">\[
\text{Weighted Power Mean}(\gamma, \phi): \left(\frac{1}{1 + \gamma} \left(\gamma P^\phi + R^\phi \right)\right)^{1/\phi}
\]</span></p>
<p><span class="math inline">\(\phi=1\)</span> yields the weighted arithmetic mean:</p>
<p><span class="math display">\[
\text{Weighted Arithmetic Mean}(\gamma, \phi=1): \left(\frac{1}{1 + \gamma} \left(\gamma P + R \right)\right)
\]</span></p>
<p><span class="math inline">\(\phi=-1\)</span> yields the weighted harmonic mean (the inverse of the arithmetic mean of the inverses):</p>
<p><span class="math display">\[
\text{Weighted Harmonic Mean}(\gamma, \phi=-1): \left(\frac{1}{1 + \gamma} \left(\gamma P^{-1} + R^{-1} \right)\right)^{-1}
\]</span></p>
<p>The weighted geometric mean turns out to be the <em>limit</em> of this expression as <span class="math inline">\(\phi\)</span> approaches 0:</p>
<p><span class="math display">\[
\begin{align*}
\text{Weighted Geometric Mean}(\gamma, \phi\to0) &amp;= \lim_{\phi\to0} \left(\frac{1}{1 + \gamma} \left(\gamma P^\phi + R^\phi \right)\right)^{1/\phi} \\
&amp;= \exp \left(\frac{1}{1 + \gamma} \left(\gamma \ln P + \ln R \right)\right) \\
&amp;= (P^\gamma R)^{1/(1+\gamma)}
\end{align*}
\]</span></p>
<p>We saw in the <a href="../../posts/2019-11-26_evaluating-classifiers-pt3/evaluating_classifiers_pt_3.html">previous post</a> that the weighted arithmetic mean has level curves that are straight, indicating that you would trade one unit of precision for <span class="math inline">\(\gamma\)</span> or more units of recall regardless of the current values for precision and recall. I recommend reviewing that post now if it is not fresh in your mind.</p>
<iframe src="plots/weighted_arithmetic_mean_curves.html" width="100%" height="550" frameborder="0">
</iframe>
<p>By contrast, the geometric and harmonic means have level curves that curve upward, indicating that you care more about recall relative to precision for instance when recall is <span class="math inline">\(.1\)</span> than when recall is <span class="math inline">\(.9\)</span>, holding precision fixed.</p>
<iframe src="plots/weighted_harmonic_mean_curves4.html" width="100%" height="550" frameborder="0">
</iframe>
<p><strong>The more general class of weighted power means can have level curves that curve in either direction and to whatever degree we like.</strong> As you take <span class="math inline">\(\phi\)</span> toward -infinity in the plot above, the level curves approach two straight lines meeting at a right angle. That metric cares about only whichever of precision and recall is lagging behind. With the weight parameter <span class="math inline">\(\gamma=1\)</span>, it is just the minimum of precision and recall.</p>
<p>Here are the level curves for the weighted power means as a function of <span class="math inline">\(\gamma\)</span> with <span class="math inline">\(\phi=-2\)</span>.</p>
<iframe src="plots/weighted_power_mean_phi_neg2_curves.html" width="100%" height="550" frameborder="0">
</iframe>
<p>When you set <span class="math inline">\(\phi&gt;1\)</span>, the level curves curve <em>downward</em>, indicating that you care about increasing precision more than we care about increasing recall when precision is <em>high</em> compared to recall, and vice versa. I am not aware of any real classification problems where this behavior seems appropriate, but if such a problem were to arise then we would be able to handle it with a weighted power mean. With <span class="math inline">\(\gamma=1\)</span>, the weighted power mean converges to the maximum of precision and recall as <span class="math inline">\(\phi\)</span> goes to infinity.</p>
<p>Here are the level curves for the weighted power means as a function of <span class="math inline">\(\gamma\)</span> with <span class="math inline">\(\phi=2\)</span>.</p>
<iframe src="plots/weighted_power_mean_phi_2_curves.html" width="100%" height="550" frameborder="0">
</iframe>
</section>
<section id="a-better-parameterization" class="level1">
<h1>A Better Parameterization</h1>
<p>Weighted power means of precision and recall form a useful class of metrics for evaluating classification models. However, the standard way of parameterizing that class is not ideal for our purposes. The plot below shows the level curves for the weighted power means as a function of <span class="math inline">\(\phi\)</span> with <span class="math inline">\(\gamma=3\)</span>. <strong>Observe that changing <span class="math inline">\(\phi\)</span> affects not only the curvature of the level curves around the black line, but also the location of the black line.</strong></p>
<iframe src="plots/weighted_power_means_gamma_3_curves.html" width="100%" height="550" frameborder="0">
</iframe>
<p><strong>Ideally, we would have separate control over those two aspects of our metric.</strong> One parameter would affect only the slope of the black line, which represents the overall importance of recall relative to precision as the recall/precision ratio at which we care about them equally. The other would affect only the curvature of the level curves, which represents how our concern for recall relative to precision changes as we move away from that break-even recall/precision ratio.</p>
<p>The key is to replace the weight parameter <span class="math inline">\(\gamma\)</span> with <span class="math inline">\(\beta=\gamma^{1/(\phi+1)}\)</span>. (We made special cases of this substitution for the harmonic and geometric means in the previous post.) While we are at it, I would like to replace the curvature parameter <span class="math inline">\(\phi\)</span> with <span class="math inline">\(\rho=\phi+1\)</span> so that the level curves curve upward for negative <span class="math inline">\(\rho\)</span> and downward for positive <span class="math inline">\(\rho\)</span>, with <span class="math inline">\(\rho=0\)</span> (the arithmetic mean) as a neutral point.</p>
<p>Here is the resulting family of metrics. For lack of a better name I will call it <span class="math inline">\(G_{\beta,\rho}\)</span> because it generalizes <span class="math inline">\(F_\beta\)</span>:</p>
<p><span class="math display">\[
G_{\beta,\rho} = \left(\frac{1}{1 + \beta^\rho} \left(\beta^\rho P^{\rho+1} + R^{\rho+1} \right)\right)^{\frac{1}{\rho+1}}
\]</span></p>
<p>In the case <span class="math inline">\(\rho=0\)</span>, this expression reduces to the simple arithmetic mean <span class="math inline">\(1/2(P+R)\)</span>, and the weighting is lost. For generality, I will stipulate an expression for that special case that preserves the weighting:</p>
<p><span class="math display">\[
G_{\beta,\rho=0} = \frac{1}{1 + \beta} \left(\beta P + R \right)
\]</span></p>
<p>The expression approaches the geometric mean as <span class="math inline">\(\rho\)</span> approaches -1, so I will stipulate that <span class="math inline">\(G_{\beta,\rho}\)</span> just is the geometric mean in that case:</p>
<p><span class="math display">\[
G_{\beta,\rho=-1} = \left(P R^\beta\right)^{\frac{1}{1+\beta}}
\]</span></p>
<p>The standard <span class="math inline">\(F_\beta\)</span> score (i.e., the harmonic mean) corresponds to <span class="math inline">\(G_{\beta,\rho}\)</span> with <span class="math inline">\(\rho=-2\)</span>.</p>
<p>Observe that in the plots below, <strong>changing <span class="math inline">\(\beta\)</span> affects only the location of the black line</strong> (i.e., the value of the recall/precision ratio where we are equally concerned about improvements in one or the other). This first plot uses <span class="math inline">\(\rho=-3\)</span>, so that the level curves curve upward more strongly than those of the harmonic mean (<span class="math inline">\(\rho=-2\)</span>). Notice that the curvature of the level curves stays the same as you vary <span class="math inline">\(\beta\)</span>.</p>
<iframe src="plots/g_beta_rho_neg3_curves.html" width="100%" height="550" frameborder="0">
</iframe>
<p>The next plot uses <span class="math inline">\(\rho=3\)</span>, so that the level curves curve downward. Again, varying <span class="math inline">\(\beta\)</span> changes the location of the black line but not the curvature of the level curves around that line.</p>
<iframe src="plots/g_beta_rho_3_curves.html" width="100%" height="550" frameborder="0">
</iframe>
<p>On the other hand, <strong>changing <span class="math inline">\(\rho\)</span> affects only the curvature of the level curves around the black line</strong> (i.e., how our concern for recall versus precision shifts as their ratio departs from that value), and not the location of the black line. The plot below illustrates this point for <span class="math inline">\(\beta=2\)</span>.</p>
<iframe src="plots/g_beta_2_rho_curves.html" width="100%" height="550" frameborder="0">
</iframe>
<p>We have made the family of weighted power means <em>orthogonal</em> with respect to (1) how much we care about recall versus precision overall (the location of the black line) and (2) how that concern shifts with recall and precision (the curvature of the level curves around the black line). See the appendices at the bottom of this post for further details.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p><span class="math inline">\(G_{\beta,\rho}\)</span> is a new family of evaluation metrics for classification models. <strong>The <span class="math inline">\(\beta\)</span> parameter expresses how much you care about recall versus precision overall, just like <span class="math inline">\(\beta\)</span> in the popular <span class="math inline">\(F_\beta\)</span> family.</strong> More precisely, it specifies the ratio of recall to precision where you would be equally happy with a one-unit increase in precision or a one-unit increase in recall (in the limit as the size of the unit goes to 0).</p>
<p><strong>The <span class="math inline">\(\rho\)</span> parameter expresses how your concern about recall versus precision shifts as the recall/precision ratio moves away from that special <span class="math inline">\(\beta\)</span> value.</strong></p>
<p>For the weighted arithmetic mean (<span class="math inline">\(\rho=0\)</span>), your concern for recall versus precision does not shift at all: you would trade one unit of precision for <span class="math inline">\(\beta\)</span> units of recall regardless of the current values of precision and recall. This subclass of metrics is appropriate for settings such as medical testing where the expected cost of a single false positive or false negative is more or less fixed regardless of the total number of false positives and false negatives.</p>
<p><span class="math inline">\(\rho&lt;0\)</span>, by contrast, expresses that you are more concerned about recall than precision when the ratio of recall to precision is less than <span class="math inline">\(\beta\)</span> and more concerned about precision when it is greater than <span class="math inline">\(\beta\)</span>. The smaller the value for <span class="math inline">\(\rho\)</span>, the more your concern shifts toward whichever of precision and recall would need to increase to bring their ratio back toward <span class="math inline">\(\beta\)</span>. This sort of metric seems appropriate for settings such as information retrieval where the cost of a single false positive or a false negative depends on its contribution to the overall impression created by a set of predictions.</p>
<p><span class="math inline">\(F_\beta\)</span> has this general character, but it fixes the “curvature” parameter <span class="math inline">\(\rho\)</span> to -2, which may not be appropriate for all scenarios. <strong>The more comprehensive <span class="math inline">\(G_{\beta,\rho}\)</span> family allows us to fine-tune an evaluation metric for cases in which our concern for recall relative to precision shifts with their ratio more or less strongly than <span class="math inline">\(F_\beta\)</span> assumes.</strong></p>
</section>
<section id="epilogue-what-else-is-there" class="level1">
<h1>Epilogue: What Else Is There?</h1>
<p><span class="math inline">\(G_{\beta,\rho}\)</span> has a lot of flexibility, but it is not fully general. In the special case of the weighted arithmetic mean (<span class="math inline">\(\rho=0\)</span>), it says that you are willing to trade one unit of precision for <span class="math inline">\(\beta\)</span> units of recall regardless of the current precision and recall. In other cases, it says that there is a ratio <span class="math inline">\(\beta\)</span> of recall to precision where you would be willing to trade one unit of precision for one unit increase of recall. <strong>If your preferences don’t match either of those patterns, then no member of <span class="math inline">\(G_{\beta,\rho}\)</span> represents them.</strong></p>
<p>We can still say something about what kinds of measures might be appropriate in such a case: van Rijsbergen showed that under quite general conditions, one’s preferences over models can be represented by a sum of a function of recall and a function of precision (<a href="https://openlib.org/home/krichel/courses/lis618/readings/rijsbergen79_infor_retriev.pdf"><em>Information Retrieval</em></a>, p.&nbsp;132), in the sense that this function will give a higher score to Model A than to Model B if and only if one prefers Model A to Model B. Moreover, this measure is unique up to rescaling by addition and multiplication by constants. This result signifies that <strong>any reasonable metric for a binary classification model can be decomposed into the sum of a function that depends only on recall and a function that depends only on precision.</strong></p>
</section>
<section id="appendix-technical-details" class="level1">
<h1>Appendix: Technical Details</h1>
<section id="demonstration-that-beta-does-its-job" class="level2">
<h2 class="anchored" data-anchor-id="demonstration-that-beta-does-its-job">Demonstration That <span class="math inline">\(\beta\)</span> Does Its Job</h2>
<p>I have said that <span class="math inline">\(\beta\)</span> specifies the ratio of recall to precision where you would be equally happy with a one-unit increase in precision or a one-unit increase in precision. Let’s demonstrate this result. Stated precisely, it says that the partial derivatives of <span class="math inline">\(G_{\beta,\rho}\)</span> with respect to <span class="math inline">\(R\)</span> and <span class="math inline">\(P\)</span> are equal when <span class="math inline">\(R/P=\beta\)</span>.</p>
<p><span class="math display">\[
\begin{align*}
\frac{\partial}{\partial R} \left[ \left( \frac{1}{1 + \beta^{\rho}} \left( \beta^{\rho} P^{\rho+1} + R^{\rho+1} \right) \right)^{\frac{1}{\rho+1}} \right]
&amp;=
\frac{\partial}{\partial P} \left[ \left( \frac{1}{1 + \beta^{\rho}} \left( \beta^{\rho} P^{\rho+1} + R^{\rho+1} \right) \right)^{\frac{1}{\rho+1}} \right] \\
\frac{1}{\rho+1} \left[ \frac{1}{1 + \beta^{\rho}} \left( \beta^{\rho} P^{\rho+1} + R^{\rho+1} \right) \right]^{\frac{-\rho}{\rho+1}} \left(\frac{\rho+1}{1 + \beta^{\rho}} \right) R^{\rho}
&amp;=
\frac{1}{\rho+1} \left[ \frac{1}{1 + \beta^{\rho}} \left( \beta^{\rho} P^{\rho+1} + R^{\rho+1} \right) \right]^{\frac{-\rho}{\rho+1}} \left( \frac{\rho+1}{1 + \beta^{\rho}} \right) \beta^{\rho} P^{\rho} \\
R^{\rho} &amp;= \beta^{\rho} P^{\rho} \\
\beta &amp;= \frac{R}{P}
\end{align*}
\]</span></p>
<p>This derivation works unless <span class="math inline">\(\rho=0\)</span>. In that case we are dealing with a weighted arithmetic mean, so there are no values of <span class="math inline">\(R\)</span> and <span class="math inline">\(P\)</span> at which we are willing to trade one unit of recall for one unit of precision unless <span class="math inline">\(\beta=1\)</span>, and in that case we are willing to do so at every value of <span class="math inline">\(R\)</span> and <span class="math inline">\(P\)</span>.</p>
</section>
<section id="demonstration-that-rho-does-its-job" class="level2">
<h2 class="anchored" data-anchor-id="demonstration-that-rho-does-its-job">Demonstration That <span class="math inline">\(\rho\)</span> Does Its Job</h2>
<p>The “curvature” parameter <span class="math inline">\(\rho\)</span> characterizes how our concern for recall relative to precision changes as we move away from the neutral line <span class="math inline">\(R/P=\beta\)</span>. More precisely, it characterizes the rate of change of the slope of the tangent line to a level curve <span class="math inline">\(R/P=\beta\)</span> along the direction of that tangent line.</p>
<p>The slope of the tangent line to a level curve of the weighted power mean is given by the ratio of the partial derivative of the weighted power mean with respect to recall to its partial derivative with respect to precision. Call this quantity <span class="math inline">\(S\)</span>. Then we have</p>
<p><span class="math display">\[
\begin{align*}
S &amp;= -\frac{\frac{\partial}{\partial R} \left[ \frac{1}{1 + \beta^{\rho}} \left( \beta^{\rho} P^{\rho+1} + R^{\rho+1} \right) \right]^{\frac{1}{\rho+1}}}
{\frac{\partial}{\partial P} \left[ \frac{1}{1 + \beta^{\rho}} \left( \beta^{\rho} P^{\rho+1} + R^{\rho+1} \right) \right]^{\frac{1}{\rho+1}}} \\
&amp;= -\frac{\frac{1}{\rho+1} \left[ \frac{1}{1 + \beta^{\rho}} \left( \beta^{\rho} P^{\rho+1} + R^{\rho+1} \right) \right]^{\frac{-\rho}{\rho+1}} \frac{\rho+1}{1 + \beta^{\rho}} R^{\rho} }
{\frac{1}{\rho+1} \left[ \frac{1}{1 + \beta^{\rho}} \left( \beta^{\rho} P^{\rho+1} + R^{\rho+1} \right) \right]^{\frac{-\rho}{\rho+1}} \frac{\rho+1}{1 + \beta^{\rho}} \beta^{\rho} P^{\rho}} \\
&amp;= -\left( \frac{R}{\beta P} \right)^{\rho}
\end{align*}
\]</span></p>
<p>We want to characterize how the slope <span class="math inline">\(S\)</span> of the tangent line to the level curve of the metric changes along that tangent line, so we need to evaluate the partial derivative of <span class="math inline">\(S\)</span> with respect to <span class="math inline">\(t=R-P\)</span>. By the chain rule,</p>
<p><span class="math display">\[
\begin{align*}
\frac{\partial S}{\partial t} &amp;= \frac{\partial S}{\partial R} \frac{\partial R}{\partial t} + \frac{\partial S}{\partial P} \frac{\partial P}{\partial t} \\
&amp;= \frac{\partial}{\partial R} \left[ -\left( \frac{R}{\beta P} \right)^{\rho} \right] \frac{\partial}{\partial t} (t + P)
+ \frac{\partial}{\partial P} \left[ -\left( \frac{R}{\beta P} \right)^{\rho} \right] \frac{\partial}{\partial t} (R - t) \\
&amp;= -\rho \left( \frac{R}{\beta P} \right)^{\rho} R^{-1} - \rho \left( \frac{R}{\beta P} \right)^{\rho} P^{-1} \\
&amp;= -\rho \left( \frac{R}{\beta P} \right)^{\rho} \left( R^{-1} + P^{-1} \right)
\end{align*}
\]</span></p>
<p>So</p>
<p><span class="math display">\[
\left. \frac{\partial S}{\partial t} \right|_{\beta = \frac{R}{P}} = -\rho \left( R^{-1} + P^{-1} \right)
\]</span></p>
<p>Thus, for a given <span class="math inline">\(R\)</span> and <span class="math inline">\(P\)</span> along the line <span class="math inline">\(\beta=R/P\)</span>, the curvature of the level curve along the direction of its tangent line is proportional to <span class="math inline">\(\rho\)</span>.</p>
</section>
</section>
<section id="acknowledgements" class="level1">
<h1>Acknowledgements</h1>
<p>Thanks to Nicole Carlson, Morgan Cundiff, and the rest of the ShopRunner data science team for comments on earlier versions of this material.</p>
<p><em>Originally published at <a href="https://medium.com/@gsganden/https-medium-com-shoprunner-evaluating-classification-models-4-6dfeffd9cfaf">medium.com</a></em></p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>