<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Greg Gandenberger">
<meta name="dcterms.date" content="2019-11-26">

<title>blog - Evaluating Classification Models, Part 3: \(F_eta\) and Other Weighted Pythagorean Means of Precision and Recall</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Evaluating Classification Models, Part 3: <span class="math inline">\(F_eta\)</span> and Other Weighted Pythagorean Means of Precision and Recall</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">ml</div>
                <div class="quarto-category">evals</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Greg Gandenberger </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 26, 2019</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="level_curves.webp" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">There are many ways to combine precision and recall into an overall measure of model performance. Which one should you use?</figcaption><p></p>
</figure>
</div>
<p>This post is part of a series on evaluating classification models:</p>
<ul>
<li><a href="../../posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html"><strong>Part 1: Weighing False Positives Against False Negatives</strong></a> explains why we need systematic ways to evaluate classification models.</li>
<li><a href="../../posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html"><strong>Part 2: The Sufficiency of Precision and Recall</strong></a> explains why <em>precision</em> and <em>recall</em> are sufficient for evaluating classification models in typical cases.</li>
<li><a href="../../posts/2019-11-26_evaluating-classifiers-pt3/evaluating_classifiers_pt_3.html"><strong>Part 3: <span class="math inline">\(F_\beta\)</span> and Other Weighted Pythagorean Means of Precision and Recall</strong></a> explains what patterns of preferences are encoded by the <em>Pythagorean means</em> of precision and recall. This class of metrics includes the popular <span class="math inline">\(F_\beta\)</span> family, among others.</li>
<li><a href="../../posts/2019-12-02_evaluating-classifiers-pt4/evaluating_classifiers_pt_4.html"><strong>Part 4: Weighted Power Means of Precision and Recall</strong></a> generalizes beyond the Pythagorean means to the broader class of <em>weighted power means</em> of precision and recall.</li>
</ul>
<p>This series differs from other discussions of evaluation metrics for classification models in that it aims to provide a <strong>systematic perspective</strong>. Rather than providing a laundry list of individual metrics, it situates those metrics within a fairly comprehensive family and explains how you can choose a member of that family that is appropriate for your use case.</p>
<p>This post explains how the three weighted “Pythagorean means” (arithmetic, geometric, and harmonic) of precision and recall encode preferences over models.</p>
<section id="an-example" class="level1">
<h1>An Example</h1>
<p>Suppose we build two different models, and one has better precision while the other has better recall.</p>
<p><strong>Model A</strong>:</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>Predicted No</th>
<th>Predicted Yes</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Actual No</td>
<td>90</td>
<td>10</td>
<td>100</td>
</tr>
<tr class="even">
<td>Actual Yes</td>
<td>1</td>
<td>99</td>
<td>100</td>
</tr>
<tr class="odd">
<td>Total</td>
<td>91</td>
<td>109</td>
<td>200</td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
\text{Precision} = \frac{|\text{True Positive}|}{|\text{True Positive}| + |\text{False Positive}|} = \frac{90}{99 + 10} = 90.8\%
\]</span></p>
<p><span class="math display">\[
\text{Recall} = \frac{|\text{True Positive}|}{|\text{True Positive}| + |\text{False Negative}|} = \frac{99}{99 + 1} = 99.0\%
\]</span></p>
<p><strong>Model B</strong>:</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>Predicted No</th>
<th>Predicted Yes</th>
<th>Total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Actual No</td>
<td>91</td>
<td>9</td>
<td>100</td>
</tr>
<tr class="even">
<td>Actual Yes</td>
<td>3</td>
<td>97</td>
<td>100</td>
</tr>
<tr class="odd">
<td>Total</td>
<td>94</td>
<td>106</td>
<td>200</td>
</tr>
</tbody>
</table>
<p><span class="math display">\[
\text{Precision} = \frac{|\text{True Positive}|}{|\text{True Positive}| + |\text{False Positive}|} = \frac{91}{91 + 9} = 91.5\%
\]</span></p>
<p><span class="math display">\[
\text{Recall} = \frac{|\text{True Positive}|}{|\text{True Positive}| + |\text{False Negative}|} = \frac{97}{97 + 3} = 97.0\%
\]</span></p>
<p>To choose between these models, we need to decide whether the gain from <span class="math inline">\(90.8\%\)</span> precision to <span class="math inline">\(91.5\%\)</span> precision that we get by going from Model A to Model B is enough to offset a loss from <span class="math inline">\(99\%\)</span> recall to <span class="math inline">\(97\%\)</span> recall. <strong>We need some way to combine precision and recall into a single evaluation metric.</strong></p>
</section>
<section id="the-weighted-arithmetic-mean-a_gamma" class="level1">
<h1>The Weighted Arithmetic Mean <span class="math inline">\(A_\gamma\)</span></h1>
<p>When considering how to combine precision and recall to produce an overall evaluation metric, a natural first thought is simply to average them:</p>
<p><span class="math display">\[
\text{Simple Arithmetic Mean}: \frac{1}{2}(P + R)
\]</span></p>
<p>where <span class="math inline">\(P\)</span> is precision and <span class="math inline">\(R\)</span> is recall.</p>
<p>In the scenario above, going from Model A to Model B increases precision by <span class="math inline">\(0.7\%\)</span> but decreases recall by <span class="math inline">\(2.0\%\)</span>. A simple arithmetic mean of precision and recall would tell us not to make this trade. It implies that we are willing to trade a decrease of <span class="math inline">\(2.0\%\)</span> in recall only for an increase of <span class="math inline">\(2.0\%\)</span> or more in precision. More generally, <strong>a simple arithmetic mean says that we are willing to trade one unit of recall only for one or more units of precision, and vice versa, regardless of the current precision and recall.</strong></p>
<p>We can visualize this pattern of preferences using a set of “level curves.” In this plot, the simple arithmetic mean would tell us that two models that fall on the same curve are equally good, and that a model that lies on a curve up and to the right of another model is better than that other model.</p>
<iframe src="plots/simple_arithmetic_mean_curves.html" width="100%" height="550" frameborder="0">
</iframe>
<p>Another possible pattern of preferences is to be just willing to trade one unit of precision for <span class="math inline">\(\gamma\)</span> units of recall for some <span class="math inline">\(\gamma &gt; 0\)</span>. This pattern is captured by a weighted arithmetic mean:</p>
<p><span class="math display">\[
\text{Weighted Arithmetic Mean} (\gamma): \frac{1}{1 + \gamma}\left(\gamma P + R\right)
\]</span></p>
<p>The level curves for this pattern of preferences are straight lines with slope <span class="math inline">\(-\gamma\)</span>:</p>
<iframe src="plots/weighted_arithmetic_mean_curves.html" width="100%" height="550" frameborder="0">
</iframe>
<p>To help wrap your head around how to read the plot above, move the slider all the way to the right (<span class="math inline">\(\gamma=8\)</span>). Observe that the level curves become nearly horizontal. Pick a point on one of the lines, such as (<span class="math inline">\(x=.48\)</span>, <span class="math inline">\(y=.84\)</span>), shown up close below. That point represents a model with <span class="math inline">\(48\%\)</span> recall and <span class="math inline">\(84\%\)</span> precision. If that model lost one percentage point of precision (corresponding to a shift down one unit to <span class="math inline">\(y=.83\)</span>), it would have to gain <span class="math inline">\(\gamma=8\)</span> points of recall (corresponding to a shift right eight units to <span class="math inline">\(x=.56\)</span>) to get back to the level curve it started on. In other words, <strong>the plot says that we would require 8 units of recall in order to give up one unit of precision, indicating that we care much more about precision than recall.</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="plots/weighted_arithmetic_mean_point.webp" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">The slope of the level curve at a given point tells us how many points of recall (<span class="math inline">\(\gamma\)</span>) we would require in exchange for one point of precision for a model with the recall and precision represented by that point.</figcaption><p></p>
</figure>
</div>
</section>
<section id="the-weighted-harmonic-mean-f_beta" class="level1">
<h1>The Weighted Harmonic Mean <span class="math inline">\(F_\beta\)</span></h1>
<p><strong>In some cases, the arithmetic mean is inappropriate because the amount of recall we would require in order to give up one unit of precision is not fixed;</strong> instead, it depends on the current recall and precision values. One option in those cases is to use a weighted <em>harmonic</em> mean of precision and recall instead of a weighted arithmetic. The result is the popular <span class="math inline">\(F_\beta\)</span> score.</p>
<p>The weighted harmonic mean is the inverse of the arithmetic mean of the inverses:</p>
<p><span class="math display">\[
\text{Weighted Harmonic Mean} (\gamma): \left(\frac{1}{1 + \gamma}\left(\gamma P^{-1} + R^{-1}\right)\right)^{-1}
\]</span></p>
<p>It is convenient to use the parameter <span class="math inline">\(\beta\)</span> such that <span class="math inline">\(\beta^2 = \frac{1}{\gamma}\)</span> in place of <span class="math inline">\(\gamma\)</span>, which results in the following equivalent expression:</p>
<p><span class="math display">\[
\text{Weighted Harmonic Mean} (\beta): \left(\frac{1}{1 + \beta^2}\left(P^{-1} + \beta^2R^{-1}\right)\right)^{-1}
\]</span></p>
<p>With a weighted arithmetic mean <span class="math inline">\(A_\gamma\)</span>, the relative importance of precision and recall is fixed: that metric says that we are willing to trade one unit of precision for <span class="math inline">\(\gamma\)</span> or more units of recall <em>regardless of the current precision and recall</em>. <strong>With a weighted harmonic mean <span class="math inline">\(F_\beta\)</span>, by contrast, the relative importance of precision and recall depends on their current values.</strong> For instance, suppose that recall is <span class="math inline">\(80\%\)</span>. If precision is <span class="math inline">\(98\%\)</span>, then you might value a <span class="math inline">\(1\%\)</span> increase in recall more than a <span class="math inline">\(1\%\)</span> increase in precision, whereas if precision is <span class="math inline">\(10\%\)</span>, then you might care more about increasing precision.</p>
<p>A metric that places more importance on recall when recall is low compared to precision and vice versa has level curves that curve upward:</p>
<iframe src="plots/weighted_harmonic_mean_curves.html" width="100%" height="550" frameborder="0">
</iframe>
<p>One way to think about the harmonic mean is that <strong>it deviates from the arithmetic mean in the direction of the minimum</strong>, as we will see more clearly when we discuss weighted power means in the next post in this series. The weighted harmonic mean of a set of numbers with a given weighting <span class="math inline">\(\gamma\)</span> is always less than the arithmetic mean with the same <span class="math inline">\(\gamma\)</span> (unless all of the numbers are the same, in which case the arithmetic and harmonic means are equal.)</p>
<p>One benefit of using <span class="math inline">\(\beta\)</span> instead of <span class="math inline">\(\gamma\)</span> is that <strong><span class="math inline">\(\beta\)</span> indicates how much we care about recall relative to precision</strong>. More precisely, <span class="math inline">\(\beta\)</span> is the ratio of recall to precision where the harmonic mean places equal value on a one-unit increase in recall and a one-unit increase in precision (in the limit as the size of the unit goes to zero). <strong>If recall is more important to you than precision, for instance, then you would set <span class="math inline">\(\beta\)</span> greater than one</strong> because you would rather have a one-unit increase in recall than a one-unit increase in precision unless recall is already high compared to precision.</p>
<p>For instance, <span class="math inline">\(\beta=3\)</span> means that if precision is <span class="math inline">\(25\%\)</span>, then you would rather have one more unit of precision than one more unit of recall if recall is above <span class="math inline">\(75\%\)</span>, but you would rather have the recall if recall is below <span class="math inline">\(75\%\)</span> (because <span class="math inline">\(75\%/25\%=3\)</span>).</p>
<p><strong>You narrow down the range of <span class="math inline">\(\beta\)</span> values that are appropriate for a particular application by getting stakeholder feedback on different combinations of precision and recall values.</strong> For instance, at one point we had a model at ShopRunner that we had tuned for <span class="math inline">\(F_\beta\)</span> with <span class="math inline">\(\beta=.3\)</span>, meaning that we cared about recall about <span class="math inline">\(30\%\)</span> as much as we cared about precision. That model classified products with <span class="math inline">\(90\%\)</span> precision and <span class="math inline">\(36\%\)</span> recall. When we showed those results to our primary stakeholder, she expressed more concern about the <span class="math inline">\(36\%\)</span> recall than the <span class="math inline">\(90\%\)</span> precision. <strong>Assuming that <span class="math inline">\(F_\beta\)</span> captured the overall shape of her preferences, this feedback entailed that her <span class="math inline">\(\beta\)</span> was greater than <span class="math inline">\(36\%/90\%=.4\)</span>.</strong> After further discussion, we started tuning for <span class="math inline">\(F_\beta\)</span> with <span class="math inline">\(\beta=.5\)</span> instead of <span class="math inline">\(.3\)</span>.</p>
<p>In the plot below, the thick black line indicates the set of models where you would be equally happy with one more point of precision or one more point of recall. Above the line, you would rather have one point of recall. Below the line, you would rather have one point of precision. Larger values of <span class="math inline">\(\beta\)</span> mean that you care more about recall overall, so there is more area below the line.</p>
<iframe src="plots/weighted_harmonic_mean_curves_w_line.html" width="100%" height="550" frameborder="0">
</iframe>
<p>In these plots, the number of units of recall that you require in exchange for one unit of precision when recall is <span class="math inline">\(R\)</span> and precision is <span class="math inline">\(P\)</span> is given by the slope of the tangent line to the level curve that passes through (<span class="math inline">\(x=R, y=P\)</span>). For instance, move the slider all the way to the right (<span class="math inline">\(\beta=8\)</span>). Exactly on the black line, the tangent to each level curve has slope <span class="math inline">\(-1/1\)</span>, indicating that you would exchange one unit of precision for one unit of recall, as shown below.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="plots/weighted_harmonic_mean_point.webp" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Along the black line that passes through the points where <span class="math inline">\(\text{Recall}/\text{Precision}=\beta\)</span>, the tangent to a level curve has slope <span class="math inline">\(-1/1\)</span>, meaning that you would require one unit of recall in exchange for one unit of precision (in the limit as the size of the unit goes to zero).</figcaption><p></p>
</figure>
</div>
<p>Below the black line, where precision is high compared to recall, the tangent to each level curve has slope <span class="math inline">\(-1/\delta\)</span> for some <span class="math inline">\(\delta&gt;1\)</span>, indicating that you would require more than one unit of recall in exchange for one unit of precision, as shown below. In other words, with a harmonic mean, when precision is high enough compared to recall, you care about improving recall more than you care about improving precision. This region of the plot is large in this case because <span class="math inline">\(\beta&gt;1\)</span>, indicating that we care about recall more than precision overall.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="plots/weighted_harmonic_mean_point2.webp" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Below the black line that passes through the points where <span class="math inline">\(\text{Recall}/\text{Precision}=\beta\)</span>, the tangent to a level curve has slope <span class="math inline">\(-1/\delta\)</span> for some <span class="math inline">\(\delta&gt;1\)</span>, meaning that you would require more than one unit of recall in exchange for one unit of precision.</figcaption><p></p>
</figure>
</div>
<p>Above the black line, where recall is high compared to precision, the tangent to each level curve has slope <span class="math inline">\(-1/\delta\)</span> for some <span class="math inline">\(\delta&lt;1\)</span>, indicating that you would require less than one unit of recall in exchange for one unit of precision, as shown below. In other words, with a harmonic mean, when recall is high enough compared to precision, you care about improving precision more than you care about improving recall. This region of the plot is small in this case because <span class="math inline">\(\beta&gt;1\)</span>, indicating that we care about recall more than precision overall.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="plots/weighted_harmonic_mean_point3.webp" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Below the black line that passes through the points where <span class="math inline">\(\text{Recall}/\text{Precision}=\beta\)</span>, the tangent to a level curve has slope <span class="math inline">\(-1/\delta\)</span> for some <span class="math inline">\(\delta&lt;1\)</span>, meaning that you would require less than one unit of recall in exchange for one unit of precision.</figcaption><p></p>
</figure>
</div>
<p>For the arithmetic mean, unlike the harmonic mean just discussed, the level curves are straight lines. As a result, the tangent to a level curve is just the level curve itself. Moreover, all of the level curves have the same slope <span class="math inline">\(-1/\gamma\)</span>, corresponding to the fact that with an arithmetic mean you are willing to trade one unit of precision for <span class="math inline">\(\gamma\)</span> or more units of recall for every level of precision and recall.</p>
</section>
<section id="why-would-you-use-a-non-arithmetic-mean" class="level1">
<h1>Why Would You Use a Non-Arithmetic Mean?</h1>
<p>You might wonder whether a harmonic mean of precision and recall is ever appropriate. The author of an <a href="https://druce.ai/2019/10/understanding-classification-thresholds-using-isocurves">excellent recent blog post</a> suggested that it isn’t, writing that they had “never encountered a business problem where a real-life cost function of false positives and false negatives is a harmonic mean.” I claim on the contrary that <strong>a harmonic mean or something similar is often appropriate, in scenarios where the value of the algorithm depends on the <em>overall impression</em> created by its outputs</strong> rather than on separate effects of its individual predictions.</p>
<p><strong>An arithmetic mean tends to be appropriate for diagnostic problems.</strong> For instance, in medical testing a false positive can cause unnecessary stress and expense, while a false negative can cause worse health outcomes because of a missed opportunity at early intervention. To a first approximation, those costs are specific to each patient and do not depend on how many other false positives or false negatives the test generates. As a result, we should be willing to trade precision for recall at the same rate regardless of the current precision and recall, which makes an arithmetic mean appropriate.</p>
<p><strong>By contrast, a harmonic mean tends to be appropriate for information retrieval problems.</strong> For instance, consider the model we discussed in Part 1 of this series that identifies whether a product is a dress or not for an app that sells dresses. Suppose that the pool of products being classified is enormous — every dress for sale on the internet, if you like. Then if recall is quite high, say <span class="math inline">\(98\%\)</span>, then another point of recall doesn’t mean much: someone who is looking for a dress will already be able to find one that they love. If precision is low in this scenario, then it would be better to focus on improving precision. Whereas if precision is very high, say <span class="math inline">\(98\%\)</span>, then encountering a product that should not be in the app because it is not a dress is a rare experience that users will plausibly overlook. If recall is low in that scenario, so that users are not always able to find the perfect dress even though it is out there somewhere, then it would be better to focus on recall. <strong>An arithmetic mean cannot capture this pattern of preferences, while a harmonic mean can.</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="district.webp" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">If we had a lot of non-dresses in the dress feed of ShopRunner’s District app, then we would prioritize increasing precision for that category. By contrast, when precision is high we care more about increasing recall so that we can offer more dress options. An arithmetic mean of precision and recall cannot capture this situation, but a harmonic mean can.</figcaption><p></p>
</figure>
</div>
<p><strong>That being said, a harmonic mean is not the only option that captures this general pattern of preferences.</strong> For instance, a <strong>geometric mean</strong> also puts more weight on precision or recall the lower it is, but to a smaller degree than a harmonic mean.</p>
</section>
<section id="the-weighted-geometric-mean-g_gamma" class="level1">
<h1>The Weighted Geometric Mean <span class="math inline">\(G_\gamma\)</span></h1>
<p><strong>The geometric mean is intermediate between the harmonic mean and the arithmetic mean:</strong> unless all of the inputs are equal, a weighted geometric mean is strictly greater than the corresponding weighted harmonic mean and strictly less than the corresponding arithmetic mean. (If the inputs are all equal, then all three means are equal to their common value.)</p>
<p>The simple geometric mean of two numbers can be calculated by multiplying them and then taking the square root of the result:</p>
<p><span class="math display">\[
\text{Simple Geometric Mean}: \left(PR\right)^{1/2}
\]</span></p>
<p>We can incorporate weights by taking precision to the power <span class="math inline">\(\gamma\)</span> before multiplying and taking the result to the power <span class="math inline">\(1/(1+\gamma)\)</span>:</p>
<p><span class="math display">\[
\text{Weighted Geometric Mean} (\gamma): \left(P^\gamma R\right)^{1/(1+\gamma)}
\]</span></p>
<p>We can change this expression so that the parameter that controls the weighting has the same interpretation as <span class="math inline">\(\beta\)</span> in the weighted harmonic mean <span class="math inline">\(F_\beta\)</span>. We will use the same parameter name “<span class="math inline">\(\beta\)</span>,” but in this case <span class="math inline">\(\beta=1/\gamma\)</span>:</p>
<p><span class="math display">\[
\text{Weighted Geometric Mean} (\beta): \left(P R^\beta\right)^{1/(1+\beta)}
\]</span></p>
<p>Here again <span class="math inline">\(\beta\)</span> is the ratio of recall to precision at which the metric responds equally to an increase in precision and an increase in recall, so that higher <span class="math inline">\(\beta\)</span> corresponds to placing more importance on recall relative to precision.</p>
<p>The level curves for the weighted geometric mean curve upward like those for the weighted harmonic mean, but to a smaller degree:</p>
<iframe src="plots/weighted_geometric_mean_curves.html" width="100%" height="550" frameborder="0">
</iframe>
<p>The weighted geometric mean deviates from the weighted arithmetic mean in the direction of the minimum, again like the harmonic mean but to a smaller degree.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="level_curves.webp" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">The harmonic mean deviates from the arithmetic mean in the direction of the minimum, so it is more sensitive to whichever is smaller between the precision and the recall. The geometric mean has the same property to a smaller degree.</figcaption><p></p>
</figure>
</div>
</section>
<section id="which-mean-should-you-use" class="level1">
<h1>Which Mean Should You Use?</h1>
<p>When each false positive or false negative has roughly the same expected cost regardless of how many false positives or false negatives the model generates overall, you should use an arithmetic mean of precision and recall to evaluate a model. Models used for medical diagnosis often have this property.</p>
<p>But when what matters is the <strong>overall impression</strong> that the model’s outputs create, you might use something like a geometric or harmonic mean that prioritizes whichever of precision or recall is lagging behind. Models used for information retrieval often have this property.</p>
<p>Subsequent posts in this series will situate the arithmetic, geometric, and harmonic means within a larger family of metrics and provide additional guidance on how to choose among them.</p>
</section>
<section id="acknowledgements" class="level1">
<h1>Acknowledgements</h1>
<p>Thanks to Nicole Carlson, Morgan Cundiff, and the rest of the ShopRunner data science team for comments on earlier versions of this material.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>