[
  {
    "objectID": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html",
    "href": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html",
    "title": "Parameter-Efficient Fine-Tuning Illustrated with Regression Tasks",
    "section": "",
    "text": "LLMS and other foundation models are capable of accomplishing a wide range of tasks. However, we may have a task that they do not perform well off the shelf. In principle, we can address this problem by fine tuning a model for that task. However, fine tuning foundation models is extremely expensive.\nParameter-efficient fine tuning (PEFT) is meant to address this problem. Rather than adjusting all of the weights of a model, it adds relatively small adapters to the model and trains those adapters on new data.\nThis post implements a few parameter efficient fine tuning techniques: LORA, DORA, and RS-LORA. It illustrates these methods on a simple regression problem in which we adapt a model that has been trained on a quadratic data set to a cubic data set. These polynomial fitting problems are many orders of magnitude simpler than language modeling, so the way these PEFT methods behave on them may not tell us much about how they behave when applied to foundation models. However, the illustrations do illustrate the general concepts of full and parameter-efficient fine tuning and help confirm that the methods are working as expected.\nThe LoRA and DoRA are implementations adapted from Sebastian Raschka’s Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch. The visualization methods are adapted from Jeremy Howard’s FastAI v3 Lesson 2: SGD. Both were published under the Apache License 2.0."
  },
  {
    "objectID": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html#reproduce-lora-results",
    "href": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html#reproduce-lora-results",
    "title": "Parameter-Efficient Fine-Tuning Illustrated with Regression Tasks",
    "section": "Reproduce LoRA results",
    "text": "Reproduce LoRA results\nLet’s implement rsLoRA, but initially adjust \\(\\alpha\\) to get the same scaling factor as before, so that we should get the same results as with LoRA.\n\ntorch.manual_seed(\n    678\n)  # resetting seed so we get the same LoRA weight initializations as before\nrslora_model = create_lora_model(\n    model,\n    lora_layer_indices=[2],\n    lora_layer_class=partial(\n        LinearWithLoRA,\n        rank=RANK,\n        # will give same gamma_r as LoRA above, so should train the same\n        alpha=ALPHA / (RANK ** 1/2),\n        lora_layer_class=RsLoRALayer,\n    ),\n)\nrslora_model\n\nMultilayerPerceptron(\n  (layers): Sequential(\n    (0): Linear(in_features=1, out_features=20, bias=True)\n    (1): ReLU()\n    (2): LinearWithLoRA(\n      (linear): Linear(in_features=20, out_features=20, bias=True)\n      (lora): RsLoRALayer()\n    )\n    (3): ReLU()\n    (4): Linear(in_features=20, out_features=1, bias=True)\n  )\n)\n\n\n\nfor name, param in rslora_model.named_parameters():\n    print(f\"{name}: {param.requires_grad}\")\n\nlayers.0.weight: False\nlayers.0.bias: False\nlayers.2.linear.weight: False\nlayers.2.linear.bias: False\nlayers.2.lora.A: True\nlayers.2.lora.B: True\nlayers.4.weight: False\nlayers.4.bias: False\n\n\n\nanim = create_training_animation(\n    rslora_model,\n    x,\n    y2,\n    torch.optim.Adam(rslora_model.parameters(), lr=LEARNING_RATE),\n)\nHTML(anim.to_html5_video())\n\nInitial Loss: 0.3533\nInitial Loss: 0.3533\nIteration 10, Loss: 0.0761\nIteration 20, Loss: 0.0494\nIteration 30, Loss: 0.0287\nIteration 40, Loss: 0.0241\nIteration 50, Loss: 0.0183\nIteration 60, Loss: 0.0154\nIteration 70, Loss: 0.0138\nIteration 80, Loss: 0.0130\nIteration 90, Loss: 0.0126\nIteration 100, Loss: 0.0124\nIteration 110, Loss: 0.0122\nIteration 120, Loss: 0.0121\nIteration 130, Loss: 0.0119\nIteration 140, Loss: 0.0118\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html#compare-lora-and-rslora-at-extreme-ranks",
    "href": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html#compare-lora-and-rslora-at-extreme-ranks",
    "title": "Parameter-Efficient Fine-Tuning Illustrated with Regression Tasks",
    "section": "Compare LoRA and rsLoRA at Extreme Ranks",
    "text": "Compare LoRA and rsLoRA at Extreme Ranks\nNow let’s keep \\(\\alpha\\) the same and see how LoRA and rsLoRA perform at extreme ranks. I would not necessarily expect rsLoRA to perform better than LoRA at extreme ranks in this simple case, but at least we can illustrate the type of situation in which rsLoRA is expected to perform better.\n\nLoRA at Low Rank\n\nlora_model = create_lora_model(\n    model,\n    lora_layer_indices=[2],\n    lora_layer_class=partial(LinearWithLoRAMerged, rank=1, alpha=ALPHA),\n)\nanim = create_training_animation(\n    lora_model,\n    x,\n    y2,\n    torch.optim.Adam(lora_model.parameters(), lr=LEARNING_RATE),\n)\nHTML(anim.to_html5_video())\n\nInitial Loss: 0.3533\nInitial Loss: 0.3533\nIteration 10, Loss: 0.0739\nIteration 20, Loss: 0.0678\nIteration 30, Loss: 0.0291\nIteration 40, Loss: 0.0260\nIteration 50, Loss: 0.0236\nIteration 60, Loss: 0.0209\nIteration 70, Loss: 0.0195\nIteration 80, Loss: 0.0186\nIteration 90, Loss: 0.0177\nIteration 100, Loss: 0.0170\nIteration 110, Loss: 0.0167\nIteration 120, Loss: 0.0164\nIteration 130, Loss: 0.0163\nIteration 140, Loss: 0.0162\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\n\nLoRA at High Rank\n\nlora_model = create_lora_model(\n    model,\n    lora_layer_indices=[2],\n    lora_layer_class=partial(LinearWithLoRAMerged, rank=20, alpha=ALPHA),\n)\nanim = create_training_animation(\n    lora_model,\n    x,\n    y2,\n    torch.optim.Adam(lora_model.parameters(), lr=LEARNING_RATE),\n)\nHTML(anim.to_html5_video())\n\nInitial Loss: 0.3533\nInitial Loss: 0.3533\nIteration 10, Loss: 0.0561\nIteration 20, Loss: 0.0304\nIteration 30, Loss: 0.0235\nIteration 40, Loss: 0.0161\nIteration 50, Loss: 0.0137\nIteration 60, Loss: 0.0130\nIteration 70, Loss: 0.0125\nIteration 80, Loss: 0.0121\nIteration 90, Loss: 0.0118\nIteration 100, Loss: 0.0116\nIteration 110, Loss: 0.0115\nIteration 120, Loss: 0.0113\nIteration 130, Loss: 0.0113\nIteration 140, Loss: 0.0112\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\n\nrsLoRA at Low Rank\n\nrslora_model = create_lora_model(\n    model,\n    lora_layer_indices=[2],\n    lora_layer_class=partial(\n        LinearWithLoRA,\n        rank=1,\n        alpha=ALPHA,\n        lora_layer_class=RsLoRALayer,\n    ),\n)\nanim = create_training_animation(\n    rslora_model,\n    x,\n    y2,\n    torch.optim.Adam(rslora_model.parameters(), lr=LEARNING_RATE),\n)\nHTML(anim.to_html5_video())\n\nInitial Loss: 0.3533\nInitial Loss: 0.3533\nIteration 10, Loss: 0.1752\nIteration 20, Loss: 0.1717\nIteration 30, Loss: 0.1737\nIteration 40, Loss: 0.1724\nIteration 50, Loss: 0.1708\nIteration 60, Loss: 0.1691\nIteration 70, Loss: 0.1666\nIteration 80, Loss: 0.1623\nIteration 90, Loss: 0.1540\nIteration 100, Loss: 0.0966\nIteration 110, Loss: 0.0926\nIteration 120, Loss: 0.0903\nIteration 130, Loss: 0.0896\nIteration 140, Loss: 0.0884\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\n\nrsLoRA at High Rank\n\nrslora_model = create_lora_model(\n    model,\n    lora_layer_indices=[2],\n    lora_layer_class=partial(\n        LinearWithLoRA,\n        rank=20,\n        alpha=ALPHA,\n        lora_layer_class=RsLoRALayer,\n    ),\n)\nanim = create_training_animation(\n    rslora_model,\n    x,\n    y2,\n    torch.optim.Adam(rslora_model.parameters(), lr=LEARNING_RATE),\n)\nHTML(anim.to_html5_video())\n\nInitial Loss: 0.3533\nInitial Loss: 0.3533\nIteration 10, Loss: 0.0635\nIteration 20, Loss: 0.0302\nIteration 30, Loss: 0.0185\nIteration 40, Loss: 0.0145\nIteration 50, Loss: 0.0129\nIteration 60, Loss: 0.0120\nIteration 70, Loss: 0.0117\nIteration 80, Loss: 0.0114\nIteration 90, Loss: 0.0113\nIteration 100, Loss: 0.0112\nIteration 110, Loss: 0.0111\nIteration 120, Loss: 0.0110\nIteration 130, Loss: 0.0110\nIteration 140, Loss: 0.0109\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Greg Gandenberger",
    "section": "",
    "text": "Parameter-Efficient Fine-Tuning Illustrated with Regression Tasks\n\n\n\n\n\n\n\nml\n\n\nllms\n\n\npeft\n\n\nlora\n\n\ndora\n\n\nrslora\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating Classification Models, Part 1: Weighing False Positives Against False Negatives\n\n\n\n\n\n\n\nml\n\n\nevals\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2019\n\n\nGreg Gandenberger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Machine learning engineer posting as I learn about AI engineering. Previously at Cruise, Argo AI, ShopRunner, General Assembly, and Uptake. Public projects include the file-processing framework Wildebeest, visualization library Model Inspector, and camera trap animal classification service Autofocus."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "",
    "text": "This plot shows level curves for an F₁ score, which is one way to weigh precision against recall in evaluating a classification model.\nThis post is part of a series on evaluating classification models:\nThis series differs from other discussions of evaluation metrics for classification models in that it aims to provide a systematic perspective. Rather than providing a laundry list of individual metrics, it situates those metrics within a fairly comprehensive family and explains how you can choose a member of that family that is appropriate for your use case."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#classifying-products-at-shoprunner",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#classifying-products-at-shoprunner",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "Classifying Products at ShopRunner",
    "text": "Classifying Products at ShopRunner\nShopRunner’s mission is to power extraordinary shopping experiences that connect customers to the brands they love. To accomplish this mission, we need to make it easy for users to find products they want. Many of our efforts toward this end would benefit from being able to classify products within a single product taxonomy. For instance, classifying a particular product as a dress allows us to display it under “Women > Clothing > Dresses” in our District app.\n\n\n\nDresses in ShopRunner’s District app\n\n\nClassifying all of the products across our retailer network into a single taxonomy is challenging. Our partners all use different product taxonomies, often including categories such as “New Arrivals” that do not map cleanly into categories like “Dresses.” As a result, simple rules-based approaches based on the product categories that our partners use are not sufficient. Moreover, classifying each product manually would be too expensive: we have more than 100 partners with millions of total products, many of which turn over several times each year.\nWe are using machine learning to address this challenge. Modern deep learning systems can learn to classify products based on images and text descriptions with accuracy similar to that of manual classification for a small fraction of the cost."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#false-positives-and-false-negatives",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#false-positives-and-false-negatives",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "False Positives and False Negatives",
    "text": "False Positives and False Negatives\nEven with cutting-edge machine learning, no classification system is perfect. For instance, a model that classifies a product as “dress” or “not a dress” might sometimes misidentify a skirt as a dress. We would call that misclassification a false positive for the dress category. In addition, it might sometimes misidentify a dress as “not a dress,” which we would call a false negative for the dress category.\nExamples:\n\n\n\nCorrectly labeling this dress as a dress would be a true positive.\n\n\n\n\n\nCorrectly labeling this skirt as “not a dress” would be a true negative.\n\n\n\n\n\nIncorrectly labeling this skirt as a dress would be a false positive.\n\n\n\n\n\nIncorrectly labeling this dress as “not a dress” would be a false negative. (Yes, it is a dress. Life is pain.)\n\n\nMany machine learning classification models actually produce a probability for a label such as dress. By default, we would typically take a model to be predicting “dress” if its probability for “dress” is at least \\(50\\%\\) and “not a dress” otherwise. But we can use a cutoff other than \\(50\\%\\) if we wish. For instance, we might only classify a product as a dress if its probability for dress is at least \\(70\\%\\).\nUsing \\(70\\%\\) rather than \\(50\\%\\) as our cutoff probability for a positive prediction would change our model’s error rates. Specifically, by causing the model to produce fewer positives, it would reduce the false positive rate (which is good) but increase the false negative rate (which is bad). Part of our job in developing a classification model is to choose cutoff probabilities that strike an appropriate balance between false positives and false negatives.\nFor simplicity, let’s pretend that our District app only sells dresses. (It doesn’t!) Showing a product that isn’t a dress in the app would be potentially off-putting to our users, so the cost of a false positive is significant. On the other hand, a dress that we mistakenly classify as “not a dress” typically will not have much impact as long as we still have a large number of other dresses to choose from. As a result, the cost of a single false positive is generally higher than that of a single false negative, so we should set our cutoff probability higher than \\(50\\%\\)."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#precision-and-recall",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#precision-and-recall",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "Precision and Recall",
    "text": "Precision and Recall\nWe have said that we should set the cutoff probability higher than \\(50\\%\\), but how much higher? To answer this question, we need ways to quantify our model’s performance as we vary the cutoff probability. There are many metrics that we could use for this purpose. I will focus on precision and recall. Part 2 of this series will justify that choice.\nA model’s precision is the accuracy of its positive predictions. For instance, when our model says that something is a dress, how often is it actually a dress?\n\\[\n\\text{Precision} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Positives}|}\n\\]\nwhere e.g. “|True Positives|” is the number of true positive predictions that our model generates.\nRecall is the model’s accuracy on the actual positive class. For instance, when a product is actually a dress, how often does our model say that it is a dress?\n\\[\n\\text{Recall} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Negatives}|}\n\\]\nNow suppose we build two different models, and one has better precision while the other has better recall. (We can typically create such a scenario by changing the cutoff probability for positive predictions.)\nModel A:\n\n\n\n\nPredicted No\nPredicted Yes\nTotal\n\n\n\n\nActual No\n90\n10\n100\n\n\nActual Yes\n1\n99\n100\n\n\nTotal\n91\n109\n200\n\n\n\n\\[\n\\text{Precision} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Positives}|} = \\frac{|99|}{|99| + |10|} = 90.8\\%\n\\]\n\\[\n\\text{Recall} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Negatives}|} = \\frac{|99|}{|99| + |1|} = 99\\%\n\\]\nModel B:\n\n\n\n\nPredicted No\nPredicted Yes\nTotal\n\n\n\n\nActual No\n91\n9\n100\n\n\nActual Yes\n3\n97\n100\n\n\nTotal\n94\n106\n200\n\n\n\n\\[\n\\text{Precision} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Positives}|} = \\frac{|97|}{|97| + |9|} = 91.5\\%\n\\]\n\\[\n\\text{Recall} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Negatives}|} = \\frac{|97|}{|97| + |3|} = 97\\%\n\\]\nTo choose between these models, we need to decide whether the gain from \\(90.8\\%\\) precision to \\(91.5\\%\\) precision that we get by going from Model A to Model B is enough to offset a loss from \\(99\\%\\) recall to \\(97\\%\\) recall."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#why-we-need-a-variety-of-model-evaluation-metrics",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#why-we-need-a-variety-of-model-evaluation-metrics",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "Why We Need a Variety of Model Evaluation Metrics",
    "text": "Why We Need a Variety of Model Evaluation Metrics\nIf we only had to make occasional decisions between pairs of models, then it would be reasonable to make decisions about how to trade off precision against recall on an ad hoc basis. However, we can typically generate dozens of models that all differ in terms of precision and recall with no model dominating all others on both measures, for instance by varying our cutoff probability. As a result, we need a way to combine precision and recall into a single metric that we can use to select the “best” model for a given application.\nA natural way to combine precision and recall is to average them. For instance, using simple averaging in the example above, Model A would get a score of \\(1/2(90.8\\% + 99\\%) = 94.9\\%\\), while Model B would get a score of \\(1/2(91.5\\% + 97\\%) = 94.3\\%\\). However, this simple mean gives equal weight to precision and recall. We have said that it is more important to avoid showing non-dresses in our app than it is to show all of the dresses that we have, so precision is more important to us than recall. Thus, a simple arithmetic mean of precision and recall isn’t the right choice in this application. We should use a weighted average instead.\nMoreover, perhaps we would be willing to give up one point of precision in exchange for one point of recall if precision were very high (say \\(99%\\)) and recall were very low (say \\(10%\\)), whereas if precision and recall were similar then we would not be willing to make that trade. If so, then a weighted arithmetic mean of precision and recall is not the right choice, because a weighted arithmetic mean encodes a willingness to trade one point of recall for a fixed number of points of precision at every level of precision and recall.\nIn that case, we might consider using something like a weighted geometric or harmonic mean rather than a weighted arithmetic mean. Part 3 of this series discusses those options.\nThere are many ways to weigh precision against recall, and which way is most appropriate depends on the problem. Many sources help with the challenge of choosing an evaluation metric for a given problem by providing a laundry list of metrics with a few comments about the characteristics of each one. This series of blog posts aims to provide a more systematic perspective. It culminates in a new family of metrics that generalizes the popular \\(F_\\beta\\) score. This family allows the user to specify not only the overall importance of recall relative to precision but also how that relative importance shifts as recall and precision become increasingly imbalanced."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#appendix",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#appendix",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "Appendix",
    "text": "Appendix\n\nExtending Binary Model Evaluation Metrics to Multiclass Problems\nSo far we have discussed only binary classification problems, such as identifying whether a particular product is or is not a dress. The full problem of placing products within our taxonomy is not a binary classification problem, because there are many possible categories for each item. For instance, we need to say not just whether a given item is a dress or not, but also if it is not a dress, whether it is a skirt, a pair of jeans, and so on. However, we can regard this multiclass problem as a set of binary classification problems (one for each category) and aggregate performance on each of those subproblems to produce an overall measure of model performance. As a result, the discussion in these blog posts is relevant to evaluating multiclass models even though it focuses on binary models.\nFor instance, here is one possible procedure:\n\nCalculate precision and recall for the “dress” category\nCalculate an overall score for the “dress” category by averaging those precision and recall values in one of the ways we will discuss.\nDo likewise for all other categories.\nUse a simple average of the individual category scores as the score for the multiclass model as a whole.\n\nSee the scikit-learn documentation for a discussion of several ways to produce multiclass model evaluation metrics from binary model evaluation metrics.\n\n\nUsing a Single Evaluation Metric\nIt is common to use multiple metrics to measure various aspects of model performance separately. In my view, this approach is helpful for diagnostic purposes — that is, for figuring out where your model is failing and using that information to guide model development. However, it is best to use but one metric for evaluation purposes — that is, for deciding which of a set of models is best and whether it is better than no model.\nThis single evaluation metric can be different for different projects, and it is subject to revision within a project, but for a given project there should be one primary metric at any given time that is used to guide modeling decisions. This approach allows a team developing the model to iterate quickly, without getting bogged down in long discussions about how to weigh different aspects of model performance against one another at every turn. See this lecture from Andrew Ng’s “Structuring Machine Learning Projects” course for more discussion of this issue."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#acknowledgements",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#acknowledgements",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Nicole Carlson, Ali Vanderveld, and the rest of the ShopRunner data science team for comments on earlier versions of this material.\nOriginally published at medium.com"
  }
]