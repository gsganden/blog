<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Greg Gandenberger">
<meta name="dcterms.date" content="2025-03-05">

<title>Blog - Some Best Practices for LLM Math Evaluation</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://hypothes.is/embed.js"></script>


<link rel="stylesheet" href="../../styles.css">
<meta property="og:title" content="Blog - Some Best Practices for LLM Math Evaluation">
<meta property="og:description" content="Evaluating LLM answers to open-ended math questions is hard because LLMs can generate arbitrary text. They might surround their answer with additional information.">
<meta property="og:image" content="https://gandenberger.org/posts/2025-03-05_llm-math-evaluation-best-practices/image.jpg">
<meta property="og:site-name" content="Blog">
<meta name="twitter:title" content="Blog - Some Best Practices for LLM Math Evaluation">
<meta name="twitter:description" content="Evaluating LLM answers to open-ended math questions is hard because LLMs can generate arbitrary text. They might surround their answer with additional information.">
<meta name="twitter:image" content="https://gandenberger.org/posts/2025-03-05_llm-math-evaluation-best-practices/image.jpg">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">Blog</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com"><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Some Best Practices for LLM Math Evaluation</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">ml</div>
                <div class="quarto-category">llms</div>
                <div class="quarto-category">evals</div>
                <div class="quarto-category">math</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Greg Gandenberger </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 5, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="image.jpg" class="img-fluid figure-img" width="350"></p>
<p></p><figcaption class="figure-caption">Image generated by ChatGPT-4o.</figcaption><p></p>
</figure>
</div>
<p>Evaluating LLM answers to open-ended math questions is hard because LLMs can generate arbitrary text. They might surround their answer with additional information. They are not guaranteed to format their answer in any particular way. They might not answer the question at all! Comparing their answer to ground truth requires finding it and then making the comparison in a flexible yet discerning way.</p>
<p><strong>This blog post describes some best practices for evaluating LLM answers to open-ended math questions.</strong> It illustrates these ideas in the context of using <a href="https://github.com/huggingface/math-verify">HuggingFace’s Math-Verify library</a> with <a href="https://github.com/mosaicml/llm-foundry">MosaicML’s LLM Foundry</a>, but the principles apply more broadly. It does NOT address using an LLM as a judge, which is an alternative to the kind of approach described here.</p>
<section id="make-sure-the-model-is-answering-the-question" class="level1">
<h1>1. Make Sure the Model Is Answering the Question</h1>
<p><strong>First, make sure that when you present the model with a question, it is attempting to answer that question</strong> at least most of the time. This point might sound trivial, but the wrong configuration can lead to other types of responses.</p>
<div class="callout-example callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Example
</div>
</div>
<div class="callout-body-container callout-body">
<p>For instance, in my first attempt at a recent project developing a math evaluation procedure, the model was generating responses that looked like this:</p>
<blockquote class="blockquote">
<p>15 cups</p>
<p>B: 25 cups</p>
<p>C: 30 cups</p>
<p>D: 40 cups</p>
<p>E: 50 cups</p>
</blockquote>
<p>It was behaving that way because I had not configured the prompting procedure well, so it was getting inputs like this:</p>
<blockquote class="blockquote">
<p>Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, mealworms and vegetables to help keep them healthy. She gives the chickens their feed in three separate meals. In the morning, she gives her flock of chickens 15 cups of feed. In the afternoon, she gives her chickens another 25 cups of feed. How many cups of feed does she need to give her chickens in the final meal of the day if the size of Wendi’s flock is 20 chickens?</p>
<p>A:</p>
</blockquote>
<p>The “A:” at the end of the prompt was meant to signify “answer”, but the model took it to signify “option A” in a multiple-choice question.</p>
</div>
</div>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>You would get this result by using this <a href="https://github.com/mosaicml/llm-foundry/tree/main/scripts/eval/yamls">LLM Foundry eval configuration</a>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">icl_tasks</span><span class="kw">:</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="kw">-</span><span class="at"> </span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">label</span><span class="kw">:</span><span class="at"> gsm8k</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">dataset_uri</span><span class="kw">:</span><span class="at"> eval/local_data/symbolic_problem_solving/gsm8k.jsonl</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">num_fewshot</span><span class="kw">:</span><span class="at"> </span><span class="kw">[</span><span class="dv">0</span><span class="kw">]</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">icl_task_type</span><span class="kw">:</span><span class="at"> generation_task_with_answers</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">cot_delimiter</span><span class="kw">:</span><span class="at"> </span><span class="st">"The answer is "</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">continuation_delimiter</span><span class="kw">:</span><span class="at"> </span><span class="st">"</span><span class="sc">\n\n</span><span class="st">A:"</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">question_prelimiter</span><span class="kw">:</span><span class="at"> </span><span class="st">""</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">do_normalization</span><span class="kw">:</span><span class="at"> </span><span class="ch">false</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">early_stopping_criteria</span><span class="kw">:</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="st">"</span><span class="sc">\n\n</span><span class="st">"</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="kw">-</span><span class="at"> </span><span class="st">"Question:"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
<section id="a.-use-an-instruction-tuned-model" class="level2">
<h2 class="anchored" data-anchor-id="a.-use-an-instruction-tuned-model">1.a. Use an instruction-tuned model</h2>
<p>Base language models are trained to generate more text rather than specifically to answer questions. <strong>If you are evaluating performance on math questions, you should probably be using an instruction-tuned model.</strong> This point is particularly easy to overlook if you are just grabbing a small model to get started with development. For instance, <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-135M-Instruct"><code>HuggingFaceTB/SmolLM2-135M-Instruct</code></a> would be a better choice for this purpose than <a href="https://huggingface.co/HuggingFaceTB/SmolLM2-135M"><code>HuggingFaceTB/SmolLM2-135M</code></a>.</p>
</section>
<section id="b.-use-few-shot-prompting" class="level2">
<h2 class="anchored" data-anchor-id="b.-use-few-shot-prompting">1.b. Use few-shot prompting</h2>
<p>LLMs are fundamentally pattern-matching machines, so giving them examples of what you want them to do is generally effective.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In LLM Foundry, you can use few-shot prompting by setting e.g.&nbsp;<code>num_fewshot: [5]</code> in the eval configuration, or by using a dataset such as <code>gsm8k_prepended_8shot.jsonl</code> that has few-shot examples built into each question. The example below also uses <code>question_prelimiter: "Question: "</code>.</p>
</div>
</div>
<p>For instance, you can give them input that looks like this:</p>
<blockquote class="blockquote">
<p>Question: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers’ market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers’ market?</p>
<p>A: Janet sells 16 - 3 - 4 = &lt;&lt;16-3-4=9&gt;&gt;9 duck eggs a day. She makes 9 * 2 = $&lt;&lt;9*2=18&gt;&gt;18 every day at the farmer’s market.The answer is 18 Question: A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?</p>
<p>A:It takes 2/2=&lt;&lt;2/2=1&gt;&gt;1 bolt of white fiber So the total amount of fabric is 2+1=&lt;&lt;2+1=3&gt;&gt;3 bolts of fabric The answer is 3 … Question: Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, mealworms and vegetables to help keep them healthy. She gives the chickens their feed in three separate meals. In the morning, she gives her flock of chickens 15 cups of feed. In the afternoon, she gives her chickens another 25 cups of feed. How many cups of feed does she need to give her chickens in the final meal of the day if the size of Wendi’s flock is 20 chickens?</p>
<p>A:</p>
</blockquote>
<p>The model will typically then get the clue that it should be trying to answer the last question.</p>
</section>
<section id="c.-cut-it-off-at-the-question-delimiter" class="level2">
<h2 class="anchored" data-anchor-id="c.-cut-it-off-at-the-question-delimiter">1.c. Cut it off at the question delimiter</h2>
<p>Few-shot prompting might lead to a further problem: the model might think that it should not only answer the last question but continue to generate more question/answer pairs, with responses like this:</p>
<blockquote class="blockquote">
<p>He made 150% of the original price The answer is 150% Question: A snail is at the bottom of a 20-foot well. Each day, it climbs up 3 feet, but at night, it slips back 2 feet. How many days will it take for the snail to reach the top of the well? …</p>
</blockquote>
<p>You can deal with this problem by ignoring everything in its response after the “question prelimiter”, in this case “Question:”.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In LLM Foundry, you can implement this approach by including “Question:” in the <code>early_stopping_criteria</code>.</p>
</div>
</div>
</section>
</section>
<section id="have-the-model-box-its-answer-in-a-way-that-the-metric-recognizes" class="level1">
<h1>2. Have the Model “Box” Its Answer In a Way That the Metric Recognizes</h1>
<p>In the few-shot example above, each answer example ends with a statement of the form “The answer is X”, which encourages the model to give its final answer in this same format. You can then look for this same pattern when extracting the model’s answer.</p>
<div class="callout-note callout callout-style-default callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-caption-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>In LLM Foundry, you can use <code>cot_delimiter</code> for this purpose. It will formulate its few-shot prompts accordingly, and its default metric for the <code>generation_task_with_answers</code> task type appropriate for open-ended math questions will look for that formulation in extracting the model’s answer.</p>
</div>
</div>
<p>HuggingFace’s Math-Verify library uses regular expressions to extract the model’s answer. For LaTeX output, it optionally <a href="https://github.com/huggingface/Math-Verify/blob/main/src/math_verify/parser.py#L261">prioritizes</a> matches that fall inside <code>\boxed{}</code>, and for general mathematical expressions it <a href="https://github.com/huggingface/Math-Verify/blob/main/src/math_verify/parser.py#L110">looks for</a> variations on “the final answer is”. Using those formulations in your prompts will help Math-Verify find the model’s answer!</p>
</section>
<section id="use-a-smart-metric" class="level1">
<h1>3. Use a Smart Metric</h1>
<p>Once your model is answering the intended question, and you are able to find its answer, you still need to compare that answer to the ground truth. Math-Verify has an <a href="https://gandenberger.org/posts/2025-02-22_til_math-verify-2/til_math-verify-2.html">extensive procedure</a> for this purpose. It is thoughtfully designed, but you should still spot check its results on your problem! By contrast, LLM Foundry’s default metric for the <code>generation_task_with_answers</code> task type simply checks whether the extracted answer starts with the ground truth answer (or one of the ground truth answers, if there are multiple), which can lead to false positives, for instance when the answer is “1” and the model’s answer is “100”. In my <a href="https://colab.research.google.com/drive/1nBEhsoVVB2FGsRYtWbUAJVXWbO1RwRA4?usp=sharing">testing</a> so far, Math-Verify has given more reliable results.</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>The practices in this post can help avoid pitfalls in evaluating LLMs on open-ended math questions. However, the most important practice of all is the one that got me to this point: <strong>look at your data!</strong> Check some inputs and outputs and dig into anything that is unclear or looks wrong. Do not blindly trust a library, especially in such a rapidly evolving field.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>