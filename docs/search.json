[
  {
    "objectID": "posts/2019-12-02_evaluating-classifiers-pt4/evaluating_classifiers_pt4.html",
    "href": "posts/2019-12-02_evaluating-classifiers-pt4/evaluating_classifiers_pt4.html",
    "title": "Evaluating Classification Models, Part 4: Weighted Power Means of Precision and Recall",
    "section": "",
    "text": "G\\(_{\\beta,\\rho}\\) generalizes the popular F\\(_\\beta\\) family of classification model evaluation metrics.\nThis post is part of a series on evaluating classification models:\nThis series differs from other discussions of evaluation metrics for classification models in that it aims to provide a systematic perspective. Rather than providing a laundry list of individual metrics, it situates those metrics within a fairly comprehensive family and explains how you can choose a member of that family that is appropriate for your use case.\nThe previous post describes the weighted “Pythagorean” (arithmetic, geometric, and harmonic) means of precision and recall. The arithmetic mean with weight \\(\\gamma\\) says that you are willing to trade one unit of precision for \\(\\gamma\\) or more units of recall, regardless of the current precision and recall. It is appropriate in situations such as medical testing where an individual false positive or false negative is basically unaffected by the total number of false positives and false negatives. For a harmonic mean, by contrast, the number of points of units you require in exchange for one unit of precision depends on the current precision and recall. The same is true for a geometric mean, but to a smaller degree. As a result, a geometric or harmonic mean may be appropriate in settings such as information retrieval where the cost of a false positive or false negative depends on the “big picture” of the model’s outputs.\nThis post discusses the full class of weighted power means of precision and recall, which includes the Pythagorean means as special cases. It may be overkill for practical purposes: an appropriately weighted arithmetic or harmonic mean is good enough for most problems, and using anything else will probably confuse your collaborators. However, it will give you a stronger handle on how the Pythagorean means work as well as additional options for handling unusual situations."
  },
  {
    "objectID": "posts/2019-12-02_evaluating-classifiers-pt4/evaluating_classifiers_pt4.html#demonstration-that-beta-does-its-job",
    "href": "posts/2019-12-02_evaluating-classifiers-pt4/evaluating_classifiers_pt4.html#demonstration-that-beta-does-its-job",
    "title": "Evaluating Classification Models, Part 4: Weighted Power Means of Precision and Recall",
    "section": "Demonstration That \\(\\beta\\) Does Its Job",
    "text": "Demonstration That \\(\\beta\\) Does Its Job\nI have said that \\(\\beta\\) specifies the ratio of recall to precision where you would be equally happy with a one-unit increase in precision or a one-unit increase in precision. Let’s demonstrate this result. Stated precisely, it says that the partial derivatives of \\(G_{\\beta,\\rho}\\) with respect to \\(R\\) and \\(P\\) are equal when \\(R/P=\\beta\\).\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial R} \\left[ \\left( \\frac{1}{1 + \\beta^{\\rho}} \\left( \\beta^{\\rho} P^{\\rho+1} + R^{\\rho+1} \\right) \\right)^{\\frac{1}{\\rho+1}} \\right]\n&=\n\\frac{\\partial}{\\partial P} \\left[ \\left( \\frac{1}{1 + \\beta^{\\rho}} \\left( \\beta^{\\rho} P^{\\rho+1} + R^{\\rho+1} \\right) \\right)^{\\frac{1}{\\rho+1}} \\right] \\\\\n\\frac{1}{\\rho+1} \\left[ \\frac{1}{1 + \\beta^{\\rho}} \\left( \\beta^{\\rho} P^{\\rho+1} + R^{\\rho+1} \\right) \\right]^{\\frac{-\\rho}{\\rho+1}} \\left(\\frac{\\rho+1}{1 + \\beta^{\\rho}} \\right) R^{\\rho}\n&=\n\\frac{1}{\\rho+1} \\left[ \\frac{1}{1 + \\beta^{\\rho}} \\left( \\beta^{\\rho} P^{\\rho+1} + R^{\\rho+1} \\right) \\right]^{\\frac{-\\rho}{\\rho+1}} \\left( \\frac{\\rho+1}{1 + \\beta^{\\rho}} \\right) \\beta^{\\rho} P^{\\rho} \\\\\nR^{\\rho} &= \\beta^{\\rho} P^{\\rho} \\\\\n\\beta &= \\frac{R}{P}\n\\end{align*}\n\\]\nThis derivation works unless \\(\\rho=0\\). In that case we are dealing with a weighted arithmetic mean, so there are no values of \\(R\\) and \\(P\\) at which we are willing to trade one unit of recall for one unit of precision unless \\(\\beta=1\\), and in that case we are willing to do so at every value of \\(R\\) and \\(P\\)."
  },
  {
    "objectID": "posts/2019-12-02_evaluating-classifiers-pt4/evaluating_classifiers_pt4.html#demonstration-that-rho-does-its-job",
    "href": "posts/2019-12-02_evaluating-classifiers-pt4/evaluating_classifiers_pt4.html#demonstration-that-rho-does-its-job",
    "title": "Evaluating Classification Models, Part 4: Weighted Power Means of Precision and Recall",
    "section": "Demonstration That \\(\\rho\\) Does Its Job",
    "text": "Demonstration That \\(\\rho\\) Does Its Job\nThe “curvature” parameter \\(\\rho\\) characterizes how our concern for recall relative to precision changes as we move away from the neutral line \\(R/P=\\beta\\). More precisely, it characterizes the rate of change of the slope of the tangent line to a level curve \\(R/P=\\beta\\) along the direction of that tangent line.\nThe slope of the tangent line to a level curve of the weighted power mean is given by the ratio of the partial derivative of the weighted power mean with respect to recall to its partial derivative with respect to precision. Call this quantity \\(S\\). Then we have\n\\[\n\\begin{align*}\nS &= -\\frac{\\frac{\\partial}{\\partial R} \\left[ \\frac{1}{1 + \\beta^{\\rho}} \\left( \\beta^{\\rho} P^{\\rho+1} + R^{\\rho+1} \\right) \\right]^{\\frac{1}{\\rho+1}}}\n{\\frac{\\partial}{\\partial P} \\left[ \\frac{1}{1 + \\beta^{\\rho}} \\left( \\beta^{\\rho} P^{\\rho+1} + R^{\\rho+1} \\right) \\right]^{\\frac{1}{\\rho+1}}} \\\\\n&= -\\frac{\\frac{1}{\\rho+1} \\left[ \\frac{1}{1 + \\beta^{\\rho}} \\left( \\beta^{\\rho} P^{\\rho+1} + R^{\\rho+1} \\right) \\right]^{\\frac{-\\rho}{\\rho+1}} \\frac{\\rho+1}{1 + \\beta^{\\rho}} R^{\\rho} }\n{\\frac{1}{\\rho+1} \\left[ \\frac{1}{1 + \\beta^{\\rho}} \\left( \\beta^{\\rho} P^{\\rho+1} + R^{\\rho+1} \\right) \\right]^{\\frac{-\\rho}{\\rho+1}} \\frac{\\rho+1}{1 + \\beta^{\\rho}} \\beta^{\\rho} P^{\\rho}} \\\\\n&= -\\left( \\frac{R}{\\beta P} \\right)^{\\rho}\n\\end{align*}\n\\]\nWe want to characterize how the slope \\(S\\) of the tangent line to the level curve of the metric changes along that tangent line, so we need to evaluate the partial derivative of \\(S\\) with respect to \\(t=R-P\\). By the chain rule,\n\\[\n\\begin{align*}\n\\frac{\\partial S}{\\partial t} &= \\frac{\\partial S}{\\partial R} \\frac{\\partial R}{\\partial t} + \\frac{\\partial S}{\\partial P} \\frac{\\partial P}{\\partial t} \\\\\n&= \\frac{\\partial}{\\partial R} \\left[ -\\left( \\frac{R}{\\beta P} \\right)^{\\rho} \\right] \\frac{\\partial}{\\partial t} (t + P)\n+ \\frac{\\partial}{\\partial P} \\left[ -\\left( \\frac{R}{\\beta P} \\right)^{\\rho} \\right] \\frac{\\partial}{\\partial t} (R - t) \\\\\n&= -\\rho \\left( \\frac{R}{\\beta P} \\right)^{\\rho} R^{-1} - \\rho \\left( \\frac{R}{\\beta P} \\right)^{\\rho} P^{-1} \\\\\n&= -\\rho \\left( \\frac{R}{\\beta P} \\right)^{\\rho} \\left( R^{-1} + P^{-1} \\right)\n\\end{align*}\n\\]\nSo\n\\[\n\\left. \\frac{\\partial S}{\\partial t} \\right|_{\\beta = \\frac{R}{P}} = -\\rho \\left( R^{-1} + P^{-1} \\right)\n\\]\nThus, for a given \\(R\\) and \\(P\\) along the line \\(\\beta=R/P\\), the curvature of the level curve along the direction of its tangent line is proportional to \\(\\rho\\)."
  },
  {
    "objectID": "posts/2019-11-26_evaluating-classifiers-pt3/evaluating_classifiers_pt_3.html",
    "href": "posts/2019-11-26_evaluating-classifiers-pt3/evaluating_classifiers_pt_3.html",
    "title": "Evaluating Classification Models, Part 3: Fᵦ and Other Weighted Pythagorean Means of Precision and Recall",
    "section": "",
    "text": "There are many ways to combine precision and recall into an overall measure of model performance. Which one should you use?\n\n\nThis post is part of a series on evaluating classification models:\n\nPart 1: Weighing False Positives Against False Negatives explains why we need systematic ways to evaluate classification models.\nPart 2: The Sufficiency of Precision and Recall explains why precision and recall are sufficient for evaluating classification models in typical cases.\nPart 3: \\(F_\\beta\\) and Other Weighted Pythagorean Means of Precision and Recall explains what patterns of preferences are encoded by the Pythagorean means of precision and recall. This class of metrics includes the popular \\(F_\\beta\\) family, among others.\nPart 4: Weighted Power Means of Precision and Recall generalizes beyond the Pythagorean means to the broader class of weighted power means of precision and recall.\n\nThis series differs from other discussions of evaluation metrics for classification models in that it aims to provide a systematic perspective. Rather than providing a laundry list of individual metrics, it situates those metrics within a fairly comprehensive family and explains how you can choose a member of that family that is appropriate for your use case.\nThis post explains how the three weighted “Pythagorean means” (arithmetic, geometric, and harmonic) of precision and recall encode preferences over models.\n\nAn Example\nSuppose we build two different models, and one has better precision while the other has better recall.\nModel A:\n\n\n\n\nPredicted No\nPredicted Yes\nTotal\n\n\n\n\nActual No\n90\n10\n100\n\n\nActual Yes\n1\n99\n100\n\n\nTotal\n91\n109\n200\n\n\n\n\\[\n\\text{Precision} = \\frac{|\\text{True Positive}|}{|\\text{True Positive}| + |\\text{False Positive}|} = \\frac{90}{99 + 10} = 90.8\\%\n\\]\n\\[\n\\text{Recall} = \\frac{|\\text{True Positive}|}{|\\text{True Positive}| + |\\text{False Negative}|} = \\frac{99}{99 + 1} = 99.0\\%\n\\]\nModel B:\n\n\n\n\nPredicted No\nPredicted Yes\nTotal\n\n\n\n\nActual No\n91\n9\n100\n\n\nActual Yes\n3\n97\n100\n\n\nTotal\n94\n106\n200\n\n\n\n\\[\n\\text{Precision} = \\frac{|\\text{True Positive}|}{|\\text{True Positive}| + |\\text{False Positive}|} = \\frac{91}{91 + 9} = 91.5\\%\n\\]\n\\[\n\\text{Recall} = \\frac{|\\text{True Positive}|}{|\\text{True Positive}| + |\\text{False Negative}|} = \\frac{97}{97 + 3} = 97.0\\%\n\\]\nTo choose between these models, we need to decide whether the gain from \\(90.8\\%\\) precision to \\(91.5\\%\\) precision that we get by going from Model A to Model B is enough to offset a loss from \\(99\\%\\) recall to \\(97\\%\\) recall. We need some way to combine precision and recall into a single evaluation metric.\n\n\nThe Weighted Arithmetic Mean \\(A_\\gamma\\)\nWhen considering how to combine precision and recall to produce an overall evaluation metric, a natural first thought is simply to average them:\n\\[\n\\text{Simple Arithmetic Mean}: \\frac{1}{2}(P + R)\n\\]\nwhere \\(P\\) is precision and \\(R\\) is recall.\nIn the scenario above, going from Model A to Model B increases precision by \\(0.7\\%\\) but decreases recall by \\(2.0\\%\\). A simple arithmetic mean of precision and recall would tell us not to make this trade. It implies that we are willing to trade a decrease of \\(2.0\\%\\) in recall only for an increase of \\(2.0\\%\\) or more in precision. More generally, a simple arithmetic mean says that we are willing to trade one unit of recall only for one or more units of precision, and vice versa, regardless of the current precision and recall.\nWe can visualize this pattern of preferences using a set of “level curves.” In this plot, the simple arithmetic mean would tell us that two models that fall on the same curve are equally good, and that a model that lies on a curve up and to the right of another model is better than that other model.\n\n\nAnother possible pattern of preferences is to be just willing to trade one unit of precision for \\(\\gamma\\) units of recall for some \\(\\gamma > 0\\). This pattern is captured by a weighted arithmetic mean:\n\\[\n\\text{Weighted Arithmetic Mean} (\\gamma): \\frac{1}{1 + \\gamma}\\left(\\gamma P + R\\right)\n\\]\nThe level curves for this pattern of preferences are straight lines with slope \\(-\\gamma\\):\n\n\nTo help wrap your head around how to read the plot above, move the slider all the way to the right (\\(\\gamma=8\\)). Observe that the level curves become nearly horizontal. Pick a point on one of the lines, such as (\\(x=.48\\), \\(y=.84\\)), shown up close below. That point represents a model with \\(48\\%\\) recall and \\(84\\%\\) precision. If that model lost one percentage point of precision (corresponding to a shift down one unit to \\(y=.83\\)), it would have to gain \\(\\gamma=8\\) points of recall (corresponding to a shift right eight units to \\(x=.56\\)) to get back to the level curve it started on. In other words, the plot says that we would require 8 units of recall in order to give up one unit of precision, indicating that we care much more about precision than recall.\n\n\n\nThe slope of the level curve at a given point tells us how many points of recall (\\(\\gamma\\)) we would require in exchange for one point of precision for a model with the recall and precision represented by that point.\n\n\n\n\nThe Weighted Harmonic Mean \\(F_\\beta\\)\nIn some cases, the arithmetic mean is inappropriate because the amount of recall we would require in order to give up one unit of precision is not fixed; instead, it depends on the current recall and precision values. One option in those cases is to use a weighted harmonic mean of precision and recall instead of a weighted arithmetic. The result is the popular \\(F_\\beta\\) score.\nThe weighted harmonic mean is the inverse of the arithmetic mean of the inverses:\n\\[\n\\text{Weighted Harmonic Mean} (\\gamma): \\left(\\frac{1}{1 + \\gamma}\\left(\\gamma P^{-1} + R^{-1}\\right)\\right)^{-1}\n\\]\nIt is convenient to use the parameter \\(\\beta\\) such that \\(\\beta^2 = \\frac{1}{\\gamma}\\) in place of \\(\\gamma\\), which results in the following equivalent expression:\n\\[\n\\text{Weighted Harmonic Mean} (\\beta): \\left(\\frac{1}{1 + \\beta^2}\\left(P^{-1} + \\beta^2R^{-1}\\right)\\right)^{-1}\n\\]\nWith a weighted arithmetic mean \\(A_\\gamma\\), the relative importance of precision and recall is fixed: that metric says that we are willing to trade one unit of precision for \\(\\gamma\\) or more units of recall regardless of the current precision and recall. With a weighted harmonic mean \\(F_\\beta\\), by contrast, the relative importance of precision and recall depends on their current values. For instance, suppose that recall is \\(80\\%\\). If precision is \\(98\\%\\), then you might value a \\(1\\%\\) increase in recall more than a \\(1\\%\\) increase in precision, whereas if precision is \\(10\\%\\), then you might care more about increasing precision.\nA metric that places more importance on recall when recall is low compared to precision and vice versa has level curves that curve upward:\n\n\nOne way to think about the harmonic mean is that it deviates from the arithmetic mean in the direction of the minimum, as we will see more clearly when we discuss weighted power means in the next post in this series. The weighted harmonic mean of a set of numbers with a given weighting \\(\\gamma\\) is always less than the arithmetic mean with the same \\(\\gamma\\) (unless all of the numbers are the same, in which case the arithmetic and harmonic means are equal.)\nOne benefit of using \\(\\beta\\) instead of \\(\\gamma\\) is that \\(\\beta\\) indicates how much we care about recall relative to precision. More precisely, \\(\\beta\\) is the ratio of recall to precision where the harmonic mean places equal value on a one-unit increase in recall and a one-unit increase in precision (in the limit as the size of the unit goes to zero). If recall is more important to you than precision, for instance, then you would set \\(\\beta\\) greater than one because you would rather have a one-unit increase in recall than a one-unit increase in precision unless recall is already high compared to precision.\nFor instance, \\(\\beta=3\\) means that if precision is \\(25\\%\\), then you would rather have one more unit of precision than one more unit of recall if recall is above \\(75\\%\\), but you would rather have the recall if recall is below \\(75\\%\\) (because \\(75\\%/25\\%=3\\)).\nYou narrow down the range of \\(\\beta\\) values that are appropriate for a particular application by getting stakeholder feedback on different combinations of precision and recall values. For instance, at one point we had a model at ShopRunner that we had tuned for \\(F_\\beta\\) with \\(\\beta=.3\\), meaning that we cared about recall about \\(30\\%\\) as much as we cared about precision. That model classified products with \\(90\\%\\) precision and \\(36\\%\\) recall. When we showed those results to our primary stakeholder, she expressed more concern about the \\(36\\%\\) recall than the \\(90\\%\\) precision. Assuming that \\(F_\\beta\\) captured the overall shape of her preferences, this feedback entailed that her \\(\\beta\\) was greater than \\(36\\%/90\\%=.4\\). After further discussion, we started tuning for \\(F_\\beta\\) with \\(\\beta=.5\\) instead of \\(.3\\).\nIn the plot below, the thick black line indicates the set of models where you would be equally happy with one more point of precision or one more point of recall. Above the line, you would rather have one point of recall. Below the line, you would rather have one point of precision. Larger values of \\(\\beta\\) mean that you care more about recall overall, so there is more area below the line.\n\n\nIn these plots, the number of units of recall that you require in exchange for one unit of precision when recall is \\(R\\) and precision is \\(P\\) is given by the slope of the tangent line to the level curve that passes through (\\(x=R, y=P\\)). For instance, move the slider all the way to the right (\\(\\beta=8\\)). Exactly on the black line, the tangent to each level curve has slope \\(-1/1\\), indicating that you would exchange one unit of precision for one unit of recall, as shown below.\n\n\n\nAlong the black line that passes through the points where \\(\\text{Recall}/\\text{Precision}=\\beta\\), the tangent to a level curve has slope \\(-1/1\\), meaning that you would require one unit of recall in exchange for one unit of precision (in the limit as the size of the unit goes to zero).\n\n\nBelow the black line, where precision is high compared to recall, the tangent to each level curve has slope \\(-1/\\delta\\) for some \\(\\delta>1\\), indicating that you would require more than one unit of recall in exchange for one unit of precision, as shown below. In other words, with a harmonic mean, when precision is high enough compared to recall, you care about improving recall more than you care about improving precision. This region of the plot is large in this case because \\(\\beta>1\\), indicating that we care about recall more than precision overall.\n\n\n\nBelow the black line that passes through the points where \\(\\text{Recall}/\\text{Precision}=\\beta\\), the tangent to a level curve has slope \\(-1/\\delta\\) for some \\(\\delta>1\\), meaning that you would require more than one unit of recall in exchange for one unit of precision.\n\n\nAbove the black line, where recall is high compared to precision, the tangent to each level curve has slope \\(-1/\\delta\\) for some \\(\\delta<1\\), indicating that you would require less than one unit of recall in exchange for one unit of precision, as shown below. In other words, with a harmonic mean, when recall is high enough compared to precision, you care about improving precision more than you care about improving recall. This region of the plot is small in this case because \\(\\beta>1\\), indicating that we care about recall more than precision overall.\n\n\n\nBelow the black line that passes through the points where \\(\\text{Recall}/\\text{Precision}=\\beta\\), the tangent to a level curve has slope \\(-1/\\delta\\) for some \\(\\delta<1\\), meaning that you would require less than one unit of recall in exchange for one unit of precision.\n\n\nFor the arithmetic mean, unlike the harmonic mean just discussed, the level curves are straight lines. As a result, the tangent to a level curve is just the level curve itself. Moreover, all of the level curves have the same slope \\(-1/\\gamma\\), corresponding to the fact that with an arithmetic mean you are willing to trade one unit of precision for \\(\\gamma\\) or more units of recall for every level of precision and recall.\n\n\nWhy Would You Use a Non-Arithmetic Mean?\nYou might wonder whether a harmonic mean of precision and recall is ever appropriate. The author of an excellent recent blog post suggested that it isn’t, writing that they had “never encountered a business problem where a real-life cost function of false positives and false negatives is a harmonic mean.” I claim on the contrary that a harmonic mean or something similar is often appropriate, in scenarios where the value of the algorithm depends on the overall impression created by its outputs rather than on separate effects of its individual predictions.\nAn arithmetic mean tends to be appropriate for diagnostic problems. For instance, in medical testing a false positive can cause unnecessary stress and expense, while a false negative can cause worse health outcomes because of a missed opportunity at early intervention. To a first approximation, those costs are specific to each patient and do not depend on how many other false positives or false negatives the test generates. As a result, we should be willing to trade precision for recall at the same rate regardless of the current precision and recall, which makes an arithmetic mean appropriate.\nBy contrast, a harmonic mean tends to be appropriate for information retrieval problems. For instance, consider the model we discussed in Part 1 of this series that identifies whether a product is a dress or not for an app that sells dresses. Suppose that the pool of products being classified is enormous — every dress for sale on the internet, if you like. Then if recall is quite high, say \\(98\\%\\), then another point of recall doesn’t mean much: someone who is looking for a dress will already be able to find one that they love. If precision is low in this scenario, then it would be better to focus on improving precision. Whereas if precision is very high, say \\(98\\%\\), then encountering a product that should not be in the app because it is not a dress is a rare experience that users will plausibly overlook. If recall is low in that scenario, so that users are not always able to find the perfect dress even though it is out there somewhere, then it would be better to focus on recall. An arithmetic mean cannot capture this pattern of preferences, while a harmonic mean can.\n\n\n\nIf we had a lot of non-dresses in the dress feed of ShopRunner’s District app, then we would prioritize increasing precision for that category. By contrast, when precision is high we care more about increasing recall so that we can offer more dress options. An arithmetic mean of precision and recall cannot capture this situation, but a harmonic mean can.\n\n\nThat being said, a harmonic mean is not the only option that captures this general pattern of preferences. For instance, a geometric mean also puts more weight on precision or recall the lower it is, but to a smaller degree than a harmonic mean.\n\n\nThe Weighted Geometric Mean \\(G_\\gamma\\)\nThe geometric mean is intermediate between the harmonic mean and the arithmetic mean: unless all of the inputs are equal, a weighted geometric mean is strictly greater than the corresponding weighted harmonic mean and strictly less than the corresponding arithmetic mean. (If the inputs are all equal, then all three means are equal to their common value.)\nThe simple geometric mean of two numbers can be calculated by multiplying them and then taking the square root of the result:\n\\[\n\\text{Simple Geometric Mean}: \\left(PR\\right)^{1/2}\n\\]\nWe can incorporate weights by taking precision to the power \\(\\gamma\\) before multiplying and taking the result to the power \\(1/(1+\\gamma)\\):\n\\[\n\\text{Weighted Geometric Mean} (\\gamma): \\left(P^\\gamma R\\right)^{1/(1+\\gamma)}\n\\]\nWe can change this expression so that the parameter that controls the weighting has the same interpretation as \\(\\beta\\) in the weighted harmonic mean \\(F_\\beta\\). We will use the same parameter name “\\(\\beta\\),” but in this case \\(\\beta=1/\\gamma\\):\n\\[\n\\text{Weighted Geometric Mean} (\\beta): \\left(P R^\\beta\\right)^{1/(1+\\beta)}\n\\]\nHere again \\(\\beta\\) is the ratio of recall to precision at which the metric responds equally to an increase in precision and an increase in recall, so that higher \\(\\beta\\) corresponds to placing more importance on recall relative to precision.\nThe level curves for the weighted geometric mean curve upward like those for the weighted harmonic mean, but to a smaller degree:\n\n\nThe weighted geometric mean deviates from the weighted arithmetic mean in the direction of the minimum, again like the harmonic mean but to a smaller degree.\n\n\n\nThe harmonic mean deviates from the arithmetic mean in the direction of the minimum, so it is more sensitive to whichever is smaller between the precision and the recall. The geometric mean has the same property to a smaller degree.\n\n\n\n\nWhich Mean Should You Use?\nWhen each false positive or false negative has roughly the same expected cost regardless of how many false positives or false negatives the model generates overall, you should use an arithmetic mean of precision and recall to evaluate a model. Models used for medical diagnosis often have this property.\nBut when what matters is the overall impression that the model’s outputs create, you might use something like a geometric or harmonic mean that prioritizes whichever of precision or recall is lagging behind. Models used for information retrieval often have this property.\nSubsequent posts in this series will situate the arithmetic, geometric, and harmonic means within a larger family of metrics and provide additional guidance on how to choose among them.\n\n\nAcknowledgements\nThanks to Nicole Carlson, Morgan Cundiff, and the rest of the ShopRunner data science team for comments on earlier versions of this material.\nOriginally published at medium.com"
  },
  {
    "objectID": "posts/2021-08-05_pandas-styling/pandas_styling.html",
    "href": "posts/2021-08-05_pandas-styling/pandas_styling.html",
    "title": "Problem Solved: Custom Pandas styling not rendering when reopening notebook",
    "section": "",
    "text": "Pandas has a variety of methods for styling notebooks. I have been using those methods in the model_inspector library to display correlation matrices and confusion matrices in Jupyter notebooks with appropriate highlighting.\n\n\n\nA correlation matrix rendered with appropriate coloring using Pandas styling methods in the model_inspector library. Note that it uses a diverging colormap so that positive values are red and negative values are blue. I often see sequential colormaps used here, which make it hard to distinguish between positive, negative, and near-zero values.\n\n\nI recently encountered a problem with this approach: DataFrames were not rendering with custom styling when I reopened a notebook after saving it. After an hour or two of fruitless Googling, I figured out why: the notebook was not trusted, so Jupyter was not running the code to do the custom rendering when I reopened it. Running the Terminal command jupyter trust my_notebook.ipynb fixed the problem."
  },
  {
    "objectID": "posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html",
    "href": "posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html",
    "title": "Evaluating Classification Models, Part 2: The Sufficiency of Precision and Recall",
    "section": "",
    "text": "With high recall, most dresses appear in the dress feed. With high precision, most products in the dress feed are dresses. Are those two numbers all we need?\nThis post is part of a series on evaluating classification models:\nThis series differs from other discussions of evaluation metrics for classification models in that it aims to provide a systematic perspective. Rather than providing a laundry list of individual metrics, it situates those metrics within a fairly comprehensive family and explains how you can choose a member of that family that is appropriate for your use case.\nPart 1 of this series explains why we need metrics that allow us to evaluate binary classification models systematically. For instance, if we are developing a model that takes images of a product in ShopRunner’s retailer network and returns a probability that the product is a dress, we need a metric that weighs false positives (products that are predicted to be dresses but are not) against false negatives (products that are dresses but are predicted not to be) in a way that approximately matches what our preferences over models would be upon sufficient reflection.\nThat post put forward precision and recall as ways to measure the ability of a model to avoid false positives and false negatives, respectively, using the example of a model that classifies a product as either a dress or not. Precision is the accuracy of a model’s positive predictions. For instance, when it says that a product is a dress, how often is it actually a dress? Recall is the accuracy of the model’s predictions for actual instances of the positive class. For instance, when a product is a dress, how often does the model classify it as a dress?\nPrecision and recall are useful metrics, but in order to use them to rank models we need some way to combine them into a single measure of overall performance. Subsequent posts in this series will go into detail about how to think about such composite metrics. This post addresses a prior question: why use precision and recall as our starting points for model evaluation? In particular, how can we be sure that those numbers do not leave out important information?"
  },
  {
    "objectID": "posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html#summarizing-model-performance-with-precision-and-recall",
    "href": "posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html#summarizing-model-performance-with-precision-and-recall",
    "title": "Evaluating Classification Models, Part 2: The Sufficiency of Precision and Recall",
    "section": "Summarizing Model Performance with Precision and Recall",
    "text": "Summarizing Model Performance with Precision and Recall\nThe results of applying a binary classifier to a dataset can be displayed in a confusion matrix:\n\n\n\n\nPredicted No\nPredicted Yes\n\n\n\n\nActual No\n|True Negative|\n|False Positive|\n\n\nActual Yes\n|False Negative|\n|True Positive|\n\n\n\nwhere e.g. “|True Positive|” refers to the number of true positive predictions that the model generates (i.e., where the model correctly predicted that the item in question belongs to the positive class).\nFor instance, a model that classifies a product as “dress” or “not a dress” might have the following confusion matrix:\n\n\n\n\nPredicted No\nPredicted Yes\n\n\n\n\nActual No\n92\n8\n\n\nActual Yes\n10\n90\n\n\n\nWe often act as if an observed confusion matrix contains everything we can learn about the value of a model from its performance on a test dataset. This approach is not always justified: for instance, the confusion matrix discards information about the presence or absence of problematic patterns of error such as racial biases. However, if those kinds of problems are being addressed by other means, then the assumption that a confusion matrix is a sufficient basis for preferences over models may be justified.\nWe can calculate precision and recall from a confusion matrix but not vice versa. However, when we evaluate a model against a validation dataset, we typically have access to the total number of positive and negative examples (the row sums in the confusion matrix). Given that information, we can reconstruct a confusion matrix from precision and recall (See the Appendix to this post.) Thus, in a typical scenario, precision and recall contain the same information as a confusion matrix, so they are sufficient for evaluating a model if the confusion matrix is.\nNow, there are other pairs of numbers that also contain the same information as a confusion matrix given the numbers of actual positive and actual negative cases, such as sensitivity and specificity. The choice among such pairs of numbers is a matter of taste. I use precision and recall because they are popular in the machine learning community. I suspect that they are also easier for people to understand than some of the alternatives because they have actual positive or predicted positive cases in the denominator, so that thinking about them requires processing fewer negations.\nIn any case, precision and recall are sufficient for model evaluation when a confusion matrix is sufficient for model evaluation and the numbers of actual positive and actual negative examples in the dataset are known."
  },
  {
    "objectID": "posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html#appendix-constructing-a-confusion-matrix-from-precision-recall-and-its-row-sums",
    "href": "posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html#appendix-constructing-a-confusion-matrix-from-precision-recall-and-its-row-sums",
    "title": "Evaluating Classification Models, Part 2: The Sufficiency of Precision and Recall",
    "section": "Appendix: Constructing a Confusion Matrix from Precision, Recall, and its Row Sums",
    "text": "Appendix: Constructing a Confusion Matrix from Precision, Recall, and its Row Sums\nHere is one way to reconstruct a confusion matrix from precision, recall, and the number of “Actual No” and “Actual Yes” cases.\n\n|True Positive|\n\\[\n\\text{Recall} = \\frac{|\\text{True Positive}|}{|\\text{Actual Yes}|}\n\\]\nso\n\\[\n|\\text{True Positive}| = \\text{Recall} \\times |\\text{Actual Yes}|\n\\]\n\n\n|False Negative|\n\\[\n|\\text{False Negative}| = |\\text{Actual Yes}| - |\\text{True Positive}|\n\\]\n\n\n|False Positive|\n\\[\n\\text{Precision} = \\frac{|\\text{True Positive}|}{|\\text{True Positive}| + |\\text{False Positive}|}\n\\]\nso a bit of algebra shows\n\\[\n|\\text{False Positive}| = |\\text{True Positive}|\\frac{1-\\text{Precision}}{\\text{Precision}}\n\\]\n\n\n|True Negative|\n\\[\n|\\text{True Negative}| = |\\text{Actual No}| - |\\text{False Positive}|\n\\]"
  },
  {
    "objectID": "posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html#acknowledgements",
    "href": "posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html#acknowledgements",
    "title": "Evaluating Classification Models, Part 2: The Sufficiency of Precision and Recall",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Nicole Carlson, Ali Vanderveld, and the rest of the ShopRunner data science team for comments on earlier versions of this material.\nOriginally published at medium.com"
  },
  {
    "objectID": "posts/2020-07-30_introducing-wildebeest/introducing_wildebeest.html",
    "href": "posts/2020-07-30_introducing-wildebeest/introducing_wildebeest.html",
    "title": "Introducing Wildebeest, a Python File-Processing Framework",
    "section": "",
    "text": "Photo Credit: Gopal Vijayaraghavan cc\n\n\n\nIntroduction\nShopRunner has more than ten million product images that we use to train computer vision classification models. Moving those files around and processing them is a pain without good tooling. Just downloading them serially takes many days, and the occasional corrupted image can bring the whole process to a halt. Without good logging and error handling, it might then be necessary to start the process over until the next error is raised.\nOver time we built up techniques for parallelizing over files, handling errors, and skipping files that had already been processed. We then incorporated those techniques into an open-source file-processing framework Wildebeest. With Wildebeest, the user specifies what files to process and how to process each one. Wildebeest then does the processing in parallel and generates a “run report” that records for each file the input and output path, whether the file was skipped, and what error if any was handled. We routinely use Wildebeest to download and resize millions of images in a matter of hours and tens of lines of code.\n\n\nBasic Example\nThe following code uses a fairly minimal Wildebeest pipeline to download a list of images to the current working directory as PNGs, parallelizing across up to ten threads.\nfrom functools import partial\n\nfrom wildebeest import Pipeline\nfrom wildebeest.load_funcs.image import load_image_from_url\nfrom wildebeest.path_funcs import join_outdir_filename_extension\nfrom wildebeest.write_funcs.image import write_image\n\n\nimage_urls = [\n    f\"https://bit.ly/{filename}\" for filename in [\"2RsJ8EQ\", \"2TqoToT\", \"2VocS58\"]\n]\n\n# Create a pipeline object, specifying how to load a file and how to\n# write out each file\nimage_download_pipeline = Pipeline(\n    load_func=load_image_from_url, write_func=write_image\n)\n\n# Run the pipeline, specifying input paths, how to derive an output path\n# from each input path, and how many threads to use\nimage_download_pipeline(\n    inpaths=image_urls,\n    path_func=partial(join_outdir_filename_extension, outdir=\".\", extension=\".png\"),\n    n_jobs=10,\n)\nAfter it runs, the pipeline has a Pandas DataFrame containing a record of what happened with each input file stored as an attribute called “run_report_”:\n\n\n\nRun Report\n\n\nThe trailing underscore in “run_report_” indicates that the attribute exists only after the pipeline has been run, analogous to the Scikit-Learn convention of using a trailing underscore for Estimator attributes that exist only after the attribute has been fit (e.g. LinearRegression().coef_).\n\n\nAdditional Capabilities\nYou can do more with Wildebeest than just download images:\n\nProcess input from any source you want, including arbitrary media types (e.g. text, video, or audio) and local as well as remote files.\nDo arbitrary processing on each file, for instance to resize each image before writing it to disk.\nAdd columns to the run report that record arbitrary properties of the file, such as the average brightness of each image.\nSelectively skip files based on arbitrary criteria. For instance, you can skip an input file when a file already exists at the intended output location, making it easy to pick up where you leave off quickly after a failure.\n\nOur Quickstart guide includes code examples for all of these scenarios.\nYou can also see Wildebeest in use in the open-source Autofocus project, which uses computer vision to automate animal conservation work in collaboration with the Lincoln Park Zoo’s Urban Wildlife institute. (In that project it is used under its previous name “Creevey.” That name comes from the Harry Potter series, so we changed it after becoming aware of transphobic comments by series author J.K. Rowling.)\n\n\nConclusion\nWildebeest makes big data processing jobs fast and easy. To get started with it, you can read the docs, check out the code on GitHub, or install the package from PyPI.\n\n\nAcknowledgements\nThanks to Michael Sugimura for feedback on an earlier draft and to the ShopRunner data science team for contributions to the library."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "",
    "text": "This plot shows level curves for an F₁ score, which is one way to weigh precision against recall in evaluating a classification model.\nThis post is part of a series on evaluating classification models:\nThis series differs from other discussions of evaluation metrics for classification models in that it aims to provide a systematic perspective. Rather than providing a laundry list of individual metrics, it situates those metrics within a fairly comprehensive family and explains how you can choose a member of that family that is appropriate for your use case."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#classifying-products-at-shoprunner",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#classifying-products-at-shoprunner",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "Classifying Products at ShopRunner",
    "text": "Classifying Products at ShopRunner\nShopRunner’s mission is to power extraordinary shopping experiences that connect customers to the brands they love. To accomplish this mission, we need to make it easy for users to find products they want. Many of our efforts toward this end would benefit from being able to classify products within a single product taxonomy. For instance, classifying a particular product as a dress allows us to display it under “Women > Clothing > Dresses” in our District app.\n\n\n\nDresses in ShopRunner’s District app\n\n\nClassifying all of the products across our retailer network into a single taxonomy is challenging. Our partners all use different product taxonomies, often including categories such as “New Arrivals” that do not map cleanly into categories like “Dresses.” As a result, simple rules-based approaches based on the product categories that our partners use are not sufficient. Moreover, classifying each product manually would be too expensive: we have more than 100 partners with millions of total products, many of which turn over several times each year.\nWe are using machine learning to address this challenge. Modern deep learning systems can learn to classify products based on images and text descriptions with accuracy similar to that of manual classification for a small fraction of the cost."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#false-positives-and-false-negatives",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#false-positives-and-false-negatives",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "False Positives and False Negatives",
    "text": "False Positives and False Negatives\nEven with cutting-edge machine learning, no classification system is perfect. For instance, a model that classifies a product as “dress” or “not a dress” might sometimes misidentify a skirt as a dress. We would call that misclassification a false positive for the dress category. In addition, it might sometimes misidentify a dress as “not a dress,” which we would call a false negative for the dress category.\nExamples:\n\n\n\nCorrectly labeling this dress as a dress would be a true positive.\n\n\n\n\n\nCorrectly labeling this skirt as “not a dress” would be a true negative.\n\n\n\n\n\nIncorrectly labeling this skirt as a dress would be a false positive.\n\n\n\n\n\nIncorrectly labeling this dress as “not a dress” would be a false negative. (Yes, it is a dress. Life is pain.)\n\n\nMany machine learning classification models actually produce a probability for a label such as dress. By default, we would typically take a model to be predicting “dress” if its probability for “dress” is at least \\(50\\%\\) and “not a dress” otherwise. But we can use a cutoff other than \\(50\\%\\) if we wish. For instance, we might only classify a product as a dress if its probability for dress is at least \\(70\\%\\).\nUsing \\(70\\%\\) rather than \\(50\\%\\) as our cutoff probability for a positive prediction would change our model’s error rates. Specifically, by causing the model to produce fewer positives, it would reduce the false positive rate (which is good) but increase the false negative rate (which is bad). Part of our job in developing a classification model is to choose cutoff probabilities that strike an appropriate balance between false positives and false negatives.\nFor simplicity, let’s pretend that our District app only sells dresses. (It doesn’t!) Showing a product that isn’t a dress in the app would be potentially off-putting to our users, so the cost of a false positive is significant. On the other hand, a dress that we mistakenly classify as “not a dress” typically will not have much impact as long as we still have a large number of other dresses to choose from. As a result, the cost of a single false positive is generally higher than that of a single false negative, so we should set our cutoff probability higher than \\(50\\%\\)."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#precision-and-recall",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#precision-and-recall",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "Precision and Recall",
    "text": "Precision and Recall\nWe have said that we should set the cutoff probability higher than \\(50\\%\\), but how much higher? To answer this question, we need ways to quantify our model’s performance as we vary the cutoff probability. There are many metrics that we could use for this purpose. I will focus on precision and recall. Part 2 of this series will justify that choice.\nA model’s precision is the accuracy of its positive predictions. For instance, when our model says that something is a dress, how often is it actually a dress?\n\\[\n\\text{Precision} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Positives}|}\n\\]\nwhere e.g. “|True Positives|” is the number of true positive predictions that our model generates.\nRecall is the model’s accuracy on the actual positive class. For instance, when a product is actually a dress, how often does our model say that it is a dress?\n\\[\n\\text{Recall} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Negatives}|}\n\\]\nNow suppose we build two different models, and one has better precision while the other has better recall. (We can typically create such a scenario by changing the cutoff probability for positive predictions.)\nModel A:\n\n\n\n\nPredicted No\nPredicted Yes\nTotal\n\n\n\n\nActual No\n90\n10\n100\n\n\nActual Yes\n1\n99\n100\n\n\nTotal\n91\n109\n200\n\n\n\n\\[\n\\text{Precision} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Positives}|} = \\frac{|99|}{|99| + |10|} = 90.8\\%\n\\]\n\\[\n\\text{Recall} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Negatives}|} = \\frac{|99|}{|99| + |1|} = 99\\%\n\\]\nModel B:\n\n\n\n\nPredicted No\nPredicted Yes\nTotal\n\n\n\n\nActual No\n91\n9\n100\n\n\nActual Yes\n3\n97\n100\n\n\nTotal\n94\n106\n200\n\n\n\n\\[\n\\text{Precision} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Positives}|} = \\frac{|97|}{|97| + |9|} = 91.5\\%\n\\]\n\\[\n\\text{Recall} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Negatives}|} = \\frac{|97|}{|97| + |3|} = 97\\%\n\\]\nTo choose between these models, we need to decide whether the gain from \\(90.8\\%\\) precision to \\(91.5\\%\\) precision that we get by going from Model A to Model B is enough to offset a loss from \\(99\\%\\) recall to \\(97\\%\\) recall."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#why-we-need-a-variety-of-model-evaluation-metrics",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#why-we-need-a-variety-of-model-evaluation-metrics",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "Why We Need a Variety of Model Evaluation Metrics",
    "text": "Why We Need a Variety of Model Evaluation Metrics\nIf we only had to make occasional decisions between pairs of models, then it would be reasonable to make decisions about how to trade off precision against recall on an ad hoc basis. However, we can typically generate dozens of models that all differ in terms of precision and recall with no model dominating all others on both measures, for instance by varying our cutoff probability. As a result, we need a way to combine precision and recall into a single metric that we can use to select the “best” model for a given application.\nA natural way to combine precision and recall is to average them. For instance, using simple averaging in the example above, Model A would get a score of \\(1/2(90.8\\% + 99\\%) = 94.9\\%\\), while Model B would get a score of \\(1/2(91.5\\% + 97\\%) = 94.3\\%\\). However, this simple mean gives equal weight to precision and recall. We have said that it is more important to avoid showing non-dresses in our app than it is to show all of the dresses that we have, so precision is more important to us than recall. Thus, a simple arithmetic mean of precision and recall isn’t the right choice in this application. We should use a weighted average instead.\nMoreover, perhaps we would be willing to give up one point of precision in exchange for one point of recall if precision were very high (say \\(99%\\)) and recall were very low (say \\(10%\\)), whereas if precision and recall were similar then we would not be willing to make that trade. If so, then a weighted arithmetic mean of precision and recall is not the right choice, because a weighted arithmetic mean encodes a willingness to trade one point of recall for a fixed number of points of precision at every level of precision and recall.\nIn that case, we might consider using something like a weighted geometric or harmonic mean rather than a weighted arithmetic mean. Part 3 of this series discusses those options.\nThere are many ways to weigh precision against recall, and which way is most appropriate depends on the problem. Many sources help with the challenge of choosing an evaluation metric for a given problem by providing a laundry list of metrics with a few comments about the characteristics of each one. This series of blog posts aims to provide a more systematic perspective. It culminates in a new family of metrics that generalizes the popular \\(F_\\beta\\) score. This family allows the user to specify not only the overall importance of recall relative to precision but also how that relative importance shifts as recall and precision become increasingly imbalanced."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#appendix",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#appendix",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "Appendix",
    "text": "Appendix\n\nExtending Binary Model Evaluation Metrics to Multiclass Problems\nSo far we have discussed only binary classification problems, such as identifying whether a particular product is or is not a dress. The full problem of placing products within our taxonomy is not a binary classification problem, because there are many possible categories for each item. For instance, we need to say not just whether a given item is a dress or not, but also if it is not a dress, whether it is a skirt, a pair of jeans, and so on. However, we can regard this multiclass problem as a set of binary classification problems (one for each category) and aggregate performance on each of those subproblems to produce an overall measure of model performance. As a result, the discussion in these blog posts is relevant to evaluating multiclass models even though it focuses on binary models.\nFor instance, here is one possible procedure:\n\nCalculate precision and recall for the “dress” category\nCalculate an overall score for the “dress” category by averaging those precision and recall values in one of the ways we will discuss.\nDo likewise for all other categories.\nUse a simple average of the individual category scores as the score for the multiclass model as a whole.\n\nSee the scikit-learn documentation for a discussion of several ways to produce multiclass model evaluation metrics from binary model evaluation metrics.\n\n\nUsing a Single Evaluation Metric\nIt is common to use multiple metrics to measure various aspects of model performance separately. In my view, this approach is helpful for diagnostic purposes — that is, for figuring out where your model is failing and using that information to guide model development. However, it is best to use but one metric for evaluation purposes — that is, for deciding which of a set of models is best and whether it is better than no model.\nThis single evaluation metric can be different for different projects, and it is subject to revision within a project, but for a given project there should be one primary metric at any given time that is used to guide modeling decisions. This approach allows a team developing the model to iterate quickly, without getting bogged down in long discussions about how to weigh different aspects of model performance against one another at every turn. See this lecture from Andrew Ng’s “Structuring Machine Learning Projects” course for more discussion of this issue."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#acknowledgements",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#acknowledgements",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Nicole Carlson, Ali Vanderveld, and the rest of the ShopRunner data science team for comments on earlier versions of this material.\nOriginally published at medium.com"
  },
  {
    "objectID": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html",
    "href": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html",
    "title": "Parameter-Efficient Fine-Tuning Illustrated with Regression Tasks",
    "section": "",
    "text": "LLMS and other foundation models are capable of accomplishing a wide range of tasks. However, we may have a task that they do not perform well off the shelf. In principle, we can address this problem by fine tuning a model for that task. However, fine tuning foundation models is extremely expensive.\nParameter-efficient fine tuning (PEFT) is meant to address this problem. Rather than adjusting all of the weights of a model, it adds relatively small adapters to the model and trains those adapters on new data.\nThis post implements a few parameter efficient fine tuning techniques: LORA, DORA, and RS-LORA. It illustrates these methods on a simple regression problem in which we adapt a model that has been trained on a quadratic data set to a cubic data set. These polynomial fitting problems are many orders of magnitude simpler than language modeling, so the way these PEFT methods behave on them may not tell us much about how they behave when applied to foundation models. However, the illustrations do illustrate the general concepts of full and parameter-efficient fine tuning and help confirm that the methods are working as expected.\nThe LoRA and DoRA are implementations adapted from Sebastian Raschka’s Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch. The visualization methods are adapted from Jeremy Howard’s FastAI v3 Lesson 2: SGD. Both were published under the Apache License 2.0."
  },
  {
    "objectID": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html#reproduce-lora-results",
    "href": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html#reproduce-lora-results",
    "title": "Parameter-Efficient Fine-Tuning Illustrated with Regression Tasks",
    "section": "Reproduce LoRA results",
    "text": "Reproduce LoRA results\nLet’s implement rsLoRA, but initially adjust \\(\\alpha\\) to get the same scaling factor as before, so that we should get the same results as with LoRA.\n\ntorch.manual_seed(\n    678\n)  # resetting seed so we get the same LoRA weight initializations as before\nrslora_model = create_lora_model(\n    model,\n    lora_layer_indices=[2],\n    lora_layer_class=partial(\n        LinearWithLoRA,\n        rank=RANK,\n        # will give same gamma_r as LoRA above, so should train the same\n        alpha=ALPHA / (RANK ** 1/2),\n        lora_layer_class=RsLoRALayer,\n    ),\n)\nrslora_model\n\nMultilayerPerceptron(\n  (layers): Sequential(\n    (0): Linear(in_features=1, out_features=20, bias=True)\n    (1): ReLU()\n    (2): LinearWithLoRA(\n      (linear): Linear(in_features=20, out_features=20, bias=True)\n      (lora): RsLoRALayer()\n    )\n    (3): ReLU()\n    (4): Linear(in_features=20, out_features=1, bias=True)\n  )\n)\n\n\n\nfor name, param in rslora_model.named_parameters():\n    print(f\"{name}: {param.requires_grad}\")\n\nlayers.0.weight: False\nlayers.0.bias: False\nlayers.2.linear.weight: False\nlayers.2.linear.bias: False\nlayers.2.lora.A: True\nlayers.2.lora.B: True\nlayers.4.weight: False\nlayers.4.bias: False\n\n\n\nanim = create_training_animation(\n    rslora_model,\n    x,\n    y2,\n    torch.optim.Adam(rslora_model.parameters(), lr=LEARNING_RATE),\n)\nHTML(anim.to_html5_video())\n\nInitial Loss: 0.3533\nInitial Loss: 0.3533\nIteration 10, Loss: 0.0761\nIteration 20, Loss: 0.0494\nIteration 30, Loss: 0.0287\nIteration 40, Loss: 0.0241\nIteration 50, Loss: 0.0183\nIteration 60, Loss: 0.0154\nIteration 70, Loss: 0.0138\nIteration 80, Loss: 0.0130\nIteration 90, Loss: 0.0126\nIteration 100, Loss: 0.0124\nIteration 110, Loss: 0.0122\nIteration 120, Loss: 0.0121\nIteration 130, Loss: 0.0119\nIteration 140, Loss: 0.0118\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html#compare-lora-and-rslora-at-extreme-ranks",
    "href": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html#compare-lora-and-rslora-at-extreme-ranks",
    "title": "Parameter-Efficient Fine-Tuning Illustrated with Regression Tasks",
    "section": "Compare LoRA and rsLoRA at Extreme Ranks",
    "text": "Compare LoRA and rsLoRA at Extreme Ranks\nNow let’s keep \\(\\alpha\\) the same and see how LoRA and rsLoRA perform at extreme ranks. I would not necessarily expect rsLoRA to perform better than LoRA at extreme ranks in this simple case, but at least we can illustrate the type of situation in which rsLoRA is expected to perform better.\n\nLoRA at Low Rank\n\nlora_model = create_lora_model(\n    model,\n    lora_layer_indices=[2],\n    lora_layer_class=partial(LinearWithLoRAMerged, rank=1, alpha=ALPHA),\n)\nanim = create_training_animation(\n    lora_model,\n    x,\n    y2,\n    torch.optim.Adam(lora_model.parameters(), lr=LEARNING_RATE),\n)\nHTML(anim.to_html5_video())\n\nInitial Loss: 0.3533\nInitial Loss: 0.3533\nIteration 10, Loss: 0.0739\nIteration 20, Loss: 0.0678\nIteration 30, Loss: 0.0291\nIteration 40, Loss: 0.0260\nIteration 50, Loss: 0.0236\nIteration 60, Loss: 0.0209\nIteration 70, Loss: 0.0195\nIteration 80, Loss: 0.0186\nIteration 90, Loss: 0.0177\nIteration 100, Loss: 0.0170\nIteration 110, Loss: 0.0167\nIteration 120, Loss: 0.0164\nIteration 130, Loss: 0.0163\nIteration 140, Loss: 0.0162\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\n\nLoRA at High Rank\n\nlora_model = create_lora_model(\n    model,\n    lora_layer_indices=[2],\n    lora_layer_class=partial(LinearWithLoRAMerged, rank=20, alpha=ALPHA),\n)\nanim = create_training_animation(\n    lora_model,\n    x,\n    y2,\n    torch.optim.Adam(lora_model.parameters(), lr=LEARNING_RATE),\n)\nHTML(anim.to_html5_video())\n\nInitial Loss: 0.3533\nInitial Loss: 0.3533\nIteration 10, Loss: 0.0561\nIteration 20, Loss: 0.0304\nIteration 30, Loss: 0.0235\nIteration 40, Loss: 0.0161\nIteration 50, Loss: 0.0137\nIteration 60, Loss: 0.0130\nIteration 70, Loss: 0.0125\nIteration 80, Loss: 0.0121\nIteration 90, Loss: 0.0118\nIteration 100, Loss: 0.0116\nIteration 110, Loss: 0.0115\nIteration 120, Loss: 0.0113\nIteration 130, Loss: 0.0113\nIteration 140, Loss: 0.0112\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\n\nrsLoRA at Low Rank\n\nrslora_model = create_lora_model(\n    model,\n    lora_layer_indices=[2],\n    lora_layer_class=partial(\n        LinearWithLoRA,\n        rank=1,\n        alpha=ALPHA,\n        lora_layer_class=RsLoRALayer,\n    ),\n)\nanim = create_training_animation(\n    rslora_model,\n    x,\n    y2,\n    torch.optim.Adam(rslora_model.parameters(), lr=LEARNING_RATE),\n)\nHTML(anim.to_html5_video())\n\nInitial Loss: 0.3533\nInitial Loss: 0.3533\nIteration 10, Loss: 0.1752\nIteration 20, Loss: 0.1717\nIteration 30, Loss: 0.1737\nIteration 40, Loss: 0.1724\nIteration 50, Loss: 0.1708\nIteration 60, Loss: 0.1691\nIteration 70, Loss: 0.1666\nIteration 80, Loss: 0.1623\nIteration 90, Loss: 0.1540\nIteration 100, Loss: 0.0966\nIteration 110, Loss: 0.0926\nIteration 120, Loss: 0.0903\nIteration 130, Loss: 0.0896\nIteration 140, Loss: 0.0884\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\n\nrsLoRA at High Rank\n\nrslora_model = create_lora_model(\n    model,\n    lora_layer_indices=[2],\n    lora_layer_class=partial(\n        LinearWithLoRA,\n        rank=20,\n        alpha=ALPHA,\n        lora_layer_class=RsLoRALayer,\n    ),\n)\nanim = create_training_animation(\n    rslora_model,\n    x,\n    y2,\n    torch.optim.Adam(rslora_model.parameters(), lr=LEARNING_RATE),\n)\nHTML(anim.to_html5_video())\n\nInitial Loss: 0.3533\nInitial Loss: 0.3533\nIteration 10, Loss: 0.0635\nIteration 20, Loss: 0.0302\nIteration 30, Loss: 0.0185\nIteration 40, Loss: 0.0145\nIteration 50, Loss: 0.0129\nIteration 60, Loss: 0.0120\nIteration 70, Loss: 0.0117\nIteration 80, Loss: 0.0114\nIteration 90, Loss: 0.0113\nIteration 100, Loss: 0.0112\nIteration 110, Loss: 0.0111\nIteration 120, Loss: 0.0110\nIteration 130, Loss: 0.0110\nIteration 140, Loss: 0.0109\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html",
    "href": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html",
    "title": "Fast and Reproducible Deep Learning",
    "section": "",
    "text": "Our team’s open-source Wildebeest library is one tool for managing deep learning projects — it makes processing large datasets fast and easy. Photo Credit: Gopal Vijayaraghavan cc\nThere are endless resources for someone who wants to learn to train a deep learning model, but running a successful deep learning project requires managing many additional moving parts that are much less discussed. This talk contributes to filling that gap in our deep learning education resources.\nThanks to the Chicago ML Meetup for hosting."
  },
  {
    "objectID": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html#video",
    "href": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html#video",
    "title": "Fast and Reproducible Deep Learning",
    "section": "Video",
    "text": "Video"
  },
  {
    "objectID": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html#slides",
    "href": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html#slides",
    "title": "Fast and Reproducible Deep Learning",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html#abstract",
    "href": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html#abstract",
    "title": "Fast and Reproducible Deep Learning",
    "section": "Abstract",
    "text": "Abstract\nDeep learning projects require managing large datasets, heavy-duty dependencies, complex experiments, and large amounts of code. This talk provides best practices for accomplishing these tasks efficiently and reproducibly. Tools that are covered include:\n\nThe Wildebeest library for processing large collections of files\npip-tools and nvidia-docker for managing dependencies\nMLflow Tracking for tracking experiments"
  },
  {
    "objectID": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html#additional-resources",
    "href": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html#additional-resources",
    "title": "Fast and Reproducible Deep Learning",
    "section": "Additional Resources",
    "text": "Additional Resources\nAutofocus is a deep learning project that labels animals in images taken by motion-activated “camera traps.” It illustrates many of the ideas discussed in the talk."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Greg Gandenberger",
    "section": "",
    "text": "Parameter-Efficient Fine-Tuning Illustrated with Regression Tasks\n\n\n\n\n\n\n\nml\n\n\nllms\n\n\npeft\n\n\nlora\n\n\ndora\n\n\nrslora\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nProblem Solved: Custom Pandas styling not rendering when reopening notebook\n\n\n\n\n\n\n\nml\n\n\njupyter\n\n\npandas\n\n\n\n\nQuick fix for Pandas styling not showing up when reopening Jupyter notebooks - use jupyter trust command to enable custom styling rendering.\n\n\n\n\n\n\nAug 5, 2021\n\n\n\n\n\n\n  \n\n\n\n\nIntroducing Wildebeest, a Python File-Processing Framework\n\n\n\n\n\n\n\npython\n\n\ndata engineering\n\n\ncomputer vision\n\n\ndeep learning\n\n\nwildebeest\n\n\noss\n\n\n\n\nAn open-source framework for processing large collections of files in parallel with good error handling and logging.\n\n\n\n\n\n\nJul 30, 2020\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nFast and Reproducible Deep Learning\n\n\n\n\n\n\n\ndeep learning\n\n\nml\n\n\ncomputer vision\n\n\nwildebeest\n\n\noss\n\n\n\n\nBest practices for managing deep learning projects, including tools for processing large datasets, managing dependencies, and tracking experiments.\n\n\n\n\n\n\nMar 26, 2020\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating Classification Models, Part 4: Weighted Power Means of Precision and Recall\n\n\n\n\n\n\n\nml\n\n\nevals\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2019\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating Classification Models, Part 3: Fᵦ and Other Weighted Pythagorean Means of Precision and Recall\n\n\n\n\n\n\n\nml\n\n\nevals\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2019\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating Classification Models, Part 2: The Sufficiency of Precision and Recall\n\n\n\n\n\n\n\nml\n\n\nevals\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2019\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating Classification Models, Part 1: Weighing False Positives Against False Negatives\n\n\n\n\n\n\n\nml\n\n\nevals\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2019\n\n\nGreg Gandenberger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Machine learning engineer posting as I learn about AI engineering. Previously at Cruise, Argo AI, ShopRunner, General Assembly, and Uptake. Public projects include the file-processing framework Wildebeest, visualization library Model Inspector, and camera trap animal classification service Autofocus."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Problem Solved: Custom Pandas styling not rendering when reopening notebook\n\n\n\n\n\n\n\nml\n\n\njupyter\n\n\npandas\n\n\n\n\nQuick fix for Pandas styling not showing up when reopening Jupyter notebooks - use jupyter trust command to enable custom styling rendering.\n\n\n\n\n\n\nAug 5, 2021\n\n\n\n\n\n\n  \n\n\n\n\nIntroducing Wildebeest, a Python File-Processing Framework\n\n\n\n\n\n\n\npython\n\n\ndata engineering\n\n\ncomputer vision\n\n\ndeep learning\n\n\nwildebeest\n\n\noss\n\n\n\n\nAn open-source framework for processing large collections of files in parallel with good error handling and logging.\n\n\n\n\n\n\nJul 30, 2020\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nFast and Reproducible Deep Learning\n\n\n\n\n\n\n\ndeep learning\n\n\nml\n\n\ncomputer vision\n\n\nwildebeest\n\n\noss\n\n\n\n\nBest practices for managing deep learning projects, including tools for processing large datasets, managing dependencies, and tracking experiments.\n\n\n\n\n\n\nMar 26, 2020\n\n\nGreg Gandenberger\n\n\n\n\n\n\nNo matching items"
  }
]