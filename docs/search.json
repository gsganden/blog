[
  {
    "objectID": "posts/2025-03-05_llm-math-evaluation-best-practices/index.html",
    "href": "posts/2025-03-05_llm-math-evaluation-best-practices/index.html",
    "title": "Some Best Practices for LLM Math Evaluation",
    "section": "",
    "text": "Image generated by ChatGPT-4o.\nEvaluating LLM answers to open-ended math questions is hard because LLMs can generate arbitrary text. They might surround their answer with additional information. They are not guaranteed to format their answer in any particular way. They might not answer the question at all! Comparing their answer to ground truth requires finding it and then making the comparison in a flexible yet discerning way.\nThis blog post describes some best practices for evaluating LLM answers to open-ended math questions. It illustrates these ideas in the context of using HuggingFace’s Math-Verify library with MosaicML’s LLM Foundry, but the principles apply more broadly. It does NOT address using an LLM as a judge, which is an alternative to the kind of approach described here."
  },
  {
    "objectID": "posts/2025-03-05_llm-math-evaluation-best-practices/index.html#a.-use-an-instruction-tuned-model",
    "href": "posts/2025-03-05_llm-math-evaluation-best-practices/index.html#a.-use-an-instruction-tuned-model",
    "title": "Some Best Practices for LLM Math Evaluation",
    "section": "1.a. Use an instruction-tuned model",
    "text": "1.a. Use an instruction-tuned model\nBase language models are trained to generate more text rather than specifically to answer questions. If you are evaluating performance on math questions, you should probably be using an instruction-tuned model. This point is particularly easy to overlook if you are just grabbing a small model to get started with development. For instance, HuggingFaceTB/SmolLM2-135M-Instruct would be a better choice for this purpose than HuggingFaceTB/SmolLM2-135M."
  },
  {
    "objectID": "posts/2025-03-05_llm-math-evaluation-best-practices/index.html#b.-use-few-shot-prompting",
    "href": "posts/2025-03-05_llm-math-evaluation-best-practices/index.html#b.-use-few-shot-prompting",
    "title": "Some Best Practices for LLM Math Evaluation",
    "section": "1.b. Use few-shot prompting",
    "text": "1.b. Use few-shot prompting\nLLMs are fundamentally pattern-matching machines, so giving them examples of what you want them to do is generally effective.\n\n\n\n\n\n\nNote\n\n\n\nIn LLM Foundry, you can use few-shot prompting by setting e.g. num_fewshot: [5] in the eval configuration, or by using a dataset such as gsm8k_prepended_8shot.jsonl that has few-shot examples built into each question. The example below also uses question_prelimiter: \"Question: \".\n\n\nFor instance, you can give them input that looks like this:\n\nQuestion: Janet’s ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her friends every day with four. She sells the remainder at the farmers’ market daily for $2 per fresh duck egg. How much in dollars does she make every day at the farmers’ market?\nA: Janet sells 16 - 3 - 4 = <<16-3-4=9>>9 duck eggs a day. She makes 9 * 2 = $<<9*2=18>>18 every day at the farmer’s market.The answer is 18 Question: A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take?\nA:It takes 2/2=<<2/2=1>>1 bolt of white fiber So the total amount of fabric is 2+1=<<2+1=3>>3 bolts of fabric The answer is 3 … Question: Every day, Wendi feeds each of her chickens three cups of mixed chicken feed, containing seeds, mealworms and vegetables to help keep them healthy. She gives the chickens their feed in three separate meals. In the morning, she gives her flock of chickens 15 cups of feed. In the afternoon, she gives her chickens another 25 cups of feed. How many cups of feed does she need to give her chickens in the final meal of the day if the size of Wendi’s flock is 20 chickens?\nA:\n\nThe model will typically then get the clue that it should be trying to answer the last question."
  },
  {
    "objectID": "posts/2025-03-05_llm-math-evaluation-best-practices/index.html#c.-cut-it-off-at-the-question-delimiter",
    "href": "posts/2025-03-05_llm-math-evaluation-best-practices/index.html#c.-cut-it-off-at-the-question-delimiter",
    "title": "Some Best Practices for LLM Math Evaluation",
    "section": "1.c. Cut it off at the question delimiter",
    "text": "1.c. Cut it off at the question delimiter\nFew-shot prompting might lead to a further problem: the model might think that it should not only answer the last question but continue to generate more question/answer pairs, with responses like this:\n\nHe made 150% of the original price The answer is 150% Question: A snail is at the bottom of a 20-foot well. Each day, it climbs up 3 feet, but at night, it slips back 2 feet. How many days will it take for the snail to reach the top of the well? …\n\nYou can deal with this problem by ignoring everything in its response after the “question prelimiter”, in this case “Question:”.\n\n\n\n\n\n\nNote\n\n\n\nIn LLM Foundry, you can implement this approach by including “Question:” in the early_stopping_criteria."
  },
  {
    "objectID": "posts/2015-12-14_knapsack/knapsack.html",
    "href": "posts/2015-12-14_knapsack/knapsack.html",
    "title": "Hypothesis Test Construction as a Knapsack Problem",
    "section": "",
    "text": "Photo Credit: M J M via Compfight cc\nNeyman and Pearson (e.g. 1933) treat the problem of choosing the best rejection region for a simple-vs.-simple hypothesis test as what computer scientists call a 0/1 knapsack problem. Standard examples of 0/1 knapsack problems are easier to grasp than hypothesis testing problems, so thinking about Neyman-Pearson test construction on analogy with those examples is helpful for developing intuitions. It is also illuminating to think about points of disanalogy between those scenarios and hypothesis testing scenarios, which give rise to possible objections to the Neyman-Pearson approach."
  },
  {
    "objectID": "posts/2015-12-14_knapsack/knapsack.html#the-01-knapsack-problem",
    "href": "posts/2015-12-14_knapsack/knapsack.html#the-01-knapsack-problem",
    "title": "Hypothesis Test Construction as a Knapsack Problem",
    "section": "The 0/1 Knapsack Problem",
    "text": "The 0/1 Knapsack Problem\nIn a knapsack problem, one seeks to maximize some quantity subject to a constraint. A standard example is that of a thief who wants to maximize the value of the objects she steals from a particular home, subject to the constraint that the total weight of those objects cannot be greater than the maximum weight that she can carry. For instance, suppose the thief has the following items to choose from.1\n\n\n\n\n\n\n\n\n\n\n\n\n\nClock\nPainting\nRadio\nVase\nBook\nComputer\n\n\n\n\nValue (USD)\n175\n90\n20\n50\n10\n200\n\n\nWeight (lb.)\n10\n9\n4\n2\n1\n20\n\n\nValue/Weight (USD/lb.)\n17.5\n10\n5\n25\n10\n10\n\n\n\nOne possible approach to this problem is to choose objects in order of descending value/weight ratio until adding the next object would cause the total weight to exceed the limit. In this example, given enough space, that approach would lead one to choose first the vase, then the clock, then either the computer, the book, or the painting, and so on. This approach has the following virtue: it yields a set of objects that has at least as much value as any other set with the same or less total weight. However, there may be a set of objects that has greater value within the maximum weight limit. For instance, if the maximum weight is 10 lb., then this approach would lead one to take the vase only, because the next object, the clock, would put one over that weight limit. This choice provides more value than any other choice with the same total weight (2lb.). However, there are other choices with total weight less than 10 lbs. and greater value: for instance, one could take just the clock, or the vase, radio, and book.\nIn the 0/1 knapsack problem, each item is either in the knapsack or not. An easier problem is the continuous knapsack program, in which objects can be arbitrarily broken up into smaller objects, preserving the ratios of their basic attributes. For instance, if the objects were things like gold bullion and crude oil, the thief might be able to take arbitrary quantities of those items at a fixed value/weight ratio. The optimal solution to the thief’s problem in this case would be to fill up on each item as much as possible in order of descending value/weight ratio, stopping precisely when the maximum weight is reached."
  },
  {
    "objectID": "posts/2015-12-14_knapsack/knapsack.html#hypothesis-testing",
    "href": "posts/2015-12-14_knapsack/knapsack.html#hypothesis-testing",
    "title": "Hypothesis Test Construction as a Knapsack Problem",
    "section": "Hypothesis Testing",
    "text": "Hypothesis Testing\nSuppose one wanted to test a null hypothesis \\(H_0\\) against an alternative hypothesis \\(H_a\\). In the simplest case, \\(H_0\\) and \\(H_a\\) are both “simple statistical hypotheses” relative to some proposed experiment, meaning that they each specify a particular chance distribution over the sample space \\(S\\) of possible outcomes of that experiment. Our task is to decide which elements of \\(S\\) to place in the “rejection region” \\(R\\), that is, the precise set of results on which will reject \\(H_0\\) for \\(H_a\\).\nNeyman and Pearson propose to choose a test on the basis of power and Type I error rate, where a test’s power is the probability that it correctly rejects \\(H_0\\) if \\(H_0\\) is false, and its Type I error rate is the probability that it incorrectly rejects \\(H_0\\) if \\(H_0\\) is true. Specifically, they propose to choose a test that maximizes power subject to the constraint that the Type I error rate cannot exceed some maximum value \\(\\alpha\\). Thus, they treat the problem of constructing a hypothesis test as a 0/1 knapsack problem, completely analogous to the thief’s problem described above, as shown in this table.\n\n\n\n\n\n\n\nLiteral Knapsack Problem\nHypothesis Test Construction\n\n\n\n\nPutting item into knapsack\nPutting element of \\(S\\) into rejection region \\(R\\)\n\n\nTotal value\nPower (sum of \\(\\Pr(s;H_a)\\) over elements of \\(S\\) in \\(R\\))\n\n\nTotal weight\nType I error rate (sum of \\(\\Pr(s;H_0)\\) over elements of \\(S\\) in \\(R\\))\n\n\nMaximizing total value subject to maximum total weight\nMaximizing power subject to maximum Type I error rate\n\n\n\nConsider the example shown in the table below. \\(s_1\\), \\(s_2\\), and \\(s_3\\) are elements of a sample space \\(S\\). They could be, for instance, the event that a three-sided die produces a 1, 2, or 3, respectively. \\(H_0\\) and \\(H_a\\) would then be hypotheses about the biases of the die.\n\n\n\n\n\\(s_1\\)\n\\(s_2\\)\n\\(s_3\\)\n\n\n\n\n\\(\\Pr(s;H_a)\\)\n0.04\n0.05\n0.91\n\n\n\\(\\Pr(s;H_0)\\)\n0.01\n0.05\n0.94\n\n\n\\(\\Pr(s;H_a)/\\Pr(s;H_0)\\)\n4\n1\n0.97\n\n\n\nI said above that putting objects into the knapsack in descending order by value/weight ratio, stopping when the next item would cause the total weight to exceed the limit, yields a set of items that has the largest value among all sets with no more than its total weight. Analogously, putting elements of the sample space into the rejection region in order by descending likelihood ratio, stopping when the next item would cause the Type I error rate to exceed \\(\\alpha\\), yields a rejection region that has the greatest power among all possible rejection regions with no more than its Type I error rate. (This result is known as the Neyman-Pearson lemma.) Just as that approach in the thief’s case may not yield the greatest possible value consistent with the cap on the total weight, so too in the hypothesis testing case it may not yield the greatest possible power consistent with the Type I error rate being no greater than \\(\\alpha\\). For instance, in the example shown above, it would lead one to perform a test that has power \\(.04\\) when \\(0.05\\leq \\alpha < .06\\) (with \\(R=\\{s_1\\}\\)) even though a test with power \\(.05\\) and Type I error rate less than \\(\\alpha\\) is available (with \\(R=\\{s_2\\}\\)).\nThere are two ways to turn the 0/1 knapsack problem of constructing a best Neyman-Pearson hypothesis test into a continuous knapsack problem. First, one can consider cases with continuous, strictly positive probability distributions over continuous sample spaces. Here, the optimal solution is to add elements of the hypothesis space to the rejection region in descending order by likelihood ratio until the Type I error rate reaches \\(\\alpha\\). Second, one can allow randomized tests that reject the null hypothesis with some non-extremal probability on some elements of the sample space. Here, the optimal solution is to add elements of the sample space to the rejection region in descending order by likelihood ratio until we get to the first element that would cause the Type I error rate to exceed \\(\\alpha\\) if we were to add it to \\(R\\). We then prescribe consulting some auxiliary randomizer to decide to reject the null hypothesis if that result is observed, in such a way that the Type I error rate of the test is exactly \\(\\alpha\\). This procedure is analogous to having the thief taking a portion of the item with the largest value/weight ratio that will not wholly fit in the bag, choosing the size of the portion so that the total weight is exactly the maximum weight.\nRandomized tests are often discussed in presentations of the Neyman-Pearson framework because they make certain results easier to state. However, they are generally rejected in practice. They violate the plausible principle that the output of a hypothesis test should depend only on aspects of the data that are evidentially relevant to the hypotheses in question. One could take the hardline view suggested by Neyman and Pearson’s own writings that this principle is false because only long-run error rates matter. However, few methodologists take this view so seriously that they are willing to countenance randomized tests.2"
  },
  {
    "objectID": "posts/2015-12-14_knapsack/knapsack.html#is-this-approach-sensible",
    "href": "posts/2015-12-14_knapsack/knapsack.html#is-this-approach-sensible",
    "title": "Hypothesis Test Construction as a Knapsack Problem",
    "section": "Is This Approach Sensible?",
    "text": "Is This Approach Sensible?\nThe Neyman-Pearson approach of treating hypothesis test construction as a knapsack problem has some odd consequences. For instance, in the example above, the optimal solution for \\(.05\\leq \\alpha < .06\\) rejects \\(H_0\\) if and only if \\(s_2\\) is observed. But \\(s_2\\) has the same probability (\\(.05\\)) under \\(H_0\\) and \\(H_a\\), whereas \\(s_1\\) is four times more probable under \\(H_a\\) than under \\(H_0\\). If one accepts the Law of Likelihood, which says that \\(s\\) favors \\(H_a\\) over \\(H_0\\) if and only if \\(\\Pr(s;H_a)/\\Pr(s;H_0)>1\\), then it follows that \\(s_1\\) favors \\(H_a\\) over \\(H_0\\) while \\(s_2\\) is neutral between them. Even if the Law of Likelihood is not acceptable in full generality, it seems to give a sensible verdict in this case. One might think, then, that \\(s_2\\) should not appear in \\(R\\) without \\(s_1\\).\nPearson provides a solution to this problem in later papers. In his (1947, 173), for instance, he prescribes a three-step process for specifying tests:\n\nStep 1. We must specify the [sample space].\nStep 2. We then divide this set by a system of ordered boundaries or contours such that as we pass across one boundary and proceed to the next, we come to a class of results which makes us more and more inclined, on the information available, to reject the hypothesis tested in favour of alternatives which differ from it by increasing amounts.\nStep 3. We then, if possible, associate with each contour level [a Type I error rate].\n\nThe key point here is that Pearson prescribes ordering results according to the degree to which they would incline one to reject \\(H_0\\) and only considering tests that reject \\(H_0\\) on result \\(s\\) but not result \\(s'\\) if \\(s\\) makes one more inclined to reject \\(H_0\\) than \\(s'\\). If the ordering of one’s inclinations to reject \\(H_0\\) on the basis of possible observations conforms to the likelihood ratios of those observations, then on this approach one will not consider problematic tests like the one that rejects on \\(s_2\\) but not \\(s_1\\) in our example. This approach is analogous to the thief always taking items in descending order by value/weight ratio, stopping when the next item will not fit. Here we see a point of disanalogy between hypothesis test construction and literal knapsack problems: because the outcomes of hypothesis tests are (at least de facto) interpreted in evidential terms, it seems inappropriate to add elements of the sample space to \\(S\\) “out of order” relative to their likelihood ratios, even if doing so allows one to get greater power while keep the Type I error rate below \\(\\alpha\\). By contrast, it is not problematic to add items to the thief’s knapsack “out of order” relative to their value/weight ratios if doing so allows one to get a higher total value while keeping the maximum weight below the maximum.\nA second point of disanalogy between a literal knapsack problem and the problem of constructing a hypothesis test concerns the appropriate way to trade off value against weight, or power against Type I error rate. We can arrange so that it is only a slight idealization to suppose that the thief does not care how heavy her bag is as long as she can carry it away. (We can assume that she has a strong back and a getaway vehicle nearby, does not have to worry about how much noise she makes, and so on.) We cannot arrange so that a scientist does not care about the Type I error rate of his or her test as long as it is below a particular threshold, at least if we impose the normative assumption that the scientist’s goal is to advance knowledge and not just, say, to get his or her paper past journal referees.\nRather than maximizing power subject to a maximum Type I error rate, it would seem to make more sense to minimize a weighted sum of the Type I and Type II error rates, where the Type II error rate is the probability of failing to reject the null hypothesis if it is false (1-power) and the weights reflect the importance of avoiding Type I and Type II errors. Like Pearson’s approach if one’s inclinations to reject \\(H_0\\) conform to likelihood ratios, this approach would lead one to reject \\(H_0\\) for \\(H_a\\) if and only if the likelihood ratio \\(\\Pr(s;H_a)/\\Pr(s;H_0)\\) exceeds some threshold \\(k\\). In this case, \\(k\\) is simply the weight one associates with the Type I error rate divided by the weight one associates with the Type II error rate. The only difference between this approach and Pearson’s is that this approach involves fixing relative weights on the Type I error rate and power and letting the likelihood ratio cutoff for rejection and the Type I error rate fall where they may, whereas Pearson’s involves putting a cap on the Type I error rate and letting the likelihood ratio cutoff and (implied) relative weights on the Type I error rate and power fall where they may.\nOne might object that weights on the Type I and Type II error rates are too subjective or arbitrary for use in science. However, they do not seem to be any more subjective or arbitrary than the maximum tolerated Type I error rate \\(\\alpha\\). There is a typical convention of setting \\(\\alpha=.05\\), but that convention is itself rather arbitrary. Moreover, we could establish the analogous convention of setting \\(k=20\\), which has the effect of guaranteeing that the Type I error rate is no greater than the standard .05 (Royall 2000).\nTo my mind, this alternative to Pearson’s approach seems more sensible. Constructing a hypothesis test is more like filling a knapsack for a long journey than for a quick getaway: every increase in weight (Type I error rate) matters and needs to be compensated by a sufficient increase in value (power).\nThe main problem for this approach is that it does not generalize well to cases involving composite hypotheses (e.g., that a particular parameter is in a specified range or is not equal to a specific value), which are the usual cases in science. In those cases one or both of \\(\\Pr(s;H_0)\\) and \\(\\Pr(s;H_a)\\) lack definite values, and many methodologists are reluctant to appeal to the corresponding conditional probabilities \\(\\Pr(s|H_0)\\) and \\(\\Pr(s|H_a)\\) because they lack generally accepted objective values."
  },
  {
    "objectID": "posts/2025-02-22_til_math-verify-2/til_math-verify-2.html",
    "href": "posts/2025-02-22_til_math-verify-2/til_math-verify-2.html",
    "title": "TIL: How Math-Verify Verifies LLM Outputs",
    "section": "",
    "text": "Note\n\n\n\nThis is a TIL (“Today I Learned”) post. I expect it to be useful to my future self and maybe to others, but it is meant to be a quick, informal way to capture something I learned rather than a polished presentation.\nHugging Face’s Math-Verify library provides relatively robust tools to evaluate LLM performance on math problems. Its README demonstrates using it by calling its parse function on both the LLM output and the gold answer, and then passing those results to verify. My last post examined the parse function. This post examines the verify function."
  },
  {
    "objectID": "posts/2025-02-22_til_math-verify-2/til_math-verify-2.html#comparing-lists",
    "href": "posts/2025-02-22_til_math-verify-2/til_math-verify-2.html#comparing-lists",
    "title": "TIL: How Math-Verify Verifies LLM Outputs",
    "section": "Comparing Lists",
    "text": "Comparing Lists\nWe saw previously that parse returns a list which may contain both a sympy expression and a string:\n\nfrom math_verify import parse, verify\nimport sympy\n\n\nparse(\"1/3\")\n\n[1/3, '1/3']\n\n\nverify ostensibly compares everything in the first list with everything in the second list, and returns True if any of those combinations pass its equality check. However, its equality check always returns False for the combination of a sympy expression and a string, so in practice it just indicates whether either the two sympy expressions or the two strings are equal to each other.\n\nzero = sympy.Number(0)\none = sympy.Number(1)\n\n# Everything is equal\nverify(gold=[zero, \"0\"], target=[zero, \"0\"])\n\nTrue\n\n\n\n# `gold` and `target` are each internally consistent but are not equal to each other\nverify(gold=[zero, \"0\"], target=[one, \"1\"])\n\nFalse\n\n\n\n# `gold` and `target` sympy expressions are equal to each other while their strings are not\nverify(gold=[zero, \"1\"], target=[zero, \"2\"])\n\nTrue\n\n\n\n# `gold` and `target` strings are equal to each other while their `sympy` expressions are not\nverify(gold=[zero, \"2\"], target=[one, \"2\"])\n\nTrue\n\n\n\n# `gold` `sympy` expression is equal to `target` string and vice versa\nverify(gold=[zero, \"1\"], target=[one, \"0\"])\n\nFalse\n\n\n\n# `gold` and `target` indicate the same value, but one is a `sympy` expression and the other is a string\nverify(gold=[zero], target=[\"0\"])\n\nFalse\n\n\nThis last example might seem surprising. The thinking behind this behavior, as I understand this comment, is that a string should only be present without a corresponding sympy expression if parsing that string failed, and it is unlikely that parsing failed on one side and yet the string on that side is genuinely equal to the sympy expression on the other side. This rationale makes sense on the assumption that the input to verify came from parse, which is probably what we want but could be documented more explicitly, as I suggested in this issue."
  },
  {
    "objectID": "posts/2025-02-22_til_math-verify-2/til_math-verify-2.html#equality-for-strings",
    "href": "posts/2025-02-22_til_math-verify-2/til_math-verify-2.html#equality-for-strings",
    "title": "TIL: How Math-Verify Verifies LLM Outputs",
    "section": "Equality for Strings",
    "text": "Equality for Strings\nEquality for strings is simply Python == after stripping whitespace off the ends and ensuring that the strings are not both empty. This approach is obviously imperfect, but it is meant only to catch some of the cases where sympy parsing fails.\n\nverify(gold=[\"1/3\"], target=[\"1 / 3\"])\n\nFalse"
  },
  {
    "objectID": "posts/2025-02-22_til_math-verify-2/til_math-verify-2.html#equality-for-sympy-expressions",
    "href": "posts/2025-02-22_til_math-verify-2/til_math-verify-2.html#equality-for-sympy-expressions",
    "title": "TIL: How Math-Verify Verifies LLM Outputs",
    "section": "Equality for sympy Expressions",
    "text": "Equality for sympy Expressions\nEquality for sympy expressions is complex. At this core it uses sympy functionality such as Eq and evalf after applying various normalization steps, with support for a few options for strictness:\nfloat_rounding: Number of decimal places to round floats to. Defaults to 6.\nnumeric_precision: Number of decimal places to consider for numeric comparisons. Defaults to 15.\n    - If know the evaluated expressions will be small, you should increase this. See: https://docs.sympy.org/latest/modules/evalf.html\nstrict: Whether to enforce strict comparison mode. Defaults to True.\n    - In strict mode: Variables matter and sets are not comparable with tuples\n    - In non-strict mode: Variables are matched by position and sets can be compared with tuples\nThe presence of both numeric_precision and float_rounding parameters could lead to confusion, as this issue notes: setting one of them to a high value will not have the result one might expect if the other is lower:\n\n# `float_rounding` is 6 by default, so increasing `numeric_precision` has no effect in this case\nverify(parse(\"0.0000001\"), parse(\"0.0000002\"), numeric_precision=99999)\n\nTrue"
  },
  {
    "objectID": "posts/2025-02-22_til_math-verify-2/til_math-verify-2.html#conclusion",
    "href": "posts/2025-02-22_til_math-verify-2/til_math-verify-2.html#conclusion",
    "title": "TIL: How Math-Verify Verifies LLM Outputs",
    "section": "Conclusion",
    "text": "Conclusion\nI see two sharp edges in Math-Verify’s verify function: it assumes that its inputs have passed through parse, and it has numeric_precision and float_rounding parameters that need to be adjusted together to avoid unexpected behavior. Otherwise it seems like a smart approach to the difficult problem of comparing LLM outputs to gold answers on open-ended math problems without relying on an LLM judge."
  },
  {
    "objectID": "posts/2025-02-19_til_llmfoundry-eval-2/til_llmfoundry-eval-2.html",
    "href": "posts/2025-02-19_til_llmfoundry-eval-2/til_llmfoundry-eval-2.html",
    "title": "TIL: An LLM Foundry Metric Must Have “Accuracy” in Its Name",
    "section": "",
    "text": "Note\n\n\n\nThis is a TIL (“Today I Learned”) post. I expect it to be useful to my future self and maybe to others, but it is meant to be a quick, informal way to capture something I learned rather than a polished presentation.\n\n\nMy last post described limitations of LLM Foundry’s default evaluation procedure for open-ended math problems and suggested that we could do better by creating a custom metric that uses Math-Verify.\nI encountered a “gotcha” in the process of creating that custom metric: the metric must have “Accuracy” in its name. Otherwise LLM Foundry’s evaluation script skips it!\nI created this GitHub issue to track this problem. I hope that we can eliminate this behavior, or at least make it more obvious."
  },
  {
    "objectID": "posts/2025-03-27_til_how-max-run-duration-affects-loss-curve/index.html",
    "href": "posts/2025-03-27_til_how-max-run-duration-affects-loss-curve/index.html",
    "title": "TIL: Why Did My Loss Curve Change With Run Duration?",
    "section": "",
    "text": "Note\n\n\n\nThis is a TIL (“Today I Learned”) post. I expect it to be useful to my future self and maybe to others, but it is meant to be a quick, informal way to capture something I learned rather than a polished presentation.\n\n\n\nThe Puzzle\nToday I encountered a puzzling phenomenon: my model’s loss curve appeared to be bimodal. I was working to eliminate sources of randomness in order to reproduce another interesting finding, and I saw that the loss usually followed one curve, but sometimes followed a second curve, and always followed one of the two:\n\n\n\nThe Solution\nI had decided to move on when a third curve appeared:\n\nI then realized that the difference between the two initial sets of runs was coming from their max duration.\nWhy would a run’s max duration affect the loss curve at intermediate stages of training? Because the learning rate was scheduled to decay over the course of the run after an initial warmup. When the run is longer, it decays more slowly:\n\n\n\n\n\n\n\nNote\n\n\n\nThis was my LLM-Foundry learning rate scheduler configuration:\nscheduler:\n  name: cosine_with_warmup\n  t_warmup: 200000tok\n  alpha_f: 0.1\n\n\n\n\nWhy It Matters\nIt is important to realize when the learning rate depends on the run max duration for at least two reasons:\n\nReproducibility requires leaving max_duration fixed even for results early in training.\nThe optimal learning rate schedule hyperparameters will depend on max_duration.\n\n\n\nHow to Deal with the Interaction Between Learning Rate and Max Duration\nThere are two ways to deal with this interaction between learning rate and max duration:\n\nEliminate it, by using a constant learning rate or a schedule that is independent of max_duration (e.g. a cyclic schedule).\nAccount for it, for instance by settling on a max_duration first and then tuning learning rate schedule hyperparameters.\n\nMy understanding is that cosine decay is commonly used in LLM training, so it is important to be aware that changing the max duration under this approach will affect the entire training process."
  },
  {
    "objectID": "posts/2025-03-14_til_vscode-gpu-containers/index.html",
    "href": "posts/2025-03-14_til_vscode-gpu-containers/index.html",
    "title": "TIL: Dev Containers and GPUs: A Match Made in Purgatory",
    "section": "",
    "text": "Note\n\n\n\nThis is a TIL (“Today I Learned”) post. I expect it to be useful to my future self and maybe to others, but it is meant to be a quick, informal way to capture something I learned rather than a polished presentation."
  },
  {
    "objectID": "posts/2025-03-14_til_vscode-gpu-containers/index.html#gpu-installation-challenges",
    "href": "posts/2025-03-14_til_vscode-gpu-containers/index.html#gpu-installation-challenges",
    "title": "TIL: Dev Containers and GPUs: A Match Made in Purgatory",
    "section": "GPU Installation Challenges",
    "text": "GPU Installation Challenges\nSetting up GPU support for deep learning is challenging. You need:\n\nThe right NVIDIA drivers for your GPU\nA compatible CUDA toolkit version\nProper environment configuration (CUDA_HOME, LD_LIBRARY_PATH, etc.)\nGPU-enabled versions of your deep learning frameworks\n\nThese requirements often lead to compatibility issues, version conflicts, and system-wide changes that can affect other projects."
  },
  {
    "objectID": "posts/2025-03-14_til_vscode-gpu-containers/index.html#the-dev-containers-promise",
    "href": "posts/2025-03-14_til_vscode-gpu-containers/index.html#the-dev-containers-promise",
    "title": "TIL: Dev Containers and GPUs: A Match Made in Purgatory",
    "section": "The Dev Containers Promise",
    "text": "The Dev Containers Promise\nDocker solves many dependency management issues by encapsulating even many system-level dependencies in a container. However, developing with Docker can be challenging. If an IDE is connected to a virtual environment on the host machine while the code is running inside a container, then mismatches between those environments can make the IDE less helpful.\nVSCode dev containers promise to solve this issue by connecting the IDE to the Docker container environment. I had hoped that they would thus provide an easy solution for developing deep learning projects on GPUs with minimal environment setup."
  },
  {
    "objectID": "posts/2025-03-14_til_vscode-gpu-containers/index.html#why-dev-containers-are-not-a-full-solution",
    "href": "posts/2025-03-14_til_vscode-gpu-containers/index.html#why-dev-containers-are-not-a-full-solution",
    "title": "TIL: Dev Containers and GPUs: A Match Made in Purgatory",
    "section": "Why Dev Containers Are Not a Full Solution",
    "text": "Why Dev Containers Are Not a Full Solution\nAlas, Docker does not encapsulate everything that is necessary to work with the GPU hardware: accessing GPUs from inside a Docker container requires installing NVIDIA drivers as well as the NVIDIA Container Toolkit on the host machine."
  },
  {
    "objectID": "posts/2025-03-14_til_vscode-gpu-containers/index.html#how-to-use-dev-containers-with-an-nvidia-gpu",
    "href": "posts/2025-03-14_til_vscode-gpu-containers/index.html#how-to-use-dev-containers-with-an-nvidia-gpu",
    "title": "TIL: Dev Containers and GPUs: A Match Made in Purgatory",
    "section": "How to Use Dev Containers with an NVIDIA GPU",
    "text": "How to Use Dev Containers with an NVIDIA GPU\nDespite this limitation, dev containers are still useful. They can encapsulate most dependencies, include the CUDA toolkit and any Python library dependencies, as well as a VSCode configuration. See this PR for an example of how to set one up for a GPU-enabled deep learning project."
  },
  {
    "objectID": "posts/2019-12-02_evaluating-classifiers-pt4/evaluating_classifiers_pt4.html",
    "href": "posts/2019-12-02_evaluating-classifiers-pt4/evaluating_classifiers_pt4.html",
    "title": "Evaluating Classification Models, Part 4: Weighted Power Means of Precision and Recall",
    "section": "",
    "text": "G\\(_{\\beta,\\rho}\\) generalizes the popular F\\(_\\beta\\) family of classification model evaluation metrics.\nThis post is part of a series on evaluating classification models:\nThis series differs from other discussions of evaluation metrics for classification models in that it aims to provide a systematic perspective. Rather than providing a laundry list of individual metrics, it situates those metrics within a fairly comprehensive family and explains how you can choose a member of that family that is appropriate for your use case.\nThe previous post describes the weighted “Pythagorean” (arithmetic, geometric, and harmonic) means of precision and recall. The arithmetic mean with weight \\(\\gamma\\) says that you are willing to trade one unit of precision for \\(\\gamma\\) or more units of recall, regardless of the current precision and recall. It is appropriate in situations such as medical testing where an individual false positive or false negative is basically unaffected by the total number of false positives and false negatives. For a harmonic mean, by contrast, the number of points of units you require in exchange for one unit of precision depends on the current precision and recall. The same is true for a geometric mean, but to a smaller degree. As a result, a geometric or harmonic mean may be appropriate in settings such as information retrieval where the cost of a false positive or false negative depends on the “big picture” of the model’s outputs.\nThis post discusses the full class of weighted power means of precision and recall, which includes the Pythagorean means as special cases. It may be overkill for practical purposes: an appropriately weighted arithmetic or harmonic mean is good enough for most problems, and using anything else will probably confuse your collaborators. However, it will give you a stronger handle on how the Pythagorean means work as well as additional options for handling unusual situations."
  },
  {
    "objectID": "posts/2019-12-02_evaluating-classifiers-pt4/evaluating_classifiers_pt4.html#demonstration-that-beta-does-its-job",
    "href": "posts/2019-12-02_evaluating-classifiers-pt4/evaluating_classifiers_pt4.html#demonstration-that-beta-does-its-job",
    "title": "Evaluating Classification Models, Part 4: Weighted Power Means of Precision and Recall",
    "section": "Demonstration That \\(\\beta\\) Does Its Job",
    "text": "Demonstration That \\(\\beta\\) Does Its Job\nI have said that \\(\\beta\\) specifies the ratio of recall to precision where you would be equally happy with a one-unit increase in precision or a one-unit increase in precision. Let’s demonstrate this result. Stated precisely, it says that the partial derivatives of \\(G_{\\beta,\\rho}\\) with respect to \\(R\\) and \\(P\\) are equal when \\(R/P=\\beta\\).\n\\[\n\\begin{align*}\n\\frac{\\partial}{\\partial R} \\left[ \\left( \\frac{1}{1 + \\beta^{\\rho}} \\left( \\beta^{\\rho} P^{\\rho+1} + R^{\\rho+1} \\right) \\right)^{\\frac{1}{\\rho+1}} \\right]\n&=\n\\frac{\\partial}{\\partial P} \\left[ \\left( \\frac{1}{1 + \\beta^{\\rho}} \\left( \\beta^{\\rho} P^{\\rho+1} + R^{\\rho+1} \\right) \\right)^{\\frac{1}{\\rho+1}} \\right] \\\\\n\\frac{1}{\\rho+1} \\left[ \\frac{1}{1 + \\beta^{\\rho}} \\left( \\beta^{\\rho} P^{\\rho+1} + R^{\\rho+1} \\right) \\right]^{\\frac{-\\rho}{\\rho+1}} \\left(\\frac{\\rho+1}{1 + \\beta^{\\rho}} \\right) R^{\\rho}\n&=\n\\frac{1}{\\rho+1} \\left[ \\frac{1}{1 + \\beta^{\\rho}} \\left( \\beta^{\\rho} P^{\\rho+1} + R^{\\rho+1} \\right) \\right]^{\\frac{-\\rho}{\\rho+1}} \\left( \\frac{\\rho+1}{1 + \\beta^{\\rho}} \\right) \\beta^{\\rho} P^{\\rho} \\\\\nR^{\\rho} &= \\beta^{\\rho} P^{\\rho} \\\\\n\\beta &= \\frac{R}{P}\n\\end{align*}\n\\]\nThis derivation works unless \\(\\rho=0\\). In that case we are dealing with a weighted arithmetic mean, so there are no values of \\(R\\) and \\(P\\) at which we are willing to trade one unit of recall for one unit of precision unless \\(\\beta=1\\), and in that case we are willing to do so at every value of \\(R\\) and \\(P\\)."
  },
  {
    "objectID": "posts/2019-12-02_evaluating-classifiers-pt4/evaluating_classifiers_pt4.html#demonstration-that-rho-does-its-job",
    "href": "posts/2019-12-02_evaluating-classifiers-pt4/evaluating_classifiers_pt4.html#demonstration-that-rho-does-its-job",
    "title": "Evaluating Classification Models, Part 4: Weighted Power Means of Precision and Recall",
    "section": "Demonstration That \\(\\rho\\) Does Its Job",
    "text": "Demonstration That \\(\\rho\\) Does Its Job\nThe “curvature” parameter \\(\\rho\\) characterizes how our concern for recall relative to precision changes as we move away from the neutral line \\(R/P=\\beta\\). More precisely, it characterizes the rate of change of the slope of the tangent line to a level curve \\(R/P=\\beta\\) along the direction of that tangent line.\nThe slope of the tangent line to a level curve of the weighted power mean is given by the ratio of the partial derivative of the weighted power mean with respect to recall to its partial derivative with respect to precision. Call this quantity \\(S\\). Then we have\n\\[\n\\begin{align*}\nS &= -\\frac{\\frac{\\partial}{\\partial R} \\left[ \\frac{1}{1 + \\beta^{\\rho}} \\left( \\beta^{\\rho} P^{\\rho+1} + R^{\\rho+1} \\right) \\right]^{\\frac{1}{\\rho+1}}}\n{\\frac{\\partial}{\\partial P} \\left[ \\frac{1}{1 + \\beta^{\\rho}} \\left( \\beta^{\\rho} P^{\\rho+1} + R^{\\rho+1} \\right) \\right]^{\\frac{1}{\\rho+1}}} \\\\\n&= -\\frac{\\frac{1}{\\rho+1} \\left[ \\frac{1}{1 + \\beta^{\\rho}} \\left( \\beta^{\\rho} P^{\\rho+1} + R^{\\rho+1} \\right) \\right]^{\\frac{-\\rho}{\\rho+1}} \\frac{\\rho+1}{1 + \\beta^{\\rho}} R^{\\rho} }\n{\\frac{1}{\\rho+1} \\left[ \\frac{1}{1 + \\beta^{\\rho}} \\left( \\beta^{\\rho} P^{\\rho+1} + R^{\\rho+1} \\right) \\right]^{\\frac{-\\rho}{\\rho+1}} \\frac{\\rho+1}{1 + \\beta^{\\rho}} \\beta^{\\rho} P^{\\rho}} \\\\\n&= -\\left( \\frac{R}{\\beta P} \\right)^{\\rho}\n\\end{align*}\n\\]\nWe want to characterize how the slope \\(S\\) of the tangent line to the level curve of the metric changes along that tangent line, so we need to evaluate the partial derivative of \\(S\\) with respect to \\(t=R-P\\). By the chain rule,\n\\[\n\\begin{align*}\n\\frac{\\partial S}{\\partial t} &= \\frac{\\partial S}{\\partial R} \\frac{\\partial R}{\\partial t} + \\frac{\\partial S}{\\partial P} \\frac{\\partial P}{\\partial t} \\\\\n&= \\frac{\\partial}{\\partial R} \\left[ -\\left( \\frac{R}{\\beta P} \\right)^{\\rho} \\right] \\frac{\\partial}{\\partial t} (t + P)\n+ \\frac{\\partial}{\\partial P} \\left[ -\\left( \\frac{R}{\\beta P} \\right)^{\\rho} \\right] \\frac{\\partial}{\\partial t} (R - t) \\\\\n&= -\\rho \\left( \\frac{R}{\\beta P} \\right)^{\\rho} R^{-1} - \\rho \\left( \\frac{R}{\\beta P} \\right)^{\\rho} P^{-1} \\\\\n&= -\\rho \\left( \\frac{R}{\\beta P} \\right)^{\\rho} \\left( R^{-1} + P^{-1} \\right)\n\\end{align*}\n\\]\nSo\n\\[\n\\left. \\frac{\\partial S}{\\partial t} \\right|_{\\beta = \\frac{R}{P}} = -\\rho \\left( R^{-1} + P^{-1} \\right)\n\\]\nThus, for a given \\(R\\) and \\(P\\) along the line \\(\\beta=R/P\\), the curvature of the level curve along the direction of its tangent line is proportional to \\(\\rho\\)."
  },
  {
    "objectID": "posts/2025-05-14_grpc_python_imports/index.html",
    "href": "posts/2025-05-14_grpc_python_imports/index.html",
    "title": "TIL: Fixing gRPC Python Imports",
    "section": "",
    "text": "I was reacquainting myself with gRPC by creating a basic “Hello, world!” project when I ran into a problem: I wanted to put the generated code files in their own generated_grpc directory, but then imports within those files failed:\nModuleNotFoundError: No module named 'helloworld_pb2'\nIt turns out that protobuf simply does not generate correct imports in this situation. LLMs suggested adding an option python_package line to my .protos file, but that option is not supported. Actual solutions include putting the generated files in the project root directory and wrapping the protobuf generation command in a script that fixes the imports.\nThe approach I landed on is to add a few lines to the __init__.py file within the generated_grpc/ package:\nfrom importlib import import_module\nimport sys\n\n_messages_module = import_module(\".helloworld_pb2\", package=__name__)\n\nsys.modules.setdefault(\"helloworld_pb2\", _messages_module)\nI prefer this approach to the alternatives because it is only annoying once. It allows me to use standard commands and a sensible directory structure. The only cost is a bit of ugliness in a file that I will probably never look at again. I can live with that!"
  },
  {
    "objectID": "posts/2019-11-26_evaluating-classifiers-pt3/evaluating_classifiers_pt_3.html",
    "href": "posts/2019-11-26_evaluating-classifiers-pt3/evaluating_classifiers_pt_3.html",
    "title": "Evaluating Classification Models, Part 3: Fᵦ and Other Weighted Pythagorean Means of Precision and Recall",
    "section": "",
    "text": "There are many ways to combine precision and recall into an overall measure of model performance. Which one should you use?\n\n\nThis post is part of a series on evaluating classification models:\n\nPart 1: Weighing False Positives Against False Negatives explains why we need systematic ways to evaluate classification models.\nPart 2: The Sufficiency of Precision and Recall explains why precision and recall are sufficient for evaluating classification models in typical cases.\nPart 3: \\(F_\\beta\\) and Other Weighted Pythagorean Means of Precision and Recall explains what patterns of preferences are encoded by the Pythagorean means of precision and recall. This class of metrics includes the popular \\(F_\\beta\\) family, among others.\nPart 4: Weighted Power Means of Precision and Recall generalizes beyond the Pythagorean means to the broader class of weighted power means of precision and recall.\n\nThis series differs from other discussions of evaluation metrics for classification models in that it aims to provide a systematic perspective. Rather than providing a laundry list of individual metrics, it situates those metrics within a fairly comprehensive family and explains how you can choose a member of that family that is appropriate for your use case.\nThis post explains how the three weighted “Pythagorean means” (arithmetic, geometric, and harmonic) of precision and recall encode preferences over models.\n\nAn Example\nSuppose we build two different models, and one has better precision while the other has better recall.\nModel A:\n\n\n\n\nPredicted No\nPredicted Yes\nTotal\n\n\n\n\nActual No\n90\n10\n100\n\n\nActual Yes\n1\n99\n100\n\n\nTotal\n91\n109\n200\n\n\n\n\\[\n\\text{Precision} = \\frac{|\\text{True Positive}|}{|\\text{True Positive}| + |\\text{False Positive}|} = \\frac{90}{99 + 10} = 90.8\\%\n\\]\n\\[\n\\text{Recall} = \\frac{|\\text{True Positive}|}{|\\text{True Positive}| + |\\text{False Negative}|} = \\frac{99}{99 + 1} = 99.0\\%\n\\]\nModel B:\n\n\n\n\nPredicted No\nPredicted Yes\nTotal\n\n\n\n\nActual No\n91\n9\n100\n\n\nActual Yes\n3\n97\n100\n\n\nTotal\n94\n106\n200\n\n\n\n\\[\n\\text{Precision} = \\frac{|\\text{True Positive}|}{|\\text{True Positive}| + |\\text{False Positive}|} = \\frac{91}{91 + 9} = 91.5\\%\n\\]\n\\[\n\\text{Recall} = \\frac{|\\text{True Positive}|}{|\\text{True Positive}| + |\\text{False Negative}|} = \\frac{97}{97 + 3} = 97.0\\%\n\\]\nTo choose between these models, we need to decide whether the gain from \\(90.8\\%\\) precision to \\(91.5\\%\\) precision that we get by going from Model A to Model B is enough to offset a loss from \\(99\\%\\) recall to \\(97\\%\\) recall. We need some way to combine precision and recall into a single evaluation metric.\n\n\nThe Weighted Arithmetic Mean \\(A_\\gamma\\)\nWhen considering how to combine precision and recall to produce an overall evaluation metric, a natural first thought is simply to average them:\n\\[\n\\text{Simple Arithmetic Mean}: \\frac{1}{2}(P + R)\n\\]\nwhere \\(P\\) is precision and \\(R\\) is recall.\nIn the scenario above, going from Model A to Model B increases precision by \\(0.7\\%\\) but decreases recall by \\(2.0\\%\\). A simple arithmetic mean of precision and recall would tell us not to make this trade. It implies that we are willing to trade a decrease of \\(2.0\\%\\) in recall only for an increase of \\(2.0\\%\\) or more in precision. More generally, a simple arithmetic mean says that we are willing to trade one unit of recall only for one or more units of precision, and vice versa, regardless of the current precision and recall.\nWe can visualize this pattern of preferences using a set of “level curves.” In this plot, the simple arithmetic mean would tell us that two models that fall on the same curve are equally good, and that a model that lies on a curve up and to the right of another model is better than that other model.\n\n\nAnother possible pattern of preferences is to be just willing to trade one unit of precision for \\(\\gamma\\) units of recall for some \\(\\gamma > 0\\). This pattern is captured by a weighted arithmetic mean:\n\\[\n\\text{Weighted Arithmetic Mean} (\\gamma): \\frac{1}{1 + \\gamma}\\left(\\gamma P + R\\right)\n\\]\nThe level curves for this pattern of preferences are straight lines with slope \\(-\\gamma\\):\n\n\nTo help wrap your head around how to read the plot above, move the slider all the way to the right (\\(\\gamma=8\\)). Observe that the level curves become nearly horizontal. Pick a point on one of the lines, such as (\\(x=.48\\), \\(y=.84\\)), shown up close below. That point represents a model with \\(48\\%\\) recall and \\(84\\%\\) precision. If that model lost one percentage point of precision (corresponding to a shift down one unit to \\(y=.83\\)), it would have to gain \\(\\gamma=8\\) points of recall (corresponding to a shift right eight units to \\(x=.56\\)) to get back to the level curve it started on. In other words, the plot says that we would require 8 units of recall in order to give up one unit of precision, indicating that we care much more about precision than recall.\n\n\n\nThe slope of the level curve at a given point tells us how many points of recall (\\(\\gamma\\)) we would require in exchange for one point of precision for a model with the recall and precision represented by that point.\n\n\n\n\nThe Weighted Harmonic Mean \\(F_\\beta\\)\nIn some cases, the arithmetic mean is inappropriate because the amount of recall we would require in order to give up one unit of precision is not fixed; instead, it depends on the current recall and precision values. One option in those cases is to use a weighted harmonic mean of precision and recall instead of a weighted arithmetic. The result is the popular \\(F_\\beta\\) score.\nThe weighted harmonic mean is the inverse of the arithmetic mean of the inverses:\n\\[\n\\text{Weighted Harmonic Mean} (\\gamma): \\left(\\frac{1}{1 + \\gamma}\\left(\\gamma P^{-1} + R^{-1}\\right)\\right)^{-1}\n\\]\nIt is convenient to use the parameter \\(\\beta\\) such that \\(\\beta^2 = \\frac{1}{\\gamma}\\) in place of \\(\\gamma\\), which results in the following equivalent expression:\n\\[\n\\text{Weighted Harmonic Mean} (\\beta): \\left(\\frac{1}{1 + \\beta^2}\\left(P^{-1} + \\beta^2R^{-1}\\right)\\right)^{-1}\n\\]\nWith a weighted arithmetic mean \\(A_\\gamma\\), the relative importance of precision and recall is fixed: that metric says that we are willing to trade one unit of precision for \\(\\gamma\\) or more units of recall regardless of the current precision and recall. With a weighted harmonic mean \\(F_\\beta\\), by contrast, the relative importance of precision and recall depends on their current values. For instance, suppose that recall is \\(80\\%\\). If precision is \\(98\\%\\), then you might value a \\(1\\%\\) increase in recall more than a \\(1\\%\\) increase in precision, whereas if precision is \\(10\\%\\), then you might care more about increasing precision.\nA metric that places more importance on recall when recall is low compared to precision and vice versa has level curves that curve upward:\n\n\nOne way to think about the harmonic mean is that it deviates from the arithmetic mean in the direction of the minimum, as we will see more clearly when we discuss weighted power means in the next post in this series. The weighted harmonic mean of a set of numbers with a given weighting \\(\\gamma\\) is always less than the arithmetic mean with the same \\(\\gamma\\) (unless all of the numbers are the same, in which case the arithmetic and harmonic means are equal.)\nOne benefit of using \\(\\beta\\) instead of \\(\\gamma\\) is that \\(\\beta\\) indicates how much we care about recall relative to precision. More precisely, \\(\\beta\\) is the ratio of recall to precision where the harmonic mean places equal value on a one-unit increase in recall and a one-unit increase in precision (in the limit as the size of the unit goes to zero). If recall is more important to you than precision, for instance, then you would set \\(\\beta\\) greater than one because you would rather have a one-unit increase in recall than a one-unit increase in precision unless recall is already high compared to precision.\nFor instance, \\(\\beta=3\\) means that if precision is \\(25\\%\\), then you would rather have one more unit of precision than one more unit of recall if recall is above \\(75\\%\\), but you would rather have the recall if recall is below \\(75\\%\\) (because \\(75\\%/25\\%=3\\)).\nYou narrow down the range of \\(\\beta\\) values that are appropriate for a particular application by getting stakeholder feedback on different combinations of precision and recall values. For instance, at one point we had a model at ShopRunner that we had tuned for \\(F_\\beta\\) with \\(\\beta=.3\\), meaning that we cared about recall about \\(30\\%\\) as much as we cared about precision. That model classified products with \\(90\\%\\) precision and \\(36\\%\\) recall. When we showed those results to our primary stakeholder, she expressed more concern about the \\(36\\%\\) recall than the \\(90\\%\\) precision. Assuming that \\(F_\\beta\\) captured the overall shape of her preferences, this feedback entailed that her \\(\\beta\\) was greater than \\(36\\%/90\\%=.4\\). After further discussion, we started tuning for \\(F_\\beta\\) with \\(\\beta=.5\\) instead of \\(.3\\).\nIn the plot below, the thick black line indicates the set of models where you would be equally happy with one more point of precision or one more point of recall. Above the line, you would rather have one point of recall. Below the line, you would rather have one point of precision. Larger values of \\(\\beta\\) mean that you care more about recall overall, so there is more area below the line.\n\n\nIn these plots, the number of units of recall that you require in exchange for one unit of precision when recall is \\(R\\) and precision is \\(P\\) is given by the slope of the tangent line to the level curve that passes through (\\(x=R, y=P\\)). For instance, move the slider all the way to the right (\\(\\beta=8\\)). Exactly on the black line, the tangent to each level curve has slope \\(-1/1\\), indicating that you would exchange one unit of precision for one unit of recall, as shown below.\n\n\n\nAlong the black line that passes through the points where \\(\\text{Recall}/\\text{Precision}=\\beta\\), the tangent to a level curve has slope \\(-1/1\\), meaning that you would require one unit of recall in exchange for one unit of precision (in the limit as the size of the unit goes to zero).\n\n\nBelow the black line, where precision is high compared to recall, the tangent to each level curve has slope \\(-1/\\delta\\) for some \\(\\delta>1\\), indicating that you would require more than one unit of recall in exchange for one unit of precision, as shown below. In other words, with a harmonic mean, when precision is high enough compared to recall, you care about improving recall more than you care about improving precision. This region of the plot is large in this case because \\(\\beta>1\\), indicating that we care about recall more than precision overall.\n\n\n\nBelow the black line that passes through the points where \\(\\text{Recall}/\\text{Precision}=\\beta\\), the tangent to a level curve has slope \\(-1/\\delta\\) for some \\(\\delta>1\\), meaning that you would require more than one unit of recall in exchange for one unit of precision.\n\n\nAbove the black line, where recall is high compared to precision, the tangent to each level curve has slope \\(-1/\\delta\\) for some \\(\\delta<1\\), indicating that you would require less than one unit of recall in exchange for one unit of precision, as shown below. In other words, with a harmonic mean, when recall is high enough compared to precision, you care about improving precision more than you care about improving recall. This region of the plot is small in this case because \\(\\beta>1\\), indicating that we care about recall more than precision overall.\n\n\n\nBelow the black line that passes through the points where \\(\\text{Recall}/\\text{Precision}=\\beta\\), the tangent to a level curve has slope \\(-1/\\delta\\) for some \\(\\delta<1\\), meaning that you would require less than one unit of recall in exchange for one unit of precision.\n\n\nFor the arithmetic mean, unlike the harmonic mean just discussed, the level curves are straight lines. As a result, the tangent to a level curve is just the level curve itself. Moreover, all of the level curves have the same slope \\(-1/\\gamma\\), corresponding to the fact that with an arithmetic mean you are willing to trade one unit of precision for \\(\\gamma\\) or more units of recall for every level of precision and recall.\n\n\nWhy Would You Use a Non-Arithmetic Mean?\nYou might wonder whether a harmonic mean of precision and recall is ever appropriate. The author of an excellent recent blog post suggested that it isn’t, writing that they had “never encountered a business problem where a real-life cost function of false positives and false negatives is a harmonic mean.” I claim on the contrary that a harmonic mean or something similar is often appropriate, in scenarios where the value of the algorithm depends on the overall impression created by its outputs rather than on separate effects of its individual predictions.\nAn arithmetic mean tends to be appropriate for diagnostic problems. For instance, in medical testing a false positive can cause unnecessary stress and expense, while a false negative can cause worse health outcomes because of a missed opportunity at early intervention. To a first approximation, those costs are specific to each patient and do not depend on how many other false positives or false negatives the test generates. As a result, we should be willing to trade precision for recall at the same rate regardless of the current precision and recall, which makes an arithmetic mean appropriate.\nBy contrast, a harmonic mean tends to be appropriate for information retrieval problems. For instance, consider the model we discussed in Part 1 of this series that identifies whether a product is a dress or not for an app that sells dresses. Suppose that the pool of products being classified is enormous — every dress for sale on the internet, if you like. Then if recall is quite high, say \\(98\\%\\), then another point of recall doesn’t mean much: someone who is looking for a dress will already be able to find one that they love. If precision is low in this scenario, then it would be better to focus on improving precision. Whereas if precision is very high, say \\(98\\%\\), then encountering a product that should not be in the app because it is not a dress is a rare experience that users will plausibly overlook. If recall is low in that scenario, so that users are not always able to find the perfect dress even though it is out there somewhere, then it would be better to focus on recall. An arithmetic mean cannot capture this pattern of preferences, while a harmonic mean can.\n\n\n\nIf we had a lot of non-dresses in the dress feed of ShopRunner’s District app, then we would prioritize increasing precision for that category. By contrast, when precision is high we care more about increasing recall so that we can offer more dress options. An arithmetic mean of precision and recall cannot capture this situation, but a harmonic mean can.\n\n\nThat being said, a harmonic mean is not the only option that captures this general pattern of preferences. For instance, a geometric mean also puts more weight on precision or recall the lower it is, but to a smaller degree than a harmonic mean.\n\n\nThe Weighted Geometric Mean \\(G_\\gamma\\)\nThe geometric mean is intermediate between the harmonic mean and the arithmetic mean: unless all of the inputs are equal, a weighted geometric mean is strictly greater than the corresponding weighted harmonic mean and strictly less than the corresponding arithmetic mean. (If the inputs are all equal, then all three means are equal to their common value.)\nThe simple geometric mean of two numbers can be calculated by multiplying them and then taking the square root of the result:\n\\[\n\\text{Simple Geometric Mean}: \\left(PR\\right)^{1/2}\n\\]\nWe can incorporate weights by taking precision to the power \\(\\gamma\\) before multiplying and taking the result to the power \\(1/(1+\\gamma)\\):\n\\[\n\\text{Weighted Geometric Mean} (\\gamma): \\left(P^\\gamma R\\right)^{1/(1+\\gamma)}\n\\]\nWe can change this expression so that the parameter that controls the weighting has the same interpretation as \\(\\beta\\) in the weighted harmonic mean \\(F_\\beta\\). We will use the same parameter name “\\(\\beta\\),” but in this case \\(\\beta=1/\\gamma\\):\n\\[\n\\text{Weighted Geometric Mean} (\\beta): \\left(P R^\\beta\\right)^{1/(1+\\beta)}\n\\]\nHere again \\(\\beta\\) is the ratio of recall to precision at which the metric responds equally to an increase in precision and an increase in recall, so that higher \\(\\beta\\) corresponds to placing more importance on recall relative to precision.\nThe level curves for the weighted geometric mean curve upward like those for the weighted harmonic mean, but to a smaller degree:\n\n\nThe weighted geometric mean deviates from the weighted arithmetic mean in the direction of the minimum, again like the harmonic mean but to a smaller degree.\n\n\n\nThe harmonic mean deviates from the arithmetic mean in the direction of the minimum, so it is more sensitive to whichever is smaller between the precision and the recall. The geometric mean has the same property to a smaller degree.\n\n\n\n\nWhich Mean Should You Use?\nWhen each false positive or false negative has roughly the same expected cost regardless of how many false positives or false negatives the model generates overall, you should use an arithmetic mean of precision and recall to evaluate a model. Models used for medical diagnosis often have this property.\nBut when what matters is the overall impression that the model’s outputs create, you might use something like a geometric or harmonic mean that prioritizes whichever of precision or recall is lagging behind. Models used for information retrieval often have this property.\nSubsequent posts in this series will situate the arithmetic, geometric, and harmonic means within a larger family of metrics and provide additional guidance on how to choose among them.\n\n\nAcknowledgements\nThanks to Nicole Carlson, Morgan Cundiff, and the rest of the ShopRunner data science team for comments on earlier versions of this material.\nOriginally published at medium.com"
  },
  {
    "objectID": "posts/2021-08-05_pandas-styling/pandas_styling.html",
    "href": "posts/2021-08-05_pandas-styling/pandas_styling.html",
    "title": "Problem Solved: Custom Pandas styling not rendering when reopening notebook",
    "section": "",
    "text": "Pandas has a variety of methods for styling notebooks. I have been using those methods in the model_inspector library to display correlation matrices and confusion matrices in Jupyter notebooks with appropriate highlighting.\n\n\n\nA correlation matrix rendered with appropriate coloring using Pandas styling methods in the model_inspector library. Note that it uses a diverging colormap so that positive values are red and negative values are blue. I often see sequential colormaps used here, which make it hard to distinguish between positive, negative, and near-zero values.\n\n\nI recently encountered a problem with this approach: DataFrames were not rendering with custom styling when I reopened a notebook after saving it. After an hour or two of fruitless Googling, I figured out why: the notebook was not trusted, so Jupyter was not running the code to do the custom rendering when I reopened it. Running the Terminal command jupyter trust my_notebook.ipynb fixed the problem."
  },
  {
    "objectID": "posts/2025-05-07_llm-test-coverage/index.html",
    "href": "posts/2025-05-07_llm-test-coverage/index.html",
    "title": "Enriching Test Coverage Reports for LLMs",
    "section": "",
    "text": "LLMs are useful for addressing test coverage gaps, but they need a bit of help. (Illustration generated with the help of ChatGPT.)"
  },
  {
    "objectID": "posts/2025-05-07_llm-test-coverage/index.html#the-allure-of-ai-powered-test-coverage",
    "href": "posts/2025-05-07_llm-test-coverage/index.html#the-allure-of-ai-powered-test-coverage",
    "title": "Enriching Test Coverage Reports for LLMs",
    "section": "The Allure of AI-Powered Test Coverage",
    "text": "The Allure of AI-Powered Test Coverage\nAchieving high test coverage can be a slog. Manually writing tests for every nook and cranny is time-consuming and tedious. It often requires ugly and unintuitive boilerplate to create all the necessary mocks, stubs, fakes, and so on.\nIt is tempting to turn to LLMs to address this problem. They are very good at writing boilerplate and have infinite patience for tedious tasks. The workflow seems straightforward: run tests, generate a coverage report, identify the gaps you care about, feed the report to an LLM, ask it to fill the relevant gaps, review the results, and edit them as needed.\nHowever, this approach hits a snag: LLMs are not great at interpreting standard test coverage reports. These reports typically provide line numbers indicating uncovered code. However, LLMs often misidentify these lines. They will guess a line in the right vicinity and then will often tell you that the line is covered and suggest that the coverage tool is not working properly.\nThis behavior is not entirely surprising. LLMs can struggle with precise counting tasks (e.g., “How many ‘r’s in ’strawberry’?”). They also sometimes refer to suggested edits from earlier in a conversation that might not match the current state of the file. My experience with Gemini 2.5 via Cursor shows that providing a line number isn’t a reliable way to direct the LLM to the specific code in question.\nFor instance, in one of my development sessions, a coverage report indicated lines 456-457 of a file were untested. Those lines contained some error handling, but the LLM instead latched onto a nearby pragma: no cover comment and an adjacent blank line and tried to tell me that the coverage tool was misbehaving. I had to correct it repeatedly and eventually feed it the exact code that was uncovered."
  },
  {
    "objectID": "posts/2025-05-07_llm-test-coverage/index.html#bridging-the-gap-a-better-coverage-report",
    "href": "posts/2025-05-07_llm-test-coverage/index.html#bridging-the-gap-a-better-coverage-report",
    "title": "Enriching Test Coverage Reports for LLMs",
    "section": "Bridging the Gap: A Better Coverage Report",
    "text": "Bridging the Gap: A Better Coverage Report\nThe solution lies in enhancing our coverage reports. The pytest-cov extension for pytest doesn’t offer a built-in way to directly output the content of missing lines to the terminal. It can show line numbers or generate annotated files, but for LLM interaction, we need a more direct approach: a list of the actual code lines that are missing coverage, annotated with their original line numbers to help the LLM (and us) locate them.\nHere is a bash script that addresses this problem. It runs pytest with coverage, and if the tests pass, it then processes the coverage data to output the specific lines of code that were not executed, along with their file names and line numbers.\n#!/usr/bin/env bash\nset -uo pipefail\n\nsource .env\n\n#######################################################################################\n# 1. Run the tests with coverage and capture exit status\n#######################################################################################\nuv run pytest \\\n  --cov=. \\\n  --cov-report=annotate\ntest_status=$?\n\n#######################################################################################\n# 2. Generate temp coverage files and show missing-line details if tests passed\n#######################################################################################\nif [ $test_status -eq 0 ] && [ -f .coverage ]; then\n  echo -e \"\\nMissing-line details:\"\n  grep -R --line-number '^!' --include='*.py,cover' . \\\n  | awk -F: '{sub(/,cover$/,\"\",$1); gsub(/^!/,\"\",$3); print $1 \":\" $2 \": \" $3}'\nfi\n\n#######################################################################################\n# 3. Enforce coverage threshold\n#######################################################################################\nthreshold=100\nif [ $test_status -eq 0 ]; then\n  uv run coverage report --fail-under=$threshold\n  cov_status=$?\nelse\n  cov_status=0\nfi\n\n#######################################################################################\n# 4. Always clean up temp files\n#######################################################################################\nfind . -name '*.py,cover' -delete\nrm -f .coverage\n\n#######################################################################################\n# 5. Propagate the correct exit code\n#######################################################################################\nif [ $test_status -ne 0 ]; then\n  exit $test_status         # tests failed\nfi\nexit $cov_status            # 0 if good, 2 if coverage too low\nLLMs have a much easier time identifying the correct lines to address based on the output of this script:"
  },
  {
    "objectID": "posts/2025-05-07_llm-test-coverage/index.html#conclusion-a-tool-not-a-panacea",
    "href": "posts/2025-05-07_llm-test-coverage/index.html#conclusion-a-tool-not-a-panacea",
    "title": "Enriching Test Coverage Reports for LLMs",
    "section": "Conclusion: A Tool, Not a Panacea",
    "text": "Conclusion: A Tool, Not a Panacea\nIt is important to keep in mind that test coverage is just a heuristic. Even 100% test coverage is not sufficient to guarantee that code is well-tested, nor is it necessary. It might be more trouble than it is worth, even with LLMs, especially for code that is changing rapidly.\nThat said, checking for coverage gaps can be a useful way to find code that is not as well-tested as it should be. LLMs can make addressing those gaps easier than ever, especially if we use something like the script above to give them the relevant information in a format that they can work with effectively."
  },
  {
    "objectID": "posts/2018-03-10_ordered-dicts/ordered_dicts.html",
    "href": "posts/2018-03-10_ordered-dicts/ordered_dicts.html",
    "title": "Python Dictionaries Are Now Ordered. Keep Using OrderedDict.",
    "section": "",
    "text": "Until recently, Python dictionaries did not preserve the order in which items were added to them. For instance, you might type {'fruits': ['apples', 'oranges'], 'vegetables': ['carrots', 'peas']} and get back {'vegetables': ['carrots', 'peas'], 'fruits': ['apples', 'oranges']}. If you wanted a dictionary that preserved order, you could use the OrderedDict class in the standard library module collections.\nHowever, this situation is changing. Standard dict objects preserve order in the reference (CPython) implementations of Python 3.5 and 3.6, and this order-preserving property is becoming a language feature in Python 3.7.\nYou might think that this change makes the OrderedDict class obsolete. However, there are at least two good reasons to continue using OrderedDict. First, relying on standard dict objects to preserve order will cause your code to break on versions of CPython earlier than 3.5 and on some alternative implementations of Python 3.5 and 3.6. Second, using an OrderedDict communicates your intention to rely on the order of items in your dictionary being preserved, both to human readers of your code and to the third-party libraries you call within it.\nIn fact, some tools that are widely used in data science assume that the order of items in a dict is not significant."
  },
  {
    "objectID": "posts/2018-03-10_ordered-dicts/ordered_dicts.html#ipython-assumes-that-dict-order-doesnt-matter",
    "href": "posts/2018-03-10_ordered-dicts/ordered_dicts.html#ipython-assumes-that-dict-order-doesnt-matter",
    "title": "Python Dictionaries Are Now Ordered. Keep Using OrderedDict.",
    "section": "IPython Assumes That dict Order Doesn’t Matter",
    "text": "IPython Assumes That dict Order Doesn’t Matter\nConsider this example IPython session:\nIn [1]: from string import ascii_lowercase\n\nIn [2]: my_dict = {letter: [1] for letter in ascii_lowercase[::-1]}\n\nIn [3]: print(my_dict)\n{'z': [1], 'y': [1], 'x': [1], 'w': [1], 'v': [1], 'u': [1], 't': [1], 's': [1], 'r': [1], 'q': [1], 'p': [1], 'o': [1], 'n': [1], 'm': [1], 'l': [1], 'k': [1], 'j': [1], 'i': [1], 'h': [1], 'g': [1], 'f': [1], 'e': [1], 'd': [1], 'c': [1], 'b': [1], 'a': [1]}\n\nIn [4]: my_dict\nOut[4]:\n{'a': [1],\n'b': [1],\n'c': [1],\n'd': [1],\n'e': [1],\n'f': [1],\n'g': [1],\n'h': [1],\n'i': [1],\n'j': [1],\n'k': [1],\n'l': [1],\n'm': [1],\n'n': [1],\n'o': [1],\n'p': [1],\n'q': [1],\n'r': [1],\n's': [1],\n't': [1],\n'u': [1],\n'v': [1],\n'w': [1],\n'x': [1],\n'y': [1],\n'z': [1]}\nHere it appears that my_dict does not in fact preserve order consistently. What is actually happening, however, is that the IPython kernel sorts the dictionary keys for display when the dictionary is evaluated. IPython assumes that the ordering of items in a dict object is not significant. By contrast, it does not sort the keys in an OrderedDict:\nIn [5]: from collections import OrderedDict\n\nIn [6]: my_ordered_dict = OrderedDict([(letter, [1]) for letter in ascii_lowercase[::-1]])\n\nIn [7]: print(my_ordered_dict)\nOrderedDict([('z', [1]), ('y', [1]), ('x', [1]), ('w', [1]), ('v', [1]), ('u', [1]), ('t', [1]), ('s', [1]), ('r', [1]), ('q', [1]), ('p', [1]), ('o', [1]), ('n', [1]), ('m', [1]), ('l', [1]), ('k', [1]), ('j', [1]), ('i', [1]), ('h', [1]), ('g', [1]), ('f', [1]), ('e', [1]), ('d', [1]), ('c', [1]), ('b', [1]), ('a', [1])])\n\nIn [8]: my_ordered_dict\nOut[8]:\nOrderedDict([('z', [1]),\n('y', [1]),\n('x', [1]),\n('w', [1]),\n('v', [1]),\n('u', [1]),\n('t', [1]),\n('s', [1]),\n('r', [1]),\n('q', [1]),\n('p', [1]),\n('o', [1]),\n('n', [1]),\n('m', [1]),\n('l', [1]),\n('k', [1]),\n('j', [1]),\n('i', [1]),\n('h', [1]),\n('g', [1]),\n('f', [1]),\n('e', [1]),\n('d', [1]),\n('c', [1]),\n('b', [1]),\n('a', [1])])\n(The relevant bit of IPython code is in its _dict_pprinter_factory function.)"
  },
  {
    "objectID": "posts/2018-03-10_ordered-dicts/ordered_dicts.html#pandas-assumes-that-dict-order-doesnt-matter",
    "href": "posts/2018-03-10_ordered-dicts/ordered_dicts.html#pandas-assumes-that-dict-order-doesnt-matter",
    "title": "Python Dictionaries Are Now Ordered. Keep Using OrderedDict.",
    "section": "Pandas Assumes That dict Order Doesn’t Matter",
    "text": "Pandas Assumes That dict Order Doesn’t Matter\nUpdate: pandas 0.23.0 (released May 15, 2018) does respect ordering when initializing a DataFrame or Series with a dict in Python 3.6+.\npandas also assumes that the ordering of items in a dict object is not significant. If you initialize a DataFrame with a dict, it sorts the items by key:\nIn [9]: import pandas as pd\n\nIn [10]: pd.DataFrame(my_dict)\nOut[10]:\na b c d e f g h i j ... q r s t u v w x y z\n0 1 1 1 1 1 1 1 1 1 1 ... 1 1 1 1 1 1 1 1 1 1\n\n[1 rows x 26 columns]\nIt does not sort the columns if you use an OrderedDict:\nIn [11]: pd.DataFrame(my_ordered_dict)\nOut[11]:\nz y x w v u t s r q ... j i h g f e d c b a\n0 1 1 1 1 1 1 1 1 1 1 ... 1 1 1 1 1 1 1 1 1 1\n\n[1 rows x 26 columns]\n(The relevant bit of pandas code is in the DataFrame class’s _init_dict method.)"
  },
  {
    "objectID": "posts/2018-03-10_ordered-dicts/ordered_dicts.html#how-adding-guarantees-can-be-dangerous",
    "href": "posts/2018-03-10_ordered-dicts/ordered_dicts.html#how-adding-guarantees-can-be-dangerous",
    "title": "Python Dictionaries Are Now Ordered. Keep Using OrderedDict.",
    "section": "How Adding Guarantees Can Be Dangerous",
    "text": "How Adding Guarantees Can Be Dangerous\nThese examples illustrate a surprising phenomenon: adding a guarantee can be dangerous. Users cannot rely on that guarantee until its existence becomes common knowledge and the implications of that common knowledge make their way through the community and into the relevant infrastructure.\nThis process will take a particularly long time in the case of the Python dict ordering guarantee because legacy Python 2.7 continues to be widely used and because many open-source packages are not well maintained. As a result, I expect that using OrderedDict to communicate intent to other developers and third-party libraries will remain a best practice for several years."
  },
  {
    "objectID": "posts/2025-02-18_til_llmfoundry-eval/til_llmfoundry-eval.html",
    "href": "posts/2025-02-18_til_llmfoundry-eval/til_llmfoundry-eval.html",
    "title": "TIL: How LLM Foundry Evaluates Performance On Open-Ended Math Problems",
    "section": "",
    "text": "Note\n\n\n\nThis is a TIL (“Today I Learned”) post. I expect it to be useful to my future self and maybe to others, but it is meant to be a quick, informal way to capture something I learned rather than a polished presentation.\n\n\n\nContext\nI am contributing to a research project on performance-efficient fine tuning of LLMs. That project involves fine tuning LLMs on math problems.\n\n\nProblem\nEvaluating LLM performance on open-ended math problems is difficult because LLMs can generate arbitrary text. Checking an LLM’s answer to an open-ended (as opposed to multiple-choice, for instance) math problem against ground truth requires extracting its answer from the generated text and then accounting for issues such as rounding (e.g. 0.33 vs. 0.333) and alternative representations (e.g. 0.33 vs. 1/3). HuggingFace recently released the Math-Verify library to address these issues and showed that it makes a massive difference to LLM math leaderboards.\nIn the meantime, we have been using LLM Foundry for model development. LLM Foundry has its own procedure for evaluating LLM math performance. By default, it simply checks whether LLM’s output starts with an exact match to one of a set of provided answers after some light post-processing to “lower text and remove punctuation, articles and extra whitespace”.\nThis approach does not always give the desired result. For instance, in this case the model seems to be generating multiple-choice options rather than answering the question. The first of those options matches the ground truth answer, so this evaluation procedure counts it as correct even though the model did not commit to it as its actual answer. (All of the examples below come from SmolLM2-135M on GSM8K. SmolLM2-135M is a base language model and is tiny by LLM standards, so it is not expected to perform well on these problems.)\n\n\n\n\n\n\nExample\n\n\n\n\nQuestion: When Freda cooks canned tomatoes into sauce, they lose half their volume. Each 16 ounce can of tomatoes that she uses contains three tomatoes. Freda’s last batch of tomato sauce made 32 ounces of sauce. How many tomatoes did Freda use?\nModel Output: “12\\nB: 16\\nC: 24\\nD: 32”\nGround Truth: “12”\nEvaluation: Correct\n\n\n\nAnother case leads to a comedy of errors: the model gets a numerically correct answer for entirely wrong reasons, and the evaluation procedure counts it as correct for the wrong reason.\n\n\n\n\n\n\nExample\n\n\n\n\nQuestion: Uriah’s book bag is getting too heavy for him. He needs to remove 15 pounds from it. His comic books weigh 1/4 pound each and his toys weigh 1/2 pound each. If he removes 30 comic books, how many toys does he need to remove?\nModel Output: “15 pounds x 1/4 pounds x 1/2 pounds = 15 pounds.”\nGround Truth: “15”\nEvaluation: Correct\n\n\n\nIn another case, the model gives the right answer, but the evaluation procedure counts it as incorrect because it does not put that answer at the start of its output.\n\n\n\n\n\n\nExample\n\n\n\n\nQuestion: Mico and Marco wanted to get to know each other. They realized that the sum of their ages is 20. What will be the sum of their ages in 10 years?\nModel Output: “20 + 20 = 40”\nGround Truth: “40”\nEvaluation: Incorrect\n\n\n\n\n\nNext Steps\nWe should be able to do better. LLM Foundry supports specifing a “metric” to use to evaluate the model’s performance. The default metric for “generation_task_with_answers” (which is at least how scripts/eval/yamls/tasks_v0.3.yaml treats GSM8K) is InContextLearningGenerationExactMatchAccuracy. We could define a custom metric that uses Math-Verify to extract the LLM’s answer and compare it to ground truth more accurately."
  },
  {
    "objectID": "posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html",
    "href": "posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html",
    "title": "Evaluating Classification Models, Part 2: The Sufficiency of Precision and Recall",
    "section": "",
    "text": "With high recall, most dresses appear in the dress feed. With high precision, most products in the dress feed are dresses. Are those two numbers all we need?\nThis post is part of a series on evaluating classification models:\nThis series differs from other discussions of evaluation metrics for classification models in that it aims to provide a systematic perspective. Rather than providing a laundry list of individual metrics, it situates those metrics within a fairly comprehensive family and explains how you can choose a member of that family that is appropriate for your use case.\nPart 1 of this series explains why we need metrics that allow us to evaluate binary classification models systematically. For instance, if we are developing a model that takes images of a product in ShopRunner’s retailer network and returns a probability that the product is a dress, we need a metric that weighs false positives (products that are predicted to be dresses but are not) against false negatives (products that are dresses but are predicted not to be) in a way that approximately matches what our preferences over models would be upon sufficient reflection.\nThat post put forward precision and recall as ways to measure the ability of a model to avoid false positives and false negatives, respectively, using the example of a model that classifies a product as either a dress or not. Precision is the accuracy of a model’s positive predictions. For instance, when it says that a product is a dress, how often is it actually a dress? Recall is the accuracy of the model’s predictions for actual instances of the positive class. For instance, when a product is a dress, how often does the model classify it as a dress?\nPrecision and recall are useful metrics, but in order to use them to rank models we need some way to combine them into a single measure of overall performance. Subsequent posts in this series will go into detail about how to think about such composite metrics. This post addresses a prior question: why use precision and recall as our starting points for model evaluation? In particular, how can we be sure that those numbers do not leave out important information?"
  },
  {
    "objectID": "posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html#summarizing-model-performance-with-precision-and-recall",
    "href": "posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html#summarizing-model-performance-with-precision-and-recall",
    "title": "Evaluating Classification Models, Part 2: The Sufficiency of Precision and Recall",
    "section": "Summarizing Model Performance with Precision and Recall",
    "text": "Summarizing Model Performance with Precision and Recall\nThe results of applying a binary classifier to a dataset can be displayed in a confusion matrix:\n\n\n\n\nPredicted No\nPredicted Yes\n\n\n\n\nActual No\n|True Negative|\n|False Positive|\n\n\nActual Yes\n|False Negative|\n|True Positive|\n\n\n\nwhere e.g. “|True Positive|” refers to the number of true positive predictions that the model generates (i.e., where the model correctly predicted that the item in question belongs to the positive class).\nFor instance, a model that classifies a product as “dress” or “not a dress” might have the following confusion matrix:\n\n\n\n\nPredicted No\nPredicted Yes\n\n\n\n\nActual No\n92\n8\n\n\nActual Yes\n10\n90\n\n\n\nWe often act as if an observed confusion matrix contains everything we can learn about the value of a model from its performance on a test dataset. This approach is not always justified: for instance, the confusion matrix discards information about the presence or absence of problematic patterns of error such as racial biases. However, if those kinds of problems are being addressed by other means, then the assumption that a confusion matrix is a sufficient basis for preferences over models may be justified.\nWe can calculate precision and recall from a confusion matrix but not vice versa. However, when we evaluate a model against a validation dataset, we typically have access to the total number of positive and negative examples (the row sums in the confusion matrix). Given that information, we can reconstruct a confusion matrix from precision and recall (See the Appendix to this post.) Thus, in a typical scenario, precision and recall contain the same information as a confusion matrix, so they are sufficient for evaluating a model if the confusion matrix is.\nNow, there are other pairs of numbers that also contain the same information as a confusion matrix given the numbers of actual positive and actual negative cases, such as sensitivity and specificity. The choice among such pairs of numbers is a matter of taste. I use precision and recall because they are popular in the machine learning community. I suspect that they are also easier for people to understand than some of the alternatives because they have actual positive or predicted positive cases in the denominator, so that thinking about them requires processing fewer negations.\nIn any case, precision and recall are sufficient for model evaluation when a confusion matrix is sufficient for model evaluation and the numbers of actual positive and actual negative examples in the dataset are known."
  },
  {
    "objectID": "posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html#appendix-constructing-a-confusion-matrix-from-precision-recall-and-its-row-sums",
    "href": "posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html#appendix-constructing-a-confusion-matrix-from-precision-recall-and-its-row-sums",
    "title": "Evaluating Classification Models, Part 2: The Sufficiency of Precision and Recall",
    "section": "Appendix: Constructing a Confusion Matrix from Precision, Recall, and its Row Sums",
    "text": "Appendix: Constructing a Confusion Matrix from Precision, Recall, and its Row Sums\nHere is one way to reconstruct a confusion matrix from precision, recall, and the number of “Actual No” and “Actual Yes” cases.\n\n|True Positive|\n\\[\n\\text{Recall} = \\frac{|\\text{True Positive}|}{|\\text{Actual Yes}|}\n\\]\nso\n\\[\n|\\text{True Positive}| = \\text{Recall} \\times |\\text{Actual Yes}|\n\\]\n\n\n|False Negative|\n\\[\n|\\text{False Negative}| = |\\text{Actual Yes}| - |\\text{True Positive}|\n\\]\n\n\n|False Positive|\n\\[\n\\text{Precision} = \\frac{|\\text{True Positive}|}{|\\text{True Positive}| + |\\text{False Positive}|}\n\\]\nso a bit of algebra shows\n\\[\n|\\text{False Positive}| = |\\text{True Positive}|\\frac{1-\\text{Precision}}{\\text{Precision}}\n\\]\n\n\n|True Negative|\n\\[\n|\\text{True Negative}| = |\\text{Actual No}| - |\\text{False Positive}|\n\\]"
  },
  {
    "objectID": "posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html#acknowledgements",
    "href": "posts/2019-11-20_evaluating-classifiers-pt2/evaluating_classifiers_pt_2.html#acknowledgements",
    "title": "Evaluating Classification Models, Part 2: The Sufficiency of Precision and Recall",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Nicole Carlson, Ali Vanderveld, and the rest of the ShopRunner data science team for comments on earlier versions of this material.\nOriginally published at medium.com"
  },
  {
    "objectID": "posts/2025-02-20_til_math-verify/til_math-verify.html",
    "href": "posts/2025-02-20_til_math-verify/til_math-verify.html",
    "title": "TIL: How Math-Verify Parses LLM Outputs",
    "section": "",
    "text": "Note\n\n\n\nThis is a TIL (“Today I Learned”) post. I expect it to be useful to my future self and maybe to others, but it is meant to be a quick, informal way to capture something I learned rather than a polished presentation.\n\n\nIn a previous post, we saw that LLM Foundry’s default evaluation procedure for open-ended math problems has some limitations. That post proposed to address this problem by using Hugging Face’s Math-Verify library.\nThis post is a first look at Math-Verify.\n\nfrom math_verify import parse, verify, LatexExtractionConfig, ExprExtractionConfig\n\n\nApplying Math-Verify to Our Examples\nNaively following the Math-Verify README, we can apply it to one of the examples that the default LLM Foundry evaluation had trouble with as follows.\n\ngold = parse(\"40\")\nanswer = parse(\"20 + 20 = 40\")\n\nverify(gold, answer)\n\nTrue\n\n\nHear Math-Verify gets the right answer, correctly telling us that the model is correct.\nLet’s try another example.\n\ngold = parse(\"15 pounds x 1/4 pounds x 1/2 pounds = 15 pounds.\")\nanswer = parse(\"15\")\n\nverify(gold, answer)\n\nTrue\n\n\nIn this case the model gets the right answer for the wrong reason. If we are expecting Math-Verify to evaluate the model’s answer and not its reasoning, then it is doing what we want here.\n\ngold = parse(\"12\\nB: 16\\nC: 24\\nD: 32\")\nanswer = parse(\"12\")\n\nverify(gold, answer)\n\nFalse\n\n\nHere the model misunderstands its job – instead of giving an answer, it gives multiple-choice options. The first option happens to be correct, but the model should not get credit for giving the right answer, and Math-Verify correctly rejects its response.\n\n\nHow Does Math-Verify’s parse Work?\nSo far, so good – Math-Verify is getting the right result in these three cases. But how? Is it getting the right results in ways that generalize?\nLet’s dig one layer deeper by seeing how Math-Verify’s parse is working.\n\nparse(\"12\\nB: 16\\nC: 24\\nD: 32\")\n\n[32, '32']\n\n\nAh. Here parse is returning a list of two values, both of which are some version of “32”. “32” is the last multiple-choice optional that the model gave. I suspect that if 32 had been the correct answer, then Math-Verify would have counted the model’s response as correct even though it was a set of options rather than a definite answer. Let’s check.\n\nverify(parse(\"12\\nB: 16\\nC: 24\\nD: 32\"), parse(\"32\"))\n\nTrue\n\n\nOK, so Math-Verify isn’t magic. In this case it simply picking out the last number where LLM Foundry picked out the first number, which happens to give the correct result in this case but is not any better in principle.\n\nfallback_mode\nWhy is parse(\"12\\nB: 16\\nC: 24\\nD: 32\") returning a list of two values?\n\nparse_results = parse(\"12\\nB: 16\\nC: 24\\nD: 32\")\n[type(item) for item in parse_results]\n\n[sympy.core.numbers.Integer, str]\n\n\nThe first item is a sympy object, and the second is a string.\nsympy objects have some advantages for our purposes. For instance, it can recognize objects as equal even when they are written differently, as in this example:\n\nimport sympy as sp\n\nsp.Eq(sp.sympify(\"1/2\"), sp.sympify(\"0.5\"))\n\n\\(\\displaystyle \\text{True}\\)\n\n\nThe string result is meant to be a fallback option. You can turn it off:\n\nparse(\n    \"12\\nB: 16\\nC: 24\\nD: 32\",\n    fallback_mode=\"no_fallback\",\n)\n\n[32]\n\n\nparse works in two stages. First, it pulls out regex matches from the input text. Then it tries to cast each of those matches as a sympy object. With fallback_mode=\"first_match\", parse returns the first regex match it pulls out as a string, independently of what happens with sympy. With fallback_mode=\"no_fallback\", it does not return a string; it only returns the sympy object, if sympy processing succeeds, or an empty list if sympy processing fails.\nHere is an example where parse pulls out the string inside \\\\boxed{}, but the string is not a well-formed mathematical expression, so sympy cannot process it. parse then returns just the string with fallback_mode=\"first_match\" (or by default) and an empty list with fallback_mode=\"no_fallback\".\n\nparse(\n    \"\\\\boxed{E=mc^}\",\n    fallback_mode=\"first_match\",\n)\n\n['E=mc^']\n\n\n\nparse(\n    \"\\\\boxed{E=mc^}\",\n)\n\n['E=mc^']\n\n\n\nparse(\n    \"\\\\boxed{E=mc^}\",\n    fallback_mode=\"no_fallback\",\n)\n\n[]\n\n\nAt this point it might be clearer for parse to have a Boolean parameter with a name like return_string_fallback rather than a string parameter with the name fallback_mode that simply controls whether or not parse returns the first match it tries to parse as a string. Perhaps the motivation for the current design is that it provides flexibility to add more “fallback modes” in the future without changing the function signature.\n\n\nextraction_mode\nparse has a second parameter extraction_mode that also affects what happens when casting to sympy fails. With extraction_mode=\"first_match\", parse will only try casting to sympy once, and will not return a sympy object if it fails. With extraction_mode=\"any_match\", parse will keep trying to cast matches to sympy until one succeeds.\nFor instance, if we have one invalid expression and one valid expression, parse will return the valid expression as a sympy object with extraction_mode=\"any_match\", but with extraction_mode=\"first_match\" it will not return any sympy objects if it processes the invalid expression before the valid one.\n\nparse(\n    \"$x + y$ $E=mc^$\",\n    fallback_mode=\"no_fallback\",  # do not return a string\n    extraction_mode=\"first_match\",  # give up on returning a `sympy` object if the first attempt fails\n)\n\n[]\n\n\n\nparse(\n    \"$x + y$ $E=mc^$\",\n    fallback_mode=\"first_match\",  # return a string from the first match regardless of whether casting to `sympy` succeeds\n    extraction_mode=\"first_match\",  # give up on returning a `sympy` object if the first attempt fails\n)\n\n['E=mc^']\n\n\n\nparse(\n    \"$x + y$ $E=mc^$\",\n    fallback_mode=\"no_fallback\",  # do not return a string\n    extraction_mode=\"any_match\",  # keep trying to return a `sympy` object until one attempt succeeds\n)\n\n[x + y]\n\n\n\nparse(\n    \"$x + y$ $E=mc^$\",\n    fallback_mode=\"first_match\",  # return a string from the first match regardless of whether casting to `sympy` succeeds\n    extraction_mode=\"any_match\",  # keep trying to return a `sympy` object until one attempt succeeds\n)\n\n[x + y, 'E=mc^']\n\n\nIn this last example, the two returned items come from different matches. The second item is the invalid expression, which we get as a string because it is processed first and we have fallback_mode=“first_match”. The first item is the valid expression, which we get as a sympy object because setting extraction_mode=\"any_match\" caused parse to keep trying to cast to sympy until it succeeded.\nIn these examples parse finds two matches and prioritizes the second one. However, it does not always prioritize the last match. Let’s take a look at what it does instead.\n\n\nextraction_config\nparse has an additional extraction_config parameter that takes a sequence of ExtractionConfig objects. Each ExtractionConfig object specifies one procedure for finding and prioritizing matches. It supports three ExtractionConfig classes: LatexExtractionConfig, StringExtractionConfig, and ExprExtractionConfig. Based on which of these classes is used, parse generates a list of regexes with with associated priority levels that it uses to find and prioritize matches. Within a given priority level, parse processes matches in the reverse order they appear in the input text.\nFor instance, LatexExtractionConfig looks for expressions within various LaTeX delimiters, such as $$...$$ and \\[..\\]. It prioritizes matches that are “marked” as the final answer, for instance by being inside \\boxed{} or after “final answer is”. The details are quite complicated, and I do not fully understand them, but let’s look at some examples.\nWith two expressions simply delimited by $, parse finds both matches and prioritizes the second one.\n\nparse(\n    \"$x + y$ $E=mc^2$\",\n    extraction_config=[LatexExtractionConfig()],\n    extraction_mode=\"any_match\",\n    fallback_mode=\"no_fallback\",\n)\n\n[Eq(E, c**2*m)]\n\n\nHowever, parse will prioritize the first match if it is marked as the final answer in a way that it recognizes.\n\nparse(\n    \"the final answer is $x + y$ $E=mc^2$\",\n    extraction_config=[LatexExtractionConfig()],\n    extraction_mode=\"any_match\",\n    fallback_mode=\"no_fallback\",\n)\n\n[x + y]\n\n\n\nparse(\n    \"\\\\boxed{x + y} $E=mc^2$\",\n    extraction_config=[LatexExtractionConfig()],\n    extraction_mode=\"any_match\",\n    fallback_mode=\"no_fallback\",\n)\n\n[x + y]\n\n\nThis approach makes sense, but it is complicated and currently not well documented. It rewards models for marking their final answers in certain specific ways, which introduces some amount of coupling between the evaluation procedure and the details of how the model is trained and prompted.\nIf I understand correctly, ExprExtractionConfig looks for numerical (rather than symbolic) mathematical expressions without relying on LaTeX delimiters.\n\nparse(\n    \"1 + 2\",\n    extraction_config=[ExprExtractionConfig()],\n    extraction_mode=\"any_match\",\n    fallback_mode=\"no_fallback\",\n)\n\n[1 + 2]\n\n\nIt is prone to pulling out parts of larger expressions in ways that may or may not match our desires.\n\n# Here we get `3`, which probably is what we want\nparse(\n    \"1 + 2 = 3\",\n    extraction_config=[ExprExtractionConfig()],\n    extraction_mode=\"any_match\",\n    fallback_mode=\"no_fallback\",\n)\n\n[3]\n\n\n\n# Here we get \"2\", which probably is not what we want\nparse(\n    \"$1 + 2$\",\n    extraction_config=[ExprExtractionConfig()],\n    extraction_mode=\"any_match\",\n    fallback_mode=\"no_fallback\",\n)\n\n[2]\n\n\nBy default, extraction_config=[LatexExtractionConfig(), ExprExtractionConfig()], so parse will find both LaTeX expressions and numerical expressions. It combines their matches into one pool, prioritizing those matches by the priorities that the configs give them and breaking ties by working back to front.\nThe final config class, StringExtractionConfig is to a first approximation simply looking for any of a fixed set of strings, by default “A”, “B”, “C”, and “D”. I take it that it is meant to be used for multiple-choice questions rather than open-ended math problems.\n\n\nExamples\nNow that we have some general idea of how parse works, let’s look back at the examples from the previous post.\n\nparse(\"12\\nB: 16\\nC: 24\\nD: 32\")\n\n[32, '32']\n\n\n\nparse(\n    \"12\\nB: 16\\nC: 24\\nD: 32\",\n    extraction_config=[LatexExtractionConfig()],\n    extraction_mode=\"any_match\",\n)\n\n[]\n\n\n\nparse(\n    \"12\\nB: 16\\nC: 24\\nD: 32\",\n    extraction_config=[ExprExtractionConfig()],\n    extraction_mode=\"any_match\",\n    fallback_mode=\"no_fallback\",\n)\n\n[32]\n\n\nIn this case, LatexExtractionConfig does not find any matches because the model’s output is not formatted as LaTeX. ExprExtractionConfig presumably finds all four numbers and gives them the same priority, so it returns the last number, 32.\nThe fact that parse returns the same result here as it would if the model had simply returned “32” is a problem, because it will cause us to count the model as correct even though it did not give a definite answer. The ideal behavior depends on our larger system design, but it would involve recognizing that the model did not give a definite answer and returning something that indicates as much, such as perhaps an empty list.\nIn our other two examples, parse returns the rightmost number for the same reason, and that happens to be the right thing to do:\n\nparse(\"20 + 20 = 40\")\n\n[40, '40']\n\n\n\nparse(\"15 pounds x 1/4 pounds x 1/2 pounds = 15 pounds.\")\n\n[15, '15']\n\n\nparse is simply picking out the last number in the model’s output here, rather than intelligently handling the = sign, as this example shows:\n\nparse(\"20 + 20 = 40. By the way, my favorite number is 50.\")\n\n[50, '50']\n\n\n\n\n\nConclusion\nMath-Verify is a library for evaluating LLM outputs on open-ended math problems. It provides a parse function that uses regexes to extract mathematical expressions and then attempts to cast them as sympy objects. It also provides a verify function that compares the parsed model output to the parsed gold answer. I will look at this verify function in a future post. The resulting evaluation process is not foolproof, but it is perhaps an improvement over LLM Foundry’s default evaluation procedure."
  },
  {
    "objectID": "posts/2020-07-30_introducing-wildebeest/introducing_wildebeest.html",
    "href": "posts/2020-07-30_introducing-wildebeest/introducing_wildebeest.html",
    "title": "Introducing Wildebeest, a Python File-Processing Framework",
    "section": "",
    "text": "Photo Credit: Gopal Vijayaraghavan cc\n\n\n\nIntroduction\nShopRunner has more than ten million product images that we use to train computer vision classification models. Moving those files around and processing them is a pain without good tooling. Just downloading them serially takes many days, and the occasional corrupted image can bring the whole process to a halt. Without good logging and error handling, it might then be necessary to start the process over until the next error is raised.\nOver time we built up techniques for parallelizing over files, handling errors, and skipping files that had already been processed. We then incorporated those techniques into an open-source file-processing framework Wildebeest. With Wildebeest, the user specifies what files to process and how to process each one. Wildebeest then does the processing in parallel and generates a “run report” that records for each file the input and output path, whether the file was skipped, and what error if any was handled. We routinely use Wildebeest to download and resize millions of images in a matter of hours and tens of lines of code.\n\n\nBasic Example\nThe following code uses a fairly minimal Wildebeest pipeline to download a list of images to the current working directory as PNGs, parallelizing across up to ten threads.\nfrom functools import partial\n\nfrom wildebeest import Pipeline\nfrom wildebeest.load_funcs.image import load_image_from_url\nfrom wildebeest.path_funcs import join_outdir_filename_extension\nfrom wildebeest.write_funcs.image import write_image\n\n\nimage_urls = [\n    f\"https://bit.ly/{filename}\" for filename in [\"2RsJ8EQ\", \"2TqoToT\", \"2VocS58\"]\n]\n\n# Create a pipeline object, specifying how to load a file and how to\n# write out each file\nimage_download_pipeline = Pipeline(\n    load_func=load_image_from_url, write_func=write_image\n)\n\n# Run the pipeline, specifying input paths, how to derive an output path\n# from each input path, and how many threads to use\nimage_download_pipeline(\n    inpaths=image_urls,\n    path_func=partial(join_outdir_filename_extension, outdir=\".\", extension=\".png\"),\n    n_jobs=10,\n)\nAfter it runs, the pipeline has a Pandas DataFrame containing a record of what happened with each input file stored as an attribute called “run_report_”:\n\n\n\nRun Report\n\n\nThe trailing underscore in “run_report_” indicates that the attribute exists only after the pipeline has been run, analogous to the Scikit-Learn convention of using a trailing underscore for Estimator attributes that exist only after the attribute has been fit (e.g. LinearRegression().coef_).\n\n\nAdditional Capabilities\nYou can do more with Wildebeest than just download images:\n\nProcess input from any source you want, including arbitrary media types (e.g. text, video, or audio) and local as well as remote files.\nDo arbitrary processing on each file, for instance to resize each image before writing it to disk.\nAdd columns to the run report that record arbitrary properties of the file, such as the average brightness of each image.\nSelectively skip files based on arbitrary criteria. For instance, you can skip an input file when a file already exists at the intended output location, making it easy to pick up where you leave off quickly after a failure.\n\nOur Quickstart guide includes code examples for all of these scenarios.\nYou can also see Wildebeest in use in the open-source Autofocus project, which uses computer vision to automate animal conservation work in collaboration with the Lincoln Park Zoo’s Urban Wildlife institute. (In that project it is used under its previous name “Creevey.” That name comes from the Harry Potter series, so we changed it after becoming aware of transphobic comments by series author J.K. Rowling.)\n\n\nConclusion\nWildebeest makes big data processing jobs fast and easy. To get started with it, you can read the docs, check out the code on GitHub, or install the package from PyPI.\n\n\nAcknowledgements\nThanks to Michael Sugimura for feedback on an earlier draft and to the ShopRunner data science team for contributions to the library."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "",
    "text": "This plot shows level curves for an F₁ score, which is one way to weigh precision against recall in evaluating a classification model.\nThis post is part of a series on evaluating classification models:\nThis series differs from other discussions of evaluation metrics for classification models in that it aims to provide a systematic perspective. Rather than providing a laundry list of individual metrics, it situates those metrics within a fairly comprehensive family and explains how you can choose a member of that family that is appropriate for your use case."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#classifying-products-at-shoprunner",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#classifying-products-at-shoprunner",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "Classifying Products at ShopRunner",
    "text": "Classifying Products at ShopRunner\nShopRunner’s mission is to power extraordinary shopping experiences that connect customers to the brands they love. To accomplish this mission, we need to make it easy for users to find products they want. Many of our efforts toward this end would benefit from being able to classify products within a single product taxonomy. For instance, classifying a particular product as a dress allows us to display it under “Women > Clothing > Dresses” in our District app.\n\n\n\nDresses in ShopRunner’s District app\n\n\nClassifying all of the products across our retailer network into a single taxonomy is challenging. Our partners all use different product taxonomies, often including categories such as “New Arrivals” that do not map cleanly into categories like “Dresses.” As a result, simple rules-based approaches based on the product categories that our partners use are not sufficient. Moreover, classifying each product manually would be too expensive: we have more than 100 partners with millions of total products, many of which turn over several times each year.\nWe are using machine learning to address this challenge. Modern deep learning systems can learn to classify products based on images and text descriptions with accuracy similar to that of manual classification for a small fraction of the cost."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#false-positives-and-false-negatives",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#false-positives-and-false-negatives",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "False Positives and False Negatives",
    "text": "False Positives and False Negatives\nEven with cutting-edge machine learning, no classification system is perfect. For instance, a model that classifies a product as “dress” or “not a dress” might sometimes misidentify a skirt as a dress. We would call that misclassification a false positive for the dress category. In addition, it might sometimes misidentify a dress as “not a dress,” which we would call a false negative for the dress category.\nExamples:\n\n\n\nCorrectly labeling this dress as a dress would be a true positive.\n\n\n\n\n\nCorrectly labeling this skirt as “not a dress” would be a true negative.\n\n\n\n\n\nIncorrectly labeling this skirt as a dress would be a false positive.\n\n\n\n\n\nIncorrectly labeling this dress as “not a dress” would be a false negative. (Yes, it is a dress. Life is pain.)\n\n\nMany machine learning classification models actually produce a probability for a label such as dress. By default, we would typically take a model to be predicting “dress” if its probability for “dress” is at least \\(50\\%\\) and “not a dress” otherwise. But we can use a cutoff other than \\(50\\%\\) if we wish. For instance, we might only classify a product as a dress if its probability for dress is at least \\(70\\%\\).\nUsing \\(70\\%\\) rather than \\(50\\%\\) as our cutoff probability for a positive prediction would change our model’s error rates. Specifically, by causing the model to produce fewer positives, it would reduce the false positive rate (which is good) but increase the false negative rate (which is bad). Part of our job in developing a classification model is to choose cutoff probabilities that strike an appropriate balance between false positives and false negatives.\nFor simplicity, let’s pretend that our District app only sells dresses. (It doesn’t!) Showing a product that isn’t a dress in the app would be potentially off-putting to our users, so the cost of a false positive is significant. On the other hand, a dress that we mistakenly classify as “not a dress” typically will not have much impact as long as we still have a large number of other dresses to choose from. As a result, the cost of a single false positive is generally higher than that of a single false negative, so we should set our cutoff probability higher than \\(50\\%\\)."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#precision-and-recall",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#precision-and-recall",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "Precision and Recall",
    "text": "Precision and Recall\nWe have said that we should set the cutoff probability higher than \\(50\\%\\), but how much higher? To answer this question, we need ways to quantify our model’s performance as we vary the cutoff probability. There are many metrics that we could use for this purpose. I will focus on precision and recall. Part 2 of this series will justify that choice.\nA model’s precision is the accuracy of its positive predictions. For instance, when our model says that something is a dress, how often is it actually a dress?\n\\[\n\\text{Precision} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Positives}|}\n\\]\nwhere e.g. “|True Positives|” is the number of true positive predictions that our model generates.\nRecall is the model’s accuracy on the actual positive class. For instance, when a product is actually a dress, how often does our model say that it is a dress?\n\\[\n\\text{Recall} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Negatives}|}\n\\]\nNow suppose we build two different models, and one has better precision while the other has better recall. (We can typically create such a scenario by changing the cutoff probability for positive predictions.)\nModel A:\n\n\n\n\nPredicted No\nPredicted Yes\nTotal\n\n\n\n\nActual No\n90\n10\n100\n\n\nActual Yes\n1\n99\n100\n\n\nTotal\n91\n109\n200\n\n\n\n\\[\n\\text{Precision} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Positives}|} = \\frac{|99|}{|99| + |10|} = 90.8\\%\n\\]\n\\[\n\\text{Recall} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Negatives}|} = \\frac{|99|}{|99| + |1|} = 99\\%\n\\]\nModel B:\n\n\n\n\nPredicted No\nPredicted Yes\nTotal\n\n\n\n\nActual No\n91\n9\n100\n\n\nActual Yes\n3\n97\n100\n\n\nTotal\n94\n106\n200\n\n\n\n\\[\n\\text{Precision} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Positives}|} = \\frac{|97|}{|97| + |9|} = 91.5\\%\n\\]\n\\[\n\\text{Recall} = \\frac{|\\text{True Positives}|}{|\\text{True Positives}| + |\\text{False Negatives}|} = \\frac{|97|}{|97| + |3|} = 97\\%\n\\]\nTo choose between these models, we need to decide whether the gain from \\(90.8\\%\\) precision to \\(91.5\\%\\) precision that we get by going from Model A to Model B is enough to offset a loss from \\(99\\%\\) recall to \\(97\\%\\) recall."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#why-we-need-a-variety-of-model-evaluation-metrics",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#why-we-need-a-variety-of-model-evaluation-metrics",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "Why We Need a Variety of Model Evaluation Metrics",
    "text": "Why We Need a Variety of Model Evaluation Metrics\nIf we only had to make occasional decisions between pairs of models, then it would be reasonable to make decisions about how to trade off precision against recall on an ad hoc basis. However, we can typically generate dozens of models that all differ in terms of precision and recall with no model dominating all others on both measures, for instance by varying our cutoff probability. As a result, we need a way to combine precision and recall into a single metric that we can use to select the “best” model for a given application.\nA natural way to combine precision and recall is to average them. For instance, using simple averaging in the example above, Model A would get a score of \\(1/2(90.8\\% + 99\\%) = 94.9\\%\\), while Model B would get a score of \\(1/2(91.5\\% + 97\\%) = 94.3\\%\\). However, this simple mean gives equal weight to precision and recall. We have said that it is more important to avoid showing non-dresses in our app than it is to show all of the dresses that we have, so precision is more important to us than recall. Thus, a simple arithmetic mean of precision and recall isn’t the right choice in this application. We should use a weighted average instead.\nMoreover, perhaps we would be willing to give up one point of precision in exchange for one point of recall if precision were very high (say \\(99%\\)) and recall were very low (say \\(10%\\)), whereas if precision and recall were similar then we would not be willing to make that trade. If so, then a weighted arithmetic mean of precision and recall is not the right choice, because a weighted arithmetic mean encodes a willingness to trade one point of recall for a fixed number of points of precision at every level of precision and recall.\nIn that case, we might consider using something like a weighted geometric or harmonic mean rather than a weighted arithmetic mean. Part 3 of this series discusses those options.\nThere are many ways to weigh precision against recall, and which way is most appropriate depends on the problem. Many sources help with the challenge of choosing an evaluation metric for a given problem by providing a laundry list of metrics with a few comments about the characteristics of each one. This series of blog posts aims to provide a more systematic perspective. It culminates in a new family of metrics that generalizes the popular \\(F_\\beta\\) score. This family allows the user to specify not only the overall importance of recall relative to precision but also how that relative importance shifts as recall and precision become increasingly imbalanced."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#appendix",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#appendix",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "Appendix",
    "text": "Appendix\n\nExtending Binary Model Evaluation Metrics to Multiclass Problems\nSo far we have discussed only binary classification problems, such as identifying whether a particular product is or is not a dress. The full problem of placing products within our taxonomy is not a binary classification problem, because there are many possible categories for each item. For instance, we need to say not just whether a given item is a dress or not, but also if it is not a dress, whether it is a skirt, a pair of jeans, and so on. However, we can regard this multiclass problem as a set of binary classification problems (one for each category) and aggregate performance on each of those subproblems to produce an overall measure of model performance. As a result, the discussion in these blog posts is relevant to evaluating multiclass models even though it focuses on binary models.\nFor instance, here is one possible procedure:\n\nCalculate precision and recall for the “dress” category\nCalculate an overall score for the “dress” category by averaging those precision and recall values in one of the ways we will discuss.\nDo likewise for all other categories.\nUse a simple average of the individual category scores as the score for the multiclass model as a whole.\n\nSee the scikit-learn documentation for a discussion of several ways to produce multiclass model evaluation metrics from binary model evaluation metrics.\n\n\nUsing a Single Evaluation Metric\nIt is common to use multiple metrics to measure various aspects of model performance separately. In my view, this approach is helpful for diagnostic purposes — that is, for figuring out where your model is failing and using that information to guide model development. However, it is best to use but one metric for evaluation purposes — that is, for deciding which of a set of models is best and whether it is better than no model.\nThis single evaluation metric can be different for different projects, and it is subject to revision within a project, but for a given project there should be one primary metric at any given time that is used to guide modeling decisions. This approach allows a team developing the model to iterate quickly, without getting bogged down in long discussions about how to weigh different aspects of model performance against one another at every turn. See this lecture from Andrew Ng’s “Structuring Machine Learning Projects” course for more discussion of this issue."
  },
  {
    "objectID": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#acknowledgements",
    "href": "posts/2019-11-14_evaluating-classifier-pt1/evaluating_classifiers_pt_1.html#acknowledgements",
    "title": "Evaluating Classification Models, Part 1: Weighing False Positives Against False Negatives",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThanks to Nicole Carlson, Ali Vanderveld, and the rest of the ShopRunner data science team for comments on earlier versions of this material.\nOriginally published at medium.com"
  },
  {
    "objectID": "posts/2025-04-09_til_debugging-api-connection-error-carriage-returns/index.html",
    "href": "posts/2025-04-09_til_debugging-api-connection-error-carriage-returns/index.html",
    "title": "TIL: Debugging an API Connection Error Caused by Invisible Carriage Returns",
    "section": "",
    "text": "Note\n\n\n\nThis is a TIL (“Today I Learned”) post. I expect it to be useful to my future self and maybe to others, but it is meant to be a quick, informal way to capture something I learned rather than a polished presentation.\n\n\nI encountered an error when running Python scripts that used llama_index to interact with LLM provider APIs:\nhttpcore.LocalProtocolError: Illegal header value b'Bearer <REDACTED KEY>\\r'\nThe \\r at the end of the header value was the problem. That character originated from an example .env file that I was copying and modifying. It contained carriage return (\\r) characters at the end of each line, likely due to being created or edited on a Windows system and then used in a Linux environment that expects line feeds (\\n).\nThese characters were invisible in standard editors but could be revealed using cat -vET:\n$ cat -vET .env.example\nexport OPENAI_API_KEY='YOUR_OPENAI_KEY_HERE'^M$\nexport GOOGLE_API_KEY='YOUR_GOOGLE_KEY_HERE'^M$\nexport ANTHROPIC_API_KEY='YOUR_ANTHROPIC_KEY_HERE'^M$\nThe ^M represents the carriage return character. When the environment variables were loaded, these carriage returns were included as part of the API key strings. The httpx library (used by the openai library) correctly identified this character as illegal in an HTTP header value, leading to the LocalProtocolError.\nCreating a new .env file from scratch resolved the issue. The carriage return character no longer appeared:\n$ cat -vET .env.example\nexport OPENAI_API_KEY='YOUR_OPENAI_KEY_HERE'$\nexport GOOGLE_API_KEY='YOUR_GOOGLE_KEY_HERE'$\nexport ANTHROPIC_API_KEY='YOUR_ANTHROPIC_KEY_HERE'$\nAnd the script ran successfully."
  },
  {
    "objectID": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html",
    "href": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html",
    "title": "Parameter-Efficient Fine-Tuning Illustrated with Regression Tasks",
    "section": "",
    "text": "LLMS and other foundation models are capable of accomplishing a wide range of tasks. However, we may have a task that they do not perform well off the shelf. In principle, we can address this problem by fine tuning a model for that task. However, fine tuning foundation models is extremely expensive.\nParameter-efficient fine tuning (PEFT) is meant to address this problem. Rather than adjusting all of the weights of a model, it adds relatively small adapters to the model and trains those adapters on new data.\nThis post implements a few parameter efficient fine tuning techniques: LORA, DORA, and RS-LORA. It illustrates these methods on a simple regression problem in which we adapt a model that has been trained on a quadratic data set to a cubic data set. These polynomial fitting problems are many orders of magnitude simpler than language modeling, so the way these PEFT methods behave on them may not tell us much about how they behave when applied to foundation models. However, the illustrations do illustrate the general concepts of full and parameter-efficient fine tuning and help confirm that the methods are working as expected.\nThe LoRA and DoRA are implementations adapted from Sebastian Raschka’s Improving LoRA: Implementing Weight-Decomposed Low-Rank Adaptation (DoRA) from Scratch. The visualization methods are adapted from Jeremy Howard’s FastAI v3 Lesson 2: SGD. Both were published under the Apache License 2.0."
  },
  {
    "objectID": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html#reproduce-lora-results",
    "href": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html#reproduce-lora-results",
    "title": "Parameter-Efficient Fine-Tuning Illustrated with Regression Tasks",
    "section": "Reproduce LoRA results",
    "text": "Reproduce LoRA results\nLet’s implement rsLoRA, but initially adjust \\(\\alpha\\) to get the same scaling factor as before, so that we should get the same results as with LoRA.\n\ntorch.manual_seed(\n    678\n)  # resetting seed so we get the same LoRA weight initializations as before\nrslora_model = create_lora_model(\n    model,\n    lora_layer_indices=[2],\n    lora_layer_class=partial(\n        LinearWithLoRA,\n        rank=RANK,\n        # will give same gamma_r as LoRA above, so should train the same\n        alpha=ALPHA / (RANK**1 / 2),\n        lora_layer_class=RsLoRALayer,\n    ),\n)\nrslora_model\n\nMultilayerPerceptron(\n  (layers): Sequential(\n    (0): Linear(in_features=1, out_features=20, bias=True)\n    (1): ReLU()\n    (2): LinearWithLoRA(\n      (linear): Linear(in_features=20, out_features=20, bias=True)\n      (lora): RsLoRALayer()\n    )\n    (3): ReLU()\n    (4): Linear(in_features=20, out_features=1, bias=True)\n  )\n)\n\n\n\nfor name, param in rslora_model.named_parameters():\n    print(f\"{name}: {param.requires_grad}\")\n\nlayers.0.weight: False\nlayers.0.bias: False\nlayers.2.linear.weight: False\nlayers.2.linear.bias: False\nlayers.2.lora.A: True\nlayers.2.lora.B: True\nlayers.4.weight: False\nlayers.4.bias: False\n\n\n\nanim = create_training_animation(\n    rslora_model,\n    x,\n    y2,\n    torch.optim.Adam(rslora_model.parameters(), lr=LEARNING_RATE),\n)\nHTML(anim.to_html5_video())\n\nInitial Loss: 0.3533\nInitial Loss: 0.3533\nIteration 10, Loss: 0.0761\nIteration 20, Loss: 0.0494\nIteration 30, Loss: 0.0287\nIteration 40, Loss: 0.0241\nIteration 50, Loss: 0.0183\nIteration 60, Loss: 0.0154\nIteration 70, Loss: 0.0138\nIteration 80, Loss: 0.0130\nIteration 90, Loss: 0.0126\nIteration 100, Loss: 0.0124\nIteration 110, Loss: 0.0122\nIteration 120, Loss: 0.0121\nIteration 130, Loss: 0.0119\nIteration 140, Loss: 0.0118\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html#compare-lora-and-rslora-at-extreme-ranks",
    "href": "posts/2025-02-14_fine-tuning-on-regression-task/fine_tuning_on_regression_tasks.html#compare-lora-and-rslora-at-extreme-ranks",
    "title": "Parameter-Efficient Fine-Tuning Illustrated with Regression Tasks",
    "section": "Compare LoRA and rsLoRA at Extreme Ranks",
    "text": "Compare LoRA and rsLoRA at Extreme Ranks\nNow let’s keep \\(\\alpha\\) the same and see how LoRA and rsLoRA perform at extreme ranks. I would not necessarily expect rsLoRA to perform better than LoRA at extreme ranks in this simple case, but at least we can illustrate the type of situation in which rsLoRA is expected to perform better.\n\nLoRA at Low Rank\n\nlora_model = create_lora_model(\n    model,\n    lora_layer_indices=[2],\n    lora_layer_class=partial(LinearWithLoRAMerged, rank=1, alpha=ALPHA),\n)\nanim = create_training_animation(\n    lora_model,\n    x,\n    y2,\n    torch.optim.Adam(lora_model.parameters(), lr=LEARNING_RATE),\n)\nHTML(anim.to_html5_video())\n\nInitial Loss: 0.3533\nInitial Loss: 0.3533\nIteration 10, Loss: 0.0739\nIteration 20, Loss: 0.0678\nIteration 30, Loss: 0.0291\nIteration 40, Loss: 0.0260\nIteration 50, Loss: 0.0236\nIteration 60, Loss: 0.0209\nIteration 70, Loss: 0.0195\nIteration 80, Loss: 0.0186\nIteration 90, Loss: 0.0177\nIteration 100, Loss: 0.0170\nIteration 110, Loss: 0.0167\nIteration 120, Loss: 0.0164\nIteration 130, Loss: 0.0163\nIteration 140, Loss: 0.0162\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\n\nLoRA at High Rank\n\nlora_model = create_lora_model(\n    model,\n    lora_layer_indices=[2],\n    lora_layer_class=partial(LinearWithLoRAMerged, rank=20, alpha=ALPHA),\n)\nanim = create_training_animation(\n    lora_model,\n    x,\n    y2,\n    torch.optim.Adam(lora_model.parameters(), lr=LEARNING_RATE),\n)\nHTML(anim.to_html5_video())\n\nInitial Loss: 0.3533\nInitial Loss: 0.3533\nIteration 10, Loss: 0.0561\nIteration 20, Loss: 0.0304\nIteration 30, Loss: 0.0235\nIteration 40, Loss: 0.0161\nIteration 50, Loss: 0.0137\nIteration 60, Loss: 0.0130\nIteration 70, Loss: 0.0125\nIteration 80, Loss: 0.0121\nIteration 90, Loss: 0.0118\nIteration 100, Loss: 0.0116\nIteration 110, Loss: 0.0115\nIteration 120, Loss: 0.0113\nIteration 130, Loss: 0.0113\nIteration 140, Loss: 0.0112\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\n\nrsLoRA at Low Rank\n\nrslora_model = create_lora_model(\n    model,\n    lora_layer_indices=[2],\n    lora_layer_class=partial(\n        LinearWithLoRA,\n        rank=1,\n        alpha=ALPHA,\n        lora_layer_class=RsLoRALayer,\n    ),\n)\nanim = create_training_animation(\n    rslora_model,\n    x,\n    y2,\n    torch.optim.Adam(rslora_model.parameters(), lr=LEARNING_RATE),\n)\nHTML(anim.to_html5_video())\n\nInitial Loss: 0.3533\nInitial Loss: 0.3533\nIteration 10, Loss: 0.1752\nIteration 20, Loss: 0.1717\nIteration 30, Loss: 0.1737\nIteration 40, Loss: 0.1724\nIteration 50, Loss: 0.1708\nIteration 60, Loss: 0.1691\nIteration 70, Loss: 0.1666\nIteration 80, Loss: 0.1623\nIteration 90, Loss: 0.1540\nIteration 100, Loss: 0.0966\nIteration 110, Loss: 0.0926\nIteration 120, Loss: 0.0903\nIteration 130, Loss: 0.0896\nIteration 140, Loss: 0.0884\n\n\n\n  \n  Your browser does not support the video tag.\n\n\n\n\n\nrsLoRA at High Rank\n\nrslora_model = create_lora_model(\n    model,\n    lora_layer_indices=[2],\n    lora_layer_class=partial(\n        LinearWithLoRA,\n        rank=20,\n        alpha=ALPHA,\n        lora_layer_class=RsLoRALayer,\n    ),\n)\nanim = create_training_animation(\n    rslora_model,\n    x,\n    y2,\n    torch.optim.Adam(rslora_model.parameters(), lr=LEARNING_RATE),\n)\nHTML(anim.to_html5_video())\n\nInitial Loss: 0.3533\nInitial Loss: 0.3533\nIteration 10, Loss: 0.0635\nIteration 20, Loss: 0.0302\nIteration 30, Loss: 0.0185\nIteration 40, Loss: 0.0145\nIteration 50, Loss: 0.0129\nIteration 60, Loss: 0.0120\nIteration 70, Loss: 0.0117\nIteration 80, Loss: 0.0114\nIteration 90, Loss: 0.0113\nIteration 100, Loss: 0.0112\nIteration 110, Loss: 0.0111\nIteration 120, Loss: 0.0110\nIteration 130, Loss: 0.0110\nIteration 140, Loss: 0.0109\n\n\n\n  \n  Your browser does not support the video tag."
  },
  {
    "objectID": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html",
    "href": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html",
    "title": "Fast and Reproducible Deep Learning",
    "section": "",
    "text": "Our team’s open-source Wildebeest library is one tool for managing deep learning projects — it makes processing large datasets fast and easy. Photo Credit: Gopal Vijayaraghavan cc\nThere are endless resources for someone who wants to learn to train a deep learning model, but running a successful deep learning project requires managing many additional moving parts that are much less discussed. This talk contributes to filling that gap in our deep learning education resources.\nThanks to the Chicago ML Meetup for hosting."
  },
  {
    "objectID": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html#video",
    "href": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html#video",
    "title": "Fast and Reproducible Deep Learning",
    "section": "Video",
    "text": "Video"
  },
  {
    "objectID": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html#slides",
    "href": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html#slides",
    "title": "Fast and Reproducible Deep Learning",
    "section": "Slides",
    "text": "Slides"
  },
  {
    "objectID": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html#abstract",
    "href": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html#abstract",
    "title": "Fast and Reproducible Deep Learning",
    "section": "Abstract",
    "text": "Abstract\nDeep learning projects require managing large datasets, heavy-duty dependencies, complex experiments, and large amounts of code. This talk provides best practices for accomplishing these tasks efficiently and reproducibly. Tools that are covered include:\n\nThe Wildebeest library for processing large collections of files\npip-tools and nvidia-docker for managing dependencies\nMLflow Tracking for tracking experiments"
  },
  {
    "objectID": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html#additional-resources",
    "href": "posts/2020-03-26_fast-and-reproducible-deep-learning/fast_and_reproducible_deep_learning.html#additional-resources",
    "title": "Fast and Reproducible Deep Learning",
    "section": "Additional Resources",
    "text": "Additional Resources\nAutofocus is a deep learning project that labels animals in images taken by motion-activated “camera traps.” It illustrates many of the ideas discussed in the talk."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Greg Gandenberger",
    "section": "",
    "text": "TIL: Fixing gRPC Python Imports\n\n\n\n\n\n\n\ntil\n\n\ngrpc\n\n\npython\n\n\nsoftware-engineering\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nEnriching Test Coverage Reports for LLMs\n\n\n\n\n\n\n\nllms\n\n\ntesting\n\n\nsoftware-engineering\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: Debugging an API Connection Error Caused by Invisible Carriage Returns\n\n\n\n\n\n\n\ntil\n\n\ndebugging\n\n\npython\n\n\nllms\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nTIL: Why Did My Loss Curve Change With Run Duration?\n\n\n\n\n\n\n\ntil\n\n\nml\n\n\ntraining\n\n\ndebugging\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: Dev Containers and GPUs: A Match Made in Purgatory\n\n\n\n\n\n\n\ntil\n\n\nvscode\n\n\ndocker\n\n\ngpu\n\n\ncuda\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nSome Best Practices for LLM Math Evaluation\n\n\n\n\n\n\n\nml\n\n\nllms\n\n\nevals\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: How Math-Verify Verifies LLM Outputs\n\n\n\n\n\n\n\nml\n\n\nllms\n\n\nevals\n\n\nmath-verify\n\n\n\n\n\n\n\n\n\n\n\nFeb 22, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: How Math-Verify Parses LLM Outputs\n\n\n\n\n\n\n\nml\n\n\nllms\n\n\nevals\n\n\nmath-verify\n\n\n\n\n\n\n\n\n\n\n\nFeb 20, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: An LLM Foundry Metric Must Have “Accuracy” in Its Name\n\n\n\n\n\n\n\nml\n\n\nllms\n\n\nevals\n\n\nllm-foundry\n\n\n\n\n\n\n\n\n\n\n\nFeb 19, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: How LLM Foundry Evaluates Performance On Open-Ended Math Problems\n\n\n\n\n\n\n\nml\n\n\nllms\n\n\nevals\n\n\nmath-verify\n\n\nllm-foundry\n\n\n\n\n\n\n\n\n\n\n\nFeb 18, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nParameter-Efficient Fine-Tuning Illustrated with Regression Tasks\n\n\n\n\n\n\n\nml\n\n\nllms\n\n\npeft\n\n\nlora\n\n\ndora\n\n\nrslora\n\n\n\n\n\n\n\n\n\n\n\nFeb 14, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nProblem Solved: Custom Pandas styling not rendering when reopening notebook\n\n\n\n\n\n\n\nml\n\n\njupyter\n\n\npandas\n\n\n\n\nQuick fix for Pandas styling not showing up when reopening Jupyter notebooks - use jupyter trust command to enable custom styling rendering.\n\n\n\n\n\n\nAug 5, 2021\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nIntroducing Wildebeest, a Python File-Processing Framework\n\n\n\n\n\n\n\npython\n\n\ndata engineering\n\n\ncomputer vision\n\n\ndeep learning\n\n\nwildebeest\n\n\noss\n\n\n\n\nAn open-source framework for processing large collections of files in parallel with good error handling and logging.\n\n\n\n\n\n\nJul 30, 2020\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nFast and Reproducible Deep Learning\n\n\n\n\n\n\n\ndeep learning\n\n\nml\n\n\ncomputer vision\n\n\nwildebeest\n\n\noss\n\n\n\n\nBest practices for managing deep learning projects, including tools for processing large datasets, managing dependencies, and tracking experiments.\n\n\n\n\n\n\nMar 26, 2020\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating Classification Models, Part 4: Weighted Power Means of Precision and Recall\n\n\n\n\n\n\n\nml\n\n\nevals\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2019\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating Classification Models, Part 3: Fᵦ and Other Weighted Pythagorean Means of Precision and Recall\n\n\n\n\n\n\n\nml\n\n\nevals\n\n\n\n\n\n\n\n\n\n\n\nNov 26, 2019\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating Classification Models, Part 2: The Sufficiency of Precision and Recall\n\n\n\n\n\n\n\nml\n\n\nevals\n\n\n\n\n\n\n\n\n\n\n\nNov 20, 2019\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nEvaluating Classification Models, Part 1: Weighing False Positives Against False Negatives\n\n\n\n\n\n\n\nml\n\n\nevals\n\n\n\n\n\n\n\n\n\n\n\nNov 14, 2019\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nPython Dictionaries Are Now Ordered. Keep Using OrderedDict.\n\n\n\n\n\n\n\npython\n\n\npandas\n\n\n\n\nPython dictionaries are now ordered, but you should still use OrderedDict for better readability and interoperability.\n\n\n\n\n\n\nMar 10, 2018\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nHypothesis Test Construction as a Knapsack Problem\n\n\n\n\n\n\n\nstats\n\n\nphilosophy\n\n\n\n\n\n\n\n\n\n\n\nDec 14, 2015\n\n\nGreg Gandenberger\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am a machine learning engineer.\nPublic projects:\n\nWildebeest (Python file processing framework)\nModel Inspector (machine learning visualization library)\nAutofocus (camera trap animal classification application)\n\nPrevious employers:\n\nCruise (autonomous vehicles)\nArgo AI (autonomous vehicles)\nShopRunner (ecommerce)\nGeneral Assembly (tech education)\nUptake (industrial analytics)\n\nIn a previous life I received a Ph.D. in History and Philosophy of Science and an MA in Statistics, both from the University of Pittsburgh. My academic research addressed questions about the proper use of statistical methods, such as what to do with the results of an experiment that ends earlier than planned."
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "TIL: Fixing gRPC Python Imports\n\n\n\n\n\n\n\ntil\n\n\ngrpc\n\n\npython\n\n\nsoftware-engineering\n\n\n\n\n\n\n\n\n\n\n\nMay 14, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nEnriching Test Coverage Reports for LLMs\n\n\n\n\n\n\n\nllms\n\n\ntesting\n\n\nsoftware-engineering\n\n\n\n\n\n\n\n\n\n\n\nMay 7, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: Debugging an API Connection Error Caused by Invisible Carriage Returns\n\n\n\n\n\n\n\ntil\n\n\ndebugging\n\n\npython\n\n\nllms\n\n\n\n\n\n\n\n\n\n\n\nApr 9, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nTIL: Why Did My Loss Curve Change With Run Duration?\n\n\n\n\n\n\n\ntil\n\n\nml\n\n\ntraining\n\n\ndebugging\n\n\n\n\n\n\n\n\n\n\n\nMar 27, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTIL: Dev Containers and GPUs: A Match Made in Purgatory\n\n\n\n\n\n\n\ntil\n\n\nvscode\n\n\ndocker\n\n\ngpu\n\n\ncuda\n\n\n\n\n\n\n\n\n\n\n\nMar 14, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nSome Best Practices for LLM Math Evaluation\n\n\n\n\n\n\n\nml\n\n\nllms\n\n\nevals\n\n\nmath\n\n\n\n\n\n\n\n\n\n\n\nMar 5, 2025\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nProblem Solved: Custom Pandas styling not rendering when reopening notebook\n\n\n\n\n\n\n\nml\n\n\njupyter\n\n\npandas\n\n\n\n\nQuick fix for Pandas styling not showing up when reopening Jupyter notebooks - use jupyter trust command to enable custom styling rendering.\n\n\n\n\n\n\nAug 5, 2021\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nIntroducing Wildebeest, a Python File-Processing Framework\n\n\n\n\n\n\n\npython\n\n\ndata engineering\n\n\ncomputer vision\n\n\ndeep learning\n\n\nwildebeest\n\n\noss\n\n\n\n\nAn open-source framework for processing large collections of files in parallel with good error handling and logging.\n\n\n\n\n\n\nJul 30, 2020\n\n\nGreg Gandenberger\n\n\n\n\n\n\n  \n\n\n\n\nFast and Reproducible Deep Learning\n\n\n\n\n\n\n\ndeep learning\n\n\nml\n\n\ncomputer vision\n\n\nwildebeest\n\n\noss\n\n\n\n\nBest practices for managing deep learning projects, including tools for processing large datasets, managing dependencies, and tracking experiments.\n\n\n\n\n\n\nMar 26, 2020\n\n\nGreg Gandenberger\n\n\n\n\n\n\nNo matching items"
  }
]